# logic.py
import re
import os
import json
import random
import utils
from utils import VOCAB_DICT, load_nlp_resources, get_file_parsers, get_genanki
from styles import ANKI_CSS

# ==========================================
# æ ¸å¿ƒé€»è¾‘ (V29: èåˆç®—æ³•)
# ==========================================
def extract_text_from_file(uploaded_file):
    pypdf, docx, ebooklib, epub, BeautifulSoup = get_file_parsers()
    text = ""
    file_type = uploaded_file.name.split('.')[-1].lower()
    
    try:
        if file_type == 'txt':
            bytes_data = uploaded_file.getvalue()
            for encoding in ['utf-8', 'gb18030', 'latin-1']:
                try:
                    text = bytes_data.decode(encoding)
                    break
                except: continue
        elif file_type == 'pdf':
            reader = pypdf.PdfReader(uploaded_file)
            text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif file_type == 'docx':
            doc = docx.Document(uploaded_file)
            text = "\n".join([p.text for p in doc.paragraphs])
        elif file_type == 'epub':
            genanki, tempfile = get_genanki()
            with tempfile.NamedTemporaryFile(delete=False, suffix='.epub') as tmp:
                tmp.write(uploaded_file.getvalue())
                tmp_path = tmp.name
            book = epub.read_epub(tmp_path)
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    text += soup.get_text(separator=' ', strip=True) + " "
            os.remove(tmp_path)
    except Exception as e:
        return f"Error: {e}"
    return text

def is_valid_word(word):
    """
    åƒåœ¾è¯æ¸…æ´—
    """
    if len(word) < 2: return False
    if len(word) > 25: return False 
    # è¿ç»­3ä¸ªç›¸åŒå­—ç¬¦ -> è®¤ä¸ºæ˜¯åƒåœ¾è¯
    if re.search(r'(.)\1{2,}', word): return False
    # æ²¡æœ‰å…ƒéŸ³ -> è®¤ä¸ºæ˜¯ç¼©å†™æˆ–ä¹±ç  (æ’é™¤ hmm, brrr, zszs)
    if not re.search(r'[aeiouy]', word): return False
    return True

def analyze_logic(text, current_lvl, target_lvl, include_unknown):
    """
    V29 æ ¸å¿ƒç®—æ³•ï¼šæ··åˆå¢å¼ºåŒ¹é…
    åŒæ—¶æ£€æŸ¥ [å•è¯åŸå½¢] å’Œ [Lemma è¿˜åŸè¯]ï¼Œåªè¦ä»»æ„ä¸€ä¸ªåœ¨è¯é¢‘è¡¨ä¸”ç¬¦åˆ Rank èŒƒå›´ï¼Œå³å‘½ä¸­ã€‚
    """
    nltk, lemminflect = load_nlp_resources()
    
    def get_lemma_local(word):
        try: return lemminflect.getLemma(word, upos='VERB')[0]
        except: return word

    # 1. å®½æ¾åˆ†è¯ (ä¿ç•™ internal hyphens)
    raw_tokens = re.findall(r"[a-zA-Z]+(?:[-'][a-zA-Z]+)*", text)
    total_words = len(raw_tokens)
    
    # 2. æ¸…æ´— + å°å†™ + åˆæ­¥å»é‡
    clean_tokens = set([t.lower() for t in raw_tokens if is_valid_word(t.lower())])
    
    final_candidates = [] # å­˜å‚¨ (display_word, rank)
    seen_lemmas = set()   # ç”¨äºå»é‡é€»è¾‘ï¼Œé˜²æ­¢ go å’Œ went åŒæ—¶å‡ºç°
    
    for w in clean_tokens:
        # A. è®¡ç®— Lemma
        lemma = get_lemma_local(w)
        
        # B. è·å– Rank (ä¼˜å…ˆæŸ¥ Lemmaï¼ŒæŸ¥ä¸åˆ°æŸ¥åŸè¯)
        # å¾ˆå¤šè¯é¢‘è¡¨é‡Œåªæœ‰ goï¼Œæ²¡æœ‰ wentã€‚ä½†ä¹Ÿæœ‰å°‘æ•°æƒ…å†µåŸè¯æœ‰æ’åã€‚
        rank_lemma = VOCAB_DICT.get(lemma, 99999)
        rank_orig = VOCAB_DICT.get(w, 99999)
        
        # å–æœ€é å‰çš„æœ‰æ•ˆæ’å (é99999çš„æœ€å°å€¼)
        if rank_lemma != 99999 and rank_orig != 99999:
            best_rank = min(rank_lemma, rank_orig)
        elif rank_lemma != 99999:
            best_rank = rank_lemma
        else:
            best_rank = rank_orig
            
        # C. åˆ¤å®šæ˜¯å¦ç¬¦åˆèŒƒå›´
        is_in_range = (best_rank >= current_lvl and best_rank <= target_lvl)
        is_unknown_included = (best_rank == 99999 and include_unknown)
        
        if is_in_range or is_unknown_included:
            # D. å»é‡æ ¸å¿ƒé€»è¾‘
            # æˆ‘ä»¬å¸Œæœ›è¾“å‡ºçš„æ˜¯ Lemma (ä¾‹å¦‚è¾“å‡º go è€Œä¸æ˜¯ went)ï¼Œè¿™æ ·å¯¹èƒŒå•è¯æ›´å‹å¥½
            # ä½†å¦‚æœ Lemma æ˜¯æœªçŸ¥è¯ï¼Œè€ŒåŸè¯æ˜¯å·²çŸ¥è¯(æå°‘è§)ï¼Œåˆ™ä¿ç•™åŸè¯
            
            word_to_keep = lemma if rank_lemma != 99999 else w
            
            # ä½¿ç”¨ lemma ä½œä¸ºå»é‡é”®å€¼ (Key)
            # è¿™æ · went(go) å’Œ go(go) ä¼šè¢«è§†ä¸ºåŒä¸€ä¸ªï¼Œåªä¿ç•™ä¸€ä¸ª
            if lemma not in seen_lemmas:
                final_candidates.append((word_to_keep, best_rank))
                seen_lemmas.add(lemma)
    
    # E. æ’åºï¼šRank å°çš„åœ¨å‰ (é«˜é¢‘ -> ä½é¢‘)ï¼ŒæœªçŸ¥è¯(99999)æ”¾æœ€å
    final_candidates.sort(key=lambda x: x[1])
    
    return final_candidates, total_words

# logic.py (ä»…æ›¿æ¢ parse_anki_data å‡½æ•°ï¼Œå…¶ä»–ä¸å˜)

def parse_anki_data(json_input):
    """
    è§£æ AI è¿”å›çš„ JSONï¼Œæ”¯æŒï¼š
    1. æ ‡å‡† JSON æ•°ç»„ [...]
    2. JSON Lines (æ¯è¡Œä¸€ä¸ªå¯¹è±¡)
    3. è‡ªåŠ¨è·³è¿‡é”™è¯¯è¡Œ
    """
    results = []
    
    # 1. å°è¯•ä½œä¸ºæ•´ä½“ JSON æ•°ç»„è§£æ
    try:
        # å°è¯•æå– [...] éƒ¨åˆ†
        match = re.search(r'\[.*\]', json_input, re.DOTALL)
        if match:
            clean_json = match.group(0)
            return json.loads(clean_json)
    except:
        pass # å¦‚æœæ•´ä½“è§£æå¤±è´¥ï¼Œè¿›å…¥é€è¡Œè§£ææ¨¡å¼

    # 2. é€è¡Œè§£ææ¨¡å¼ (é’ˆå¯¹ AI ç”Ÿæˆçš„éæ ‡å‡†æ ¼å¼æˆ–ä¸€è¡Œä¸€ä¸ª JSON çš„æƒ…å†µ)
    lines = json_input.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        # å»æ‰è¡Œå°¾å¯èƒ½çš„é€—å·
        if line.endswith(','): 
            line = line[:-1]
            
        if not line: continue
        
        try:
            obj = json.loads(line)
            # ç®€å•éªŒè¯æ˜¯å¦åŒ…å«å¿…è¦å­—æ®µ
            if isinstance(obj, dict) and 'w' in obj:
                # å…¼å®¹ä¸åŒ AI å¯èƒ½è¿”å›çš„å­—æ®µåå·®å¼‚
                standard_obj = {
                    'front_phrase': obj.get('front_phrase', obj.get('w')), # å…¼å®¹ç®€å†™ w
                    'word': obj.get('word', obj.get('w')),
                    'meaning': obj.get('meaning', obj.get('m')),           # å…¼å®¹ç®€å†™ m
                    'example_sentence': obj.get('example_sentence', obj.get('e')), # å…¼å®¹ç®€å†™ e
                    'etymology': obj.get('etymology', obj.get('r'))        # å…¼å®¹ç®€å†™ r
                }
                results.append(standard_obj)
        except json.JSONDecodeError:
            # è¿™é‡Œå¯ä»¥æ‰“å°æ—¥å¿—ï¼Œæˆ–è€…ç›´æ¥è·³è¿‡é”™è¯¯è¡Œ
            print(f"Skipping invalid line: {line[:20]}...")
            continue
            
    return results

# ==========================================
# Anki ç”Ÿæˆ
# ==========================================
def generate_anki_package(cards_data, deck_name):
    genanki, tempfile = get_genanki()
    
    model_id = random.randrange(1 << 30, 1 << 31)
    model = genanki.Model(
        model_id, f'VocabFlow JSON Model {model_id}',
        fields=[{'name': 'FrontPhrase'}, {'name': 'Meaning'}, {'name': 'Examples'}, {'name': 'Etymology'}],
        templates=[{
            'name': 'Phrase Card',
            'qfmt': '<div class="phrase">{{FrontPhrase}}</div>', 
            'afmt': '''
            {{FrontSide}}<hr>
            <div class="definition">{{Meaning}}</div>
            <div class="examples">{{Examples}}</div>
            {{#Etymology}}
            <div class="footer-info"><div class="etymology">ğŸŒ± <b>è¯æº:</b> {{Etymology}}</div></div>
            {{/Etymology}}
            ''',
        }], css=ANKI_CSS
    )
    deck = genanki.Deck(random.randrange(1 << 30, 1 << 31), deck_name)
    for c in cards_data:
        deck.add_note(genanki.Note(model=model, fields=[str(c['front_phrase']), str(c['meaning']), str(c['examples']).replace('\n','<br>'), str(c['etymology'])]))
    with tempfile.NamedTemporaryFile(delete=False, suffix='.apkg') as tmp:
        genanki.Package(deck).write_to_file(tmp.name)
        return tmp.name

# ==========================================
# Prompt Logic
# ==========================================
def get_ai_prompt(words, front_mode, def_mode, ex_count, need_ety):
    w_list = ", ".join(words)
    
    if front_mode == "å•è¯ (Word)":
        w_instr = "Key `w`: The word itself (lowercase)."
    else:
        w_instr = "Key `w`: A short practical collocation/phrase (2-5 words)."

    if def_mode == "ä¸­æ–‡":
        m_instr = "Key `m`: Concise Chinese definition (max 10 chars)."
    elif def_mode == "ä¸­è‹±åŒè¯­":
        m_instr = "Key `m`: English Definition + Chinese Definition."
    else:
        m_instr = "Key `m`: English definition (concise)."

    e_instr = f"Key `e`: {ex_count} example sentence(s). Use `<br>` to separate if multiple."

    if need_ety:
        r_instr = "Key `r`: Simplified Chinese Etymology (Root/Prefix)."
    else:
        r_instr = "Key `r`: Leave this empty string \"\"."

    return f"""
Task: Create Anki cards.
Words: {w_list}

**OUTPUT: NDJSON (One line per object).**

**Requirements:**
1. {w_instr}
2. {m_instr}
3. {e_instr}
4. {r_instr}

**Keys:** `w` (Front), `m` (Meaning), `e` (Examples), `r` (Etymology)

**Example:**
{{"w": "...", "m": "...", "e": "...", "r": "..."}}

**Start:**
"""