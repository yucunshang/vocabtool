import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import json
import time
import requests
import zipfile
import concurrent.futures

# å°è¯•å¯¼å…¥å¤šæ ¼å¼æ–‡æ¡£å¤„ç†åº“
try:
    import PyPDF2
    import docx
except ImportError:
    pass

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro V5", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode { font-family: 'Consolas', 'Courier New', monospace !important; font-size: 16px !important; }
    header {visibility: hidden;} footer {visibility: hidden;}
    .block-container { padding-top: 1rem; }
    [data-testid="stMetricValue"] { font-size: 28px !important; color: #007bff !important; }
    /* å‚æ•°åŒºåŸŸæ ·å¼ä¼˜åŒ– */
    .param-container { border-bottom: 1px solid #eee; padding-bottom: 20px; margin-bottom: 20px; }
    .copy-hint { color: #888; font-size: 14px; margin-bottom: 5px; margin-top: 10px; padding-left: 5px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. API Key è·å– (ä¸¥æ ¼éµå¾ªåŸå§‹è®¾ç½®)
# ==========================================
try:
    # ç›´æ¥è¯»å– secretsï¼Œä¸åšä»»ä½• UI å±•ç¤º
    user_api_key = st.secrets["DEEPSEEK_API_KEY"]
except Exception:
    st.error("âŒ æœªæ£€æµ‹åˆ° API Keyé…ç½®ã€‚è¯·åœ¨ .streamlit/secrets.toml ä¸­é…ç½® DEEPSEEK_API_KEY")
    st.stop()

# ==========================================
# 3. æ•°æ®ä¸ NLP åˆå§‹åŒ– (ä¿æŒå¥å£®ç‰ˆ)
# ==========================================
@st.cache_data
def load_knowledge_base():
    data = {"terms": {}, "proper": {}, "patch": {}, "ambiguous": set()}
    try:
        if os.path.exists('data/terms.json'):
            with open('data/terms.json', 'r', encoding='utf-8') as f: data["terms"] = {k.lower(): v for k, v in json.load(f).items()}
        if os.path.exists('data/proper.json'):
            with open('data/proper.json', 'r', encoding='utf-8') as f: data["proper"] = {k.lower(): v for k, v in json.load(f).items()}
        if os.path.exists('data/patch.json'):
            with open('data/patch.json', 'r', encoding='utf-8') as f: data["patch"] = json.load(f)
        if os.path.exists('data/ambiguous.json'):
            with open('data/ambiguous.json', 'r', encoding='utf-8') as f: data["ambiguous"] = set(json.load(f))
    except Exception: pass
    return data["terms"], data["proper"], data["patch"], data["ambiguous"]

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

@st.cache_resource
def setup_nltk():
    try: nltk.data.find('corpora/wordnet')
    except LookupError:
        root_dir = os.path.dirname(os.path.abspath(__file__))
        nltk_data_dir = os.path.join(root_dir, 'nltk_data')
        os.makedirs(nltk_data_dir, exist_ok=True)
        nltk.data.path.append(nltk_data_dir)
        for pkg in ['averaged_perceptron_tagger', 'punkt', 'wordnet']:
            try: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
            except: pass

setup_nltk()

def get_lemma(w):
    if not w: return ""
    try:
        lemmas_dict = lemminflect.getAllLemmas(w)
        if not lemmas_dict: return w.lower()
        for pos in ['VERB', 'NOUN', 'ADJ', 'ADV']:
            if pos in lemmas_dict: return lemmas_dict[pos][0]
        return list(lemmas_dict.values())[0][0]
    except: return w.lower()

@st.cache_data
def load_vocab():
    vocab = {}
    file_path = next((f for f in ["coca_cleaned.csv", "data.csv"] if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
            df = df.sort_values(r_col, ascending=True).drop_duplicates(subset=[w_col], keep='first')
            vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except Exception: pass
    
    if BUILTIN_PATCH_VOCAB:
        for word, rank in BUILTIN_PATCH_VOCAB.items(): vocab[word] = rank
    
    URGENT_OVERRIDES = {"china": 400, "usa": 200, "uk": 200, "google": 1000, "apple": 1000}
    for word, rank in URGENT_OVERRIDES.items(): vocab[word] = rank
    return vocab

vocab_dict = load_vocab()

# ==========================================
# 4. æ–‡æ¡£è§£æ & å¹¶å‘ API (çº¿ç¨‹å®‰å…¨)
# ==========================================
def extract_text_from_file(uploaded_file):
    ext = uploaded_file.name.split('.')[-1].lower()
    uploaded_file.seek(0)
    try:
        if ext == 'txt': return uploaded_file.getvalue().decode("utf-8", errors="ignore")
        elif ext == 'pdf':
            reader = PyPDF2.PdfReader(uploaded_file)
            return " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif ext == 'docx':
            doc = docx.Document(uploaded_file)
            return " ".join([p.text for p in doc.paragraphs])
        elif ext == 'epub': return "EPUBè§£ææš‚ç•¥" # ç®€åŒ–å±•ç¤º
    except Exception: return ""
    return ""

def get_base_prompt_template(export_format="TXT"):
    return f"""ã€è§’è‰²è®¾å®šã€‘ ä½ æ˜¯ä¸€ä½ç²¾é€šè¯æºå­¦ã€è®¤çŸ¥å¿ƒç†å­¦ä»¥åŠ Anki ç®—æ³•çš„â€œè‹±è¯­è¯æ±‡ä¸“å®¶ä¸é—ªå¡åˆ¶ä½œå¤§å¸ˆâ€ã€‚
1. æ ¸å¿ƒåŸåˆ™ï¼šåŸå­æ€§ (Atomicity)
è‹¥ä¸€ä¸ªå•è¯æœ‰å¤šä¸ªå¸¸ç”¨å«ä¹‰ï¼Œå¿…é¡»æ‹†åˆ†ä¸ºå¤šæ¡ç‹¬ç«‹æ•°æ®ã€‚
2. å¡ç‰‡æ­£é¢ (Column 1)
æä¾›è‡ªç„¶çš„çŸ­è¯­æˆ–æ­é… (Phrase/Collocation)ã€‚
3. å¡ç‰‡èƒŒé¢ (Column 2 - æ•´åˆé¡µ)
ä½¿ç”¨ HTML æ ‡ç­¾æ’ç‰ˆï¼ŒåŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼Œç”¨ <br><br> åˆ†éš”ï¼š
è‹±æ–‡é‡Šä¹‰ <br><br> <em>æ–œä½“ä¾‹å¥</em> <br><br> ã€ä¸­æ–‡è¯æº/è®°å¿†æ³•ã€‘
4. è¾“å‡ºæ ¼å¼æ ‡å‡† ({export_format} æ ¼å¼)
çº¯æ–‡æœ¬ä»£ç å—ï¼Œæ—  Markdown åŒ…è£¹ã€‚é€—å·åˆ†éš”ï¼Œå­—æ®µç”¨åŒå¼•å·åŒ…è£¹ã€‚
"""

def _fetch_deepseek_chunk_safe(batch_data):
    index, batch_words, prompt_template, api_key = batch_data
    url = "https://api.deepseek.com/chat/completions"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    system_enforcement = "\n\nã€ç³»ç»Ÿç»å¯¹å¼ºåˆ¶æŒ‡ä»¤ã€‘ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„æ•°æ®ä»£ç ï¼Œä¸è¦å›å¤â€œå¥½çš„â€ï¼Œä¸è¦ä½¿ç”¨ ```csv åŒ…è£¹ï¼"
    full_prompt = f"{prompt_template}{system_enforcement}\n\nå¾…å¤„ç†å•è¯åˆ—è¡¨ï¼š\n{', '.join(batch_words)}"
    
    payload = {"model": "deepseek-chat", "messages": [{"role": "user", "content": full_prompt}], "temperature": 0.3, "max_tokens": 4096}
    
    try:
        for attempt in range(3):
            resp = requests.post(url, json=payload, headers=headers, timeout=60)
            if resp.status_code == 429: 
                time.sleep(2 * (attempt + 1))
                continue
            if resp.status_code != 200: return (index, "", f"HTTP {resp.status_code}")
            
            result = resp.json()['choices'][0]['message']['content'].strip()
            if result.startswith("```"):
                lines = result.split('\n')
                if lines[0].startswith("```"): lines = lines[1:]
                if lines and lines[-1].startswith("```"): lines = lines[:-1]
                result = '\n'.join(lines).strip()
            return (index, result, None)
        return (index, "", "TIMEOUT")
    except Exception as e: return (index, "", str(e))

def run_concurrent_api(words, prompt_template, api_key, progress_bar, status_text):
    MAX_WORDS = 300 
    words = words[:MAX_WORDS]
    CHUNK_SIZE = 30
    chunks = [words[i:i + CHUNK_SIZE] for i in range(0, len(words), CHUNK_SIZE)]
    tasks = [(i, chunk, prompt_template, api_key) for i, chunk in enumerate(chunks)]
    results_map = {}
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_idx = {executor.submit(_fetch_deepseek_chunk_safe, task): task[0] for task in tasks}
        completed = 0
        for future in concurrent.futures.as_completed(future_to_idx):
            idx, res_str, err = future.result()
            if not err: results_map[idx] = res_str
            completed += 1
            progress_bar.progress(completed / len(chunks))
            status_text.markdown(f"âš¡ AI æ­£åœ¨å¤„ç†ç¬¬ {completed}/{len(chunks)} æ‰¹æ•°æ®...")

    final_output = []
    for i in range(len(chunks)):
        if i in results_map: final_output.append(results_map[i])
    return "\n".join(final_output)

def analyze_words(unique_word_list, min_rank):
    unique_items = [] 
    STOP_WORDS = {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it'}
    
    for item_lower in unique_word_list:
        if len(item_lower) < 2 or item_lower in STOP_WORDS: continue
        
        actual_rank = vocab_dict.get(item_lower, 99999)
        # ä¸¥æ ¼æ‰§è¡Œ rank è¿‡æ»¤
        if actual_rank < min_rank and actual_rank != 99999: continue

        if item_lower in BUILTIN_TECHNICAL_TERMS:
             unique_items.append({"word": f"{item_lower}", "rank": actual_rank, "raw": item_lower})
        elif actual_rank != 99999:
            unique_items.append({"word": item_lower, "rank": actual_rank, "raw": item_lower})
        elif item_lower in PROPER_NOUNS_DB:
             unique_items.append({"word": item_lower, "rank": 99999, "raw": item_lower})
            
    return pd.DataFrame(unique_items)

# ==========================================
# 5. UI å¸ƒå±€ (æ— ä¾§è¾¹æ ï¼Œå‚æ•°å¸¸é©»)
# ==========================================
st.title("ğŸš€ Vocab Master Pro - V5")

# åˆå§‹åŒ– Session State
if "raw_input_text" not in st.session_state: st.session_state.raw_input_text = ""
if "uploader_key" not in st.session_state: st.session_state.uploader_key = 0 
if "is_processed" not in st.session_state: st.session_state.is_processed = False
if "generated_cards" not in st.session_state: st.session_state.generated_cards = {} 

def clear_all_inputs():
    st.session_state.raw_input_text = ""
    st.session_state.uploader_key += 1 
    st.session_state.is_processed = False
    st.session_state.generated_cards = {}

# --- å‚æ•°è®¾ç½®åŒºåŸŸ (æ˜¾å¼å±•ç¤ºï¼Œä¸æŠ˜å ) ---
st.markdown("### âš™ï¸ æ ¸å¿ƒå‚æ•°")
c1, c2, c3, c4 = st.columns(4)
with c1: current_level = st.number_input("ğŸ¯ å½“å‰è¯æ±‡é‡ (èµ·)", 0, 30000, 4500, 500, help="ä½äºæ­¤æ’åçš„è¯å°†è¢«è§†ä¸ºâ€˜ç†Ÿè¯â€™")
with c2: target_level = st.number_input("ğŸ¯ ç›®æ ‡è¯æ±‡é‡ (æ­¢)", 0, 30000, 15000, 500, help="é«˜äºæ­¤æ’åçš„è¯å°†è¢«è§†ä¸ºâ€˜è¶…çº²â€™")
with c3: top_n = st.number_input("ğŸ”¥ ç²¾é€‰ Top N", 10, 500, 50, 10)
with c4: min_rank_threshold = st.number_input("ğŸ“‰ å¿½ç•¥å‰ N è¯", 0, 20000, 1000, 500, help="ç›´æ¥è¿‡æ»¤æ‰æ’åæé«˜(å¤ªç®€å•)çš„è¯")
show_rank = st.checkbox("åœ¨åˆ—è¡¨ä¸­æ˜¾ç¤ºè¯é¢‘ Rank", value=True)

st.divider()

# --- è¾“å…¥åŒº ---
col_input1, col_input2 = st.columns([3, 2])
with col_input1:
    raw_text = st.text_area("ğŸ“¥ ç²˜è´´æ–‡æœ¬", height=150, key="raw_input_text", placeholder="åœ¨æ­¤ç²˜è´´è‹±æ–‡å†…å®¹...")
with col_input2:
    st.markdown("#### ğŸ“‚ æ–‡æ¡£è§£æ")
    uploaded_file = st.file_uploader("æ”¯æŒ TXT, PDF, DOCX", type=["txt", "pdf", "docx"], key=f"uploader_{st.session_state.uploader_key}")

col_btn1, col_btn2 = st.columns([5, 1])
with col_btn1: btn_process = st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", use_container_width=True)
with col_btn2: st.button("ğŸ—‘ï¸ æ¸…ç©º", on_click=clear_all_inputs, use_container_width=True)

# ==========================================
# 6. å¤„ç†ä¸å±•ç¤ºé€»è¾‘
# ==========================================
if btn_process:
    with st.spinner("ğŸ§  åˆ†æä¸­..."):
        start_time = time.time()
        combined_text = raw_text
        if uploaded_file is not None: combined_text += "\n" + extract_text_from_file(uploaded_file)
            
        if not combined_text.strip():
            st.warning("âš ï¸ å†…å®¹ä¸ºç©º")
        else:
            raw_words = re.findall(r"[a-zA-Z]+(?:[-'][a-zA-Z]+)*", combined_text)
            lemmatized_words = [get_lemma(w) for w in raw_words]
            unique_lemmas = list(set([w.lower() for w in lemmatized_words]))
            
            # å°† min_rank_threshold ä¼ å…¥åˆ†æå‡½æ•°
            st.session_state.base_df = analyze_words(unique_lemmas, min_rank_threshold)
            st.session_state.lemma_text = " ".join(lemmatized_words)
            st.session_state.stats = {
                "raw_count": len(raw_words),
                "unique_count": len(unique_lemmas),
                "valid_count": len(st.session_state.base_df),
                "time": time.time() - start_time
            }
            st.session_state.is_processed = True
            st.session_state.generated_cards = {} 

if st.session_state.get("is_processed", False):
    stats = st.session_state.stats
    with st.container():
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("æ€»è¯æ•°", f"{stats['raw_count']:,}")
        c2.metric("å»é‡å", f"{stats['unique_count']:,}")
        c3.metric("æœ‰æ•ˆè¯", f"{stats['valid_count']:,}")
        c4.metric("è€—æ—¶", f"{stats['time']:.2f}s")
    
    df = st.session_state.base_df.copy()
    
    if not df.empty:
        def categorize(row):
            r = row['rank']
            if r <= current_level: return "known"
            elif r <= target_level: return "target"
            else: return "beyond"
        
        df['final_cat'] = df.apply(categorize, axis=1)
        df = df.sort_values(by='rank')
        
        top_df = df[df['rank'] >= min_rank_threshold].sort_values(by='rank', ascending=True).head(top_n)
        target_df = df[df['final_cat']=='target']
        beyond_df = df[df['final_cat']=='beyond']
        
        tabs = st.tabs(["ğŸ”¥ Topç²¾é€‰", "ğŸŸ¡ é‡ç‚¹è¯æ±‡", "ğŸ”´ è¶…çº²è¯æ±‡", "ğŸ“ åŸæ–‡ä¸‹è½½"])
        
        def render_word_tab(tab_obj, data_df, tab_key):
            with tab_obj:
                if data_df.empty:
                    st.info("è¯¥åŒºé—´æš‚æ— å•è¯")
                    return

                col_list, col_ai = st.columns([1, 2])
                with col_list:
                    st.markdown(f"**å•è¯é¢„è§ˆ ({len(data_df)})**")
                    display_text = []
                    for _, row in data_df.iterrows():
                        suffix = f" [{int(row['rank'])}]" if show_rank and row['rank'] != 99999 else ""
                        display_text.append(f"{row['word']}{suffix}")
                    st.text_area("åˆ—è¡¨", value="\n".join(display_text), height=400, label_visibility="collapsed")

                with col_ai:
                    st.markdown("#### ğŸ¤– AI å¡ç‰‡åˆ¶ä½œ")
                    export_fmt = st.radio("æ ¼å¼", ["TXT", "CSV"], horizontal=True, key=f"fmt_{tab_key}")
                    pure_words = data_df['word'].tolist()
                    
                    # æ¢å¤ï¼šAPIç›´æ¥è°ƒç”¨å’Œæ‰‹åŠ¨å¤åˆ¶Promptçš„åŒTabè®¾è®¡
                    ai_tab1, ai_tab2 = st.tabs(["âš¡ ä¸€é”®è°ƒç”¨ DeepSeek", "ğŸ“‹ æ‰‹åŠ¨å¤åˆ¶ Prompt"])
                    
                    with ai_tab1:
                        res_key = f"{tab_key}_{export_fmt}"
                        if st.session_state.generated_cards.get(res_key):
                            st.success("âœ… å·²ç”Ÿæˆ")
                            st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ", st.session_state.generated_cards[res_key], f"anki_{tab_key}.{export_fmt.lower()}")
                            st.code(st.session_state.generated_cards[res_key], language="text")
                        else:
                            if st.button(f"âš¡ ç”Ÿæˆ {tab_key}", key=f"btn_{tab_key}"):
                                p_bar = st.progress(0)
                                s_text = st.empty()
                                res = run_concurrent_api(pure_words, get_base_prompt_template(export_fmt), user_api_key, p_bar, s_text)
                                st.session_state.generated_cards[res_key] = res
                                st.rerun()

                    with ai_tab2:
                        st.info("ğŸ’¡ å¦‚æœæ‚¨æƒ³ä½¿ç”¨ ChatGPT/Claude ç­‰è‡ªå·±çš„ AI å·¥å…·ï¼Œè¯·ç‚¹å‡»å³ä¸Šè§’ä¸€é”®å¤åˆ¶ä¸‹æ–¹å®Œæ•´æŒ‡ä»¤ï¼š")
                        full_prompt_to_copy = f"{get_base_prompt_template(export_fmt)}\n\nå¾…å¤„ç†å•è¯ï¼š\n{', '.join(pure_words)}"
                        st.markdown("<p class='copy-hint'>ğŸ‘† é¼ æ ‡æ‚¬åœåœ¨ä¸‹æ–¹æ¡†å†…ï¼Œç‚¹å‡»å³ä¸Šè§’ ğŸ“‹ å›¾æ ‡ä¸€é”®å¤åˆ¶</p>", unsafe_allow_html=True)
                        st.code(full_prompt_to_copy, language='markdown')

        render_word_tab(tabs[0], top_df, "top")
        render_word_tab(tabs[1], target_df, "target")
        render_word_tab(tabs[2], beyond_df, "beyond")
        
        with tabs[3]:
            st.info("ğŸ’¡ è¿™æ˜¯è‡ªåŠ¨è¯å½¢è¿˜åŸåçš„å…¨æ–‡è¾“å‡ºï¼Œå·²é’ˆå¯¹é•¿æ–‡ä¼˜åŒ–é˜²å¡æ­»ä½“éªŒã€‚")
            st.download_button("ğŸ’¾ ä¸‹è½½åŸæ–‡", st.session_state.lemma_text, "lemmatized.txt")
            st.text_area("é¢„è§ˆ", st.session_state.lemma_text[:2000], height=300)