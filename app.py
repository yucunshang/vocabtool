import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import json
import time
import requests
import zipfile
import concurrent.futures
from collections import Counter
from io import BytesIO

# ==========================================
# 1. åŸºç¡€é…ç½®ä¸å¢å¼ºå‹ CSS
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode { font-family: 'Fira Code', 'Consolas', monospace !important; font-size: 15px !important; }
    .main .block-container { padding-top: 2rem; }
    .stMetric { background: #f0f2f6; padding: 10px; border-radius: 10px; border: 1px solid #d1d5db; }
    .param-box { background-color: #ffffff; padding: 20px; border-radius: 12px; box-shadow: 0 4px 6px -1px rgba(0,0,0,0.1); margin-bottom: 25px; border-left: 5px solid #ff4b4b; }
    .copy-hint { color: #6b7280; font-size: 0.85rem; margin-top: 5px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ ¸å¿ƒ NLP ä¸ æ•°æ®é€»è¾‘
# ==========================================

@st.cache_data(show_spinner=False)
def load_knowledge_base():
    """å¸¦å®¹é”™çš„çŸ¥è¯†åº“åŠ è½½"""
    base_path = "data"
    default_res = ({}, {}, {}, set())
    try:
        def load_json(name):
            p = os.path.join(base_path, name)
            if os.path.exists(p):
                with open(p, 'r', encoding='utf-8') as f: return json.load(f)
            return {}
        terms = {k.lower(): v for k, v in load_json('terms.json').items()}
        proper = {k.lower(): v for k, v in load_json('proper.json').items()}
        patch = load_json('patch.json')
        ambiguous = set(load_json('ambiguous.json'))
        return terms, proper, patch, ambiguous
    except Exception:
        return default_res

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

@st.cache_resource
def setup_nltk():
    nltk_data_dir = os.path.join(os.getcwd(), 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    for pkg in ['averaged_perceptron_tagger', 'punkt']:
        try: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
        except: pass
setup_nltk()

LEMMA_CACHE = {}
def get_lemma_optimized(w: str) -> str:
    w_lower = w.lower()
    if w_lower in LEMMA_CACHE: return LEMMA_CACHE[w_lower]
    lemmas_dict = lemminflect.getAllLemmas(w_lower)
    if not lemmas_dict: res = w_lower
    else:
        res = w_lower
        for pos in ['VERB', 'ADJ', 'NOUN', 'ADV']:
            if pos in lemmas_dict:
                res = lemmas_dict[pos][0]
                break
    LEMMA_CACHE[w_lower] = res
    return res

@st.cache_data
def load_vocab():
    vocab = {}
    for f_name in ["coca_cleaned.csv", "data.csv", "data/coca.csv"]:
        if os.path.exists(f_name):
            try:
                df = pd.read_csv(f_name)
                df.columns = [str(c).strip().lower() for c in df.columns]
                w_col = next((c for c in df.columns if 'word' in c or 'å•è¯' in c), df.columns[0])
                r_col = next((c for c in df.columns if 'rank' in c or 'æ’åº' in c), df.columns[1])
                df[w_col] = df[w_col].astype(str).str.lower().str.strip()
                df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
                df = df.sort_values(r_col).drop_duplicates(subset=[w_col])
                vocab = dict(zip(df[w_col], df[r_col]))
                break
            except: continue
    vocab.update(BUILTIN_PATCH_VOCAB)
    vocab.update({"china": 400, "google": 800, "apple": 800, "monday": 300, "january": 400})
    return vocab

VOCAB_DICT = load_vocab()

# ==========================================
# 3. æ–‡æ¡£å¤„ç†ä¸ Prompt å¼•æ“
# ==========================================

def extract_text_from_file(uploaded_file):
    ext = uploaded_file.name.split('.')[-1].lower()
    try:
        content = uploaded_file.read()
        if ext == 'txt': return content.decode("utf-8", errors="ignore")
        elif ext == 'pdf':
            import PyPDF2
            reader = PyPDF2.PdfReader(BytesIO(content))
            return " ".join([p.extract_text() for p in reader.pages if p.extract_text()])
        elif ext == 'docx':
            import docx
            doc = docx.Document(BytesIO(content))
            return " ".join([p.text for p in doc.paragraphs])
        elif ext == 'epub':
            with zipfile.ZipFile(BytesIO(content)) as z:
                return " ".join([re.sub(r'<[^>]+>', ' ', z.read(f).decode('utf-8', errors='ignore')) 
                                for f in z.namelist() if f.endswith(('.html', '.xhtml'))])
    except Exception as e:
        st.error(f"è§£æå¤±è´¥: {e}")
    return ""

def get_base_prompt_template(export_format="TXT"):
    return f"""ã€è§’è‰²ã€‘ä½ æ˜¯ä¸€ä½ Anki é—ªå¡ä¸“å®¶ã€‚è¯·å¤„ç†ä»¥ä¸‹å•è¯åˆ—è¡¨ï¼š
1. åŸå­æ€§ï¼šæ¯ä¸ªä¹‰é¡¹ç‹¬ç«‹æˆå¡ã€‚
2. æ­£é¢ï¼šè‡ªç„¶çš„çŸ­è¯­æˆ–æ­é…ã€‚
3. èƒŒé¢ï¼šHTMLæ’ç‰ˆï¼ŒåŒ…å« [è‹±æ–‡é‡Šä¹‰]<br><br><em>[ä¾‹å¥]</em><br><br>ã€è¯æ ¹è¯ç¼€ã€‘[ä¸­æ–‡è§£æ]ã€‚
4. æ ¼å¼ï¼š{export_format}ï¼Œæ¯ä¸ªå­—æ®µç”¨åŒå¼•å·åŒ…è£¹ï¼Œé€—å·åˆ†éš”ã€‚
ä¸è¦è¾“å‡ºä»»ä½• Markdown è¯­æ³•æ ‡è®°ï¼ˆå¦‚ ```csvï¼‰ï¼Œç›´æ¥è¾“å‡ºçº¯æ–‡æœ¬å†…å®¹ã€‚"""

# ==========================================
# 4. AI å¹¶å‘å¼•æ“
# ==========================================

def _fetch_deepseek_chunk(batch_words, prompt_template, api_key):
    url = "https://api.deepseek.com/chat/completions"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    full_prompt = f"{prompt_template}\n\nå¾…å¤„ç†è¯ï¼š\n{', '.join(batch_words)}"
    payload = {"model": "deepseek-chat", "messages": [{"role": "user", "content": full_prompt}], "temperature": 0.3}
    
    for attempt in range(3):
        try:
            resp = requests.post(url, json=payload, headers=headers, timeout=120)
            if resp.status_code == 429: time.sleep(5); continue
            resp.raise_for_status()
            content = resp.json()['choices'][0]['message']['content'].strip()
            return re.sub(r'^```[a-zA-Z]*\n|\n```$', '', content)
        except Exception as e:
            if attempt == 2: return f"âŒ é”™è¯¯: {e}"
            time.sleep(2)
    return "âŒ è¶…æ—¶"

def call_deepseek_api_chunked(prompt_template, words, progress_bar, status_container):
    api_key = st.secrets.get("DEEPSEEK_API_KEY")
    if not api_key: return "âš ï¸ æœªé…ç½® API KEY"
    
    CHUNK_SIZE = 25
    chunks = [words[i:i + CHUNK_SIZE] for i in range(0, len(words), CHUNK_SIZE)]
    results = [None] * len(chunks)
    
    with st.status("ğŸš€ AI å¹¶å‘å¼•æ“å¤„ç†ä¸­...", expanded=True) as status:
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            future_to_idx = {executor.submit(_fetch_deepseek_chunk, chunks[i], prompt_template, api_key): i for i in range(len(chunks))}
            completed = 0
            for future in concurrent.futures.as_completed(future_to_idx):
                idx = future_to_idx[future]
                results[idx] = future.result()
                completed += 1
                progress_bar.progress(completed / len(chunks))
                status.write(f"âœ… è¿›åº¦: {completed}/{len(chunks)} æ‰¹æ¬¡")
        status.update(label="âœ¨ ç”Ÿæˆå®Œæ¯•", state="complete", expanded=False)
    return "\n".join(filter(None, results))

# ==========================================
# 5. åˆ†ææµæ°´çº¿
# ==========================================

def process_pipeline(text):
    raw_words = re.findall(r"\b[a-zA-Z']{2,}\b", text)
    if not raw_words: return None, None
    word_counts = Counter(raw_words)
    unique_lemmas_map = {}
    for word, count in word_counts.items():
        lemma = get_lemma_optimized(word)
        unique_lemmas_map[lemma] = unique_lemmas_map.get(lemma, 0) + count
    
    data = []
    for lemma, count in unique_lemmas_map.items():
        rank = VOCAB_DICT.get(lemma, 99999)
        display = PROPER_NOUNS_DB.get(lemma, lemma)
        if lemma in BUILTIN_TECHNICAL_TERMS: display = f"{lemma} ({BUILTIN_TECHNICAL_TERMS[lemma]})"
        data.append({"word": display, "rank": rank, "count": count, "raw": lemma})
    return pd.DataFrame(data).sort_values('rank'), raw_words

# ==========================================
# 6. UI ç•Œé¢
# ==========================================

st.title("ğŸš€ Vocab Master Pro")

if "is_processed" not in st.session_state: st.session_state.is_processed = False

# å‚æ•°åŒº
with st.container():
    st.markdown("<div class='param-box'>", unsafe_allow_html=True)
    c1, c2, c3, c4 = st.columns(4)
    with c1: cur_lv = st.number_input("ğŸ¯ æŒæ¡è¯é¢‘èµ·", 0, 20000, 3500)
    with c2: tgt_lv = st.number_input("ğŸ¯ ç›®æ ‡è¯é¢‘æ­¢", 0, 30000, 12000)
    with c3: top_n = st.number_input("ğŸ”¥ ç²¾é€‰ Top N", 5, 500, 50)
    with c4: show_rank = st.checkbox("æ˜¾ç¤ºè¯é¢‘", True)
    st.markdown("</div>", unsafe_allow_html=True)

# è¾“å…¥åŒº
col_in1, col_in2 = st.columns([2, 1])
with col_in1: raw_input = st.text_area("ğŸ“¥ ç²˜è´´æ–‡æœ¬", height=150)
with col_in2: uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£", type=["txt", "pdf", "docx", "epub"])

if st.button("ğŸš€ æé€Ÿè§£æ", type="primary", use_container_width=True):
    content = raw_input + ("\n" + extract_text_from_file(uploaded_file) if uploaded_file else "")
    if content.strip():
        with st.spinner("åˆ†æä¸­..."):
            df, raw_words = process_pipeline(content)
            if df is not None:
                st.session_state.df = df
                st.session_state.raw_count = len(raw_words)
                st.session_state.is_processed = True
    else: st.warning("å†…å®¹ä¸ºç©º")

# æ¸²æŸ“é€»è¾‘
if st.session_state.get("is_processed"):
    df = st.session_state.df
    m1, m2, m3 = st.columns(3)
    m1.metric("è¯æ±‡æ€»é‡", st.session_state.raw_count)
    m2.metric("ç‹¬ç«‹è¯æ ¹", len(df))
    m3.metric("éœ€é‡ç‚¹å­¦", len(df[(df['rank'] > cur_lv) & (df['rank'] <= tgt_lv)]))

    target_df = df[(df['rank'] > cur_lv) & (df['rank'] <= tgt_lv)].copy()
    beyond_df = df[df['rank'] > tgt_lv].copy()
    top_n_df = target_df.head(top_n)

    tabs = st.tabs([f"ğŸ”¥ Top {len(top_n_df)}", "ğŸŸ¡ é‡ç‚¹è¯", "ğŸ”´ è¶…çº²è¯", "ğŸŸ¢ å·²æŒæ¡"])

    def render_vocab_tab(tab, data_df, key_prefix):
        with tab:
            if data_df.empty:
                st.info("æ— å•è¯")
                return
            
            # è¿™é‡Œæ˜¯ä¹‹å‰æŠ¥é”™çš„å…³é”®ç‚¹ï¼šç§»é™¤ from app import ...
            # å¹¶åœ¨ render_vocab_tab å†…éƒ¨ä½¿ç”¨æœ¬åœ°å˜é‡
            words_list = data_df['raw'].tolist()
            with st.expander("ğŸ‘ï¸ å•è¯é¢„è§ˆ"):
                st.code("\n".join([f"{r['word']} [{int(r['rank'])}]" for _, r in data_df.iterrows()]))
            
            st.divider()
            exp_fmt = st.radio("å¯¼å‡ºæ ¼å¼", ["TXT", "CSV"], horizontal=True, key=f"f_{key_prefix}")
            
            # ä½¿ç”¨ container åŒ…è£…æŒ‰é’®ä»¥é˜²å¸ƒå±€é”™ä¹±
            with st.container():
                if st.button(f"âš¡ AI ç”Ÿæˆ {len(data_df)} ä¸ªå•è¯çš„å¡ç‰‡", key=f"b_{key_prefix}"):
                    p_bar = st.progress(0)
                    prompt = get_base_prompt_template(exp_fmt)
                    result = call_deepseek_api_chunked(prompt, words_list, p_bar, st.empty())
                    
                    if result and "âŒ" not in result:
                        st.download_button("ğŸ“¥ ä¸‹è½½æ–‡ä»¶", result.encode('utf-8-sig'), 
                                         file_name=f"anki_{key_prefix}.{exp_fmt.lower()}", type="primary")
                        st.code(result)
                    else: st.error(result)

    render_vocab_tab(tabs[0], top_n_df, "top")
    render_vocab_tab(tabs[1], target_df, "target")
    render_vocab_tab(tabs[2], beyond_df, "beyond")
    render_vocab_tab(tabs[3], df[df['rank'] <= cur_lv], "known")