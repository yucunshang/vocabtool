import streamlit as st
import pandas as pd
import re
import os
import spacy

# ==========================================
# 0. æ ¸å¿ƒå¼•æ“ï¼šåŠ è½½ spaCy (å·¥ä¸šçº§ NLP)
# ==========================================
st.set_page_config(page_title="Vibe Vocab Studio", page_icon="ğŸ§ ", layout="wide")

@st.cache_resource
def load_nlp():
    try:
        # åŠ è½½è‹±è¯­æ¨¡å‹
        return spacy.load("en_core_web_sm")
    except OSError:
        # å¦‚æœé€šè¿‡é“¾æ¥å®‰è£…å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä¸‹è½½ï¼ˆé€šå¸¸ requirements å†™äº†é“¾æ¥ä¸éœ€è¦è¿™æ­¥ï¼‰
        from spacy.cli import download
        download("en_core_web_sm")
        return spacy.load("en_core_web_sm")

try:
    nlp = load_nlp()
    NLP_STATUS = "âœ… spaCy å¼•æ“å°±ç»ª"
except Exception as e:
    st.error(f"spaCy æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    st.stop()

def get_lemma_spacy(word):
    """
    ä½¿ç”¨ spaCy è¿›è¡Œç²¾å‡†è¿˜åŸ
    families -> family
    are -> be
    went -> go
    """
    doc = nlp(word)
    # å–ç¬¬ä¸€ä¸ªè¯çš„ lemma_ (åŸå½¢)
    return doc[0].lemma_.lower()

# ==========================================
# 1. è¯åº“åŠ è½½ (ä¿æŒä¹‹å‰çš„ç¨³å¥é€»è¾‘)
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv", "COCA20000è¯Excelç‰ˆ.xlsx - Sheet1.csv"]

@st.cache_data
def load_vocab():
    file_path = None
    for f in POSSIBLE_FILES:
        if os.path.exists(f):
            file_path = f
            break
            
    if not file_path: return None, "æœªæ‰¾åˆ°æ–‡ä»¶"

    # ä¼˜å…ˆè¯» coca_cleaned (æ ‡å‡†æ ¼å¼)
    if 'cleaned' in file_path:
        try:
            df = pd.read_csv(file_path)
            if 'word' in df.columns and 'rank' in df.columns:
                vocab = pd.Series(df['rank'].values, index=df['word'].astype(str)).to_dict()
                return vocab, "åŠ è½½æˆåŠŸ (Cleaned)"
        except: pass

    # å…œåº•è¯»åŸå§‹æ–‡ä»¶
    for enc in ['utf-8', 'utf-8-sig', 'gbk']:
        try:
            df = pd.read_csv(file_path, encoding=enc)
            cols = [str(c).lower() for c in df.columns]
            df.columns = cols
            
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c or 'è¯é¢‘' in c), cols[1] if len(cols)>1 else cols[0])
            
            df['w'] = df[w_col].astype(str).str.lower().str.strip()
            df['r'] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
            
            vocab = pd.Series(df['r'].values, index=df['w']).to_dict()
            return vocab, "åŠ è½½æˆåŠŸ (Raw)"
        except: continue
        
    return None, "åŠ è½½å¤±è´¥"

vocab_dict, msg = load_vocab()

# ==========================================
# 2. ç•Œé¢æ˜¾ç¤ºä¸çŠ¶æ€è‡ªæ£€
# ==========================================
st.title("ğŸ§  Vibe Vocab v10.0 (spaCy ç‰ˆ)")
st.caption("å·¥ä¸šçº§ NLP å¼•æ“ Â· å½»åº•è§£å†³å˜ä½“è¯†åˆ«é—®é¢˜")

if not vocab_dict:
    st.error(msg)
    st.stop()

# ä¾§è¾¹æ ï¼šçŠ¶æ€é¢æ¿
st.sidebar.success(NLP_STATUS)
st.sidebar.info(f"ğŸ“š è¯åº“: {msg}")

# === å…³é”®è‡ªæ£€åŒº ===
st.sidebar.markdown("---")
st.sidebar.markdown("**ğŸ” è¿˜åŸæµ‹è¯•:**")
test_words = ["are", "went", "families", "better", "running"]
for t in test_words:
    res = get_lemma_spacy(t)
    st.sidebar.text(f"{t} -> {res}")
st.sidebar.markdown("*(å¦‚æœ 'are' å˜æˆäº† 'be'ï¼Œè¯´æ˜æˆåŠŸï¼)*")
# ===================

vocab_range = st.sidebar.slider("å­¦ä¹ åŒºé—´", 1, 20000, (6000, 8000), 500)
r_start, r_end = vocab_range

# ==========================================
# 3. æ ¸å¿ƒå¤„ç†é€»è¾‘ (spaCy)
# ==========================================
def process_text(text):
    # ä½¿ç”¨ spaCy å¤„ç†æ•´ä¸ªæ–‡æœ¬ï¼Œå®ƒèƒ½æ ¹æ®ä¸Šä¸‹æ–‡æ›´ç²¾å‡†åœ°è¿˜åŸ
    doc = nlp(text.lower())
    
    # æå–å•è¯å¹¶å»é‡
    # token.is_alpha è¿‡æ»¤æ‰æ ‡ç‚¹å’Œæ•°å­—
    # token.lemma_ ç›´æ¥æ‹¿åˆ°è¿˜åŸåçš„è¯
    
    seen_lemmas = set()
    unique_items = []
    
    for token in doc:
        if token.is_alpha and len(token.text) > 1:
            lemma = token.lemma_.lower()
            original = token.text.lower()
            
            # æ’é™¤åœç”¨è¯(å¦‚ the, is, a)çš„å¹²æ‰°ï¼Œè¿™é‡Œæˆ‘ä»¬ä¾é è¯åº“æ’åæ¥è¿‡æ»¤
            # ä½† spaCy çš„ is_stop ä¹Ÿå¯ä»¥ç”¨ï¼Œä¸è¿‡æˆ‘ä»¬æš‚ä¸å¼€å¯ï¼Œå®Œå…¨ä¿¡ä»»è¯åº“æ’å
            
            if lemma not in seen_lemmas:
                unique_items.append((original, lemma))
                seen_lemmas.add(lemma)
    
    # æ’åºä»¥ä¾¿æŸ¥çœ‹
    unique_items.sort(key=lambda x: x[1])

    known, target, beyond = [], [], []
    
    for original, lemma in unique_items:
        rank = 99999
        match = lemma # é»˜è®¤ç”¨è¿˜åŸåçš„è¯å»æŸ¥
        note = ""

        # 1. æŸ¥è¿˜åŸåçš„è¯ (be, family, go)
        if lemma in vocab_dict:
            rank = vocab_dict[lemma]
            if original != lemma:
                note = f"<{original}>"
        # 2. å…œåº•æŸ¥åŸè¯ (æœ‰æ—¶å€™è¯åº“é‡Œæ”¶å½•çš„æ˜¯ families è€Œä¸æ˜¯ family)
        elif original in vocab_dict:
            r_orig = vocab_dict[original]
            if r_orig < rank:
                rank = r_orig
                match = original
                note = ""

        item = {'å•è¯': match, 'æ’å': int(rank), 'å¤‡æ³¨': note}
        
        if rank <= r_start: known.append(item)
        elif r_start < rank <= r_end: target.append(item)
        else: beyond.append(item)

    return pd.DataFrame(known), pd.DataFrame(target), pd.DataFrame(beyond)

# ==========================================
# 4. ä¸»ç•Œé¢
# ==========================================
text_input = st.text_area("åœ¨æ­¤ç²˜è´´æ–‡æœ¬:", height=150)

if st.button("ğŸš€ å¼€å§‹åˆ†æ (spaCy Powered)", type="primary"):
    if not text_input.strip():
        st.warning("è¯·è¾“å…¥å†…å®¹")
    else:
        with st.spinner("spaCy æ­£åœ¨æ·±åº¦åˆ†æ..."):
            df_k, df_t, df_b = process_text(text_input)
        
        st.success("åˆ†æå®Œæˆ")
        t1, t2, t3 = st.tabs([
            f"ğŸŸ¡ é‡ç‚¹ ({len(df_t)})", 
            f"ğŸ”´ è¶…çº² ({len(df_b)})", 
            f"ğŸŸ¢ ç†Ÿè¯ ({len(df_k)})"
        ])
        
        with t1: st.dataframe(df_t, use_container_width=True)
        with t2: st.dataframe(df_b, use_container_width=True)
        with t3: st.dataframe(df_k, use_container_width=True)