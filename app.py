import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import json

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode {
        font-family: 'Consolas', 'Courier New', monospace !important;
        font-size: 16px !important;
    }
    header {visibility: hidden;}
    footer {visibility: hidden;}
    .block-container { padding-top: 1rem; }
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    div[role="radiogroup"] > label {
        font-weight: bold;
        background-color: var(--secondary-background-color);
        color: var(--text-color);
        border: 1px solid var(--border-color-light);
        padding: 5px 15px;
        border-radius: 8px;
        margin-right: 10px;
    }
    div[role="radiogroup"] > label:hover {
        border-color: var(--primary-color);
        color: var(--primary-color);
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ•°æ®åŠ è½½ (Data Loading) - æ ¸å¿ƒä¼˜åŒ–
# ==========================================
@st.cache_data
def load_knowledge_base():
    """ä» JSON æ–‡ä»¶åŠ è½½é™æ€çŸ¥è¯†åº“ï¼Œæå¤§æå‡æ€§èƒ½"""
    try:
        # 1. æœ¯è¯­åº“
        with open('data/terms.json', 'r', encoding='utf-8') as f:
            terms = json.load(f)
        # 2. ä¸“æœ‰åè¯åº“
        with open('data/proper.json', 'r', encoding='utf-8') as f:
            proper = json.load(f)
        # 3. è¡¥ä¸è¯åº“
        with open('data/patch.json', 'r', encoding='utf-8') as f:
            patch = json.load(f)
        # 4. æ­§ä¹‰è¯ (åˆ—è¡¨è½¬é›†åˆ)
        with open('data/ambiguous.json', 'r', encoding='utf-8') as f:
            ambiguous = set(json.load(f))
            
        # ç¡®ä¿æœ¯è¯­ key å…¨å°å†™ï¼Œé˜²æ­¢åŒ¹é…å¤±è´¥
        terms = {k.lower(): v for k, v in terms.items()}
        proper = {k.lower(): v for k, v in proper.items()}
        
        return terms, proper, patch, ambiguous
    except FileNotFoundError:
        st.error("âš ï¸ ç¼ºå°‘æ•°æ®æ–‡ä»¶ï¼è¯·ç¡®ä¿ `data/` æ–‡ä»¶å¤¹ä¸‹åŒ…å« terms.json, proper.json, patch.json, ambiguous.json")
        return {}, {}, {}, set()

# å…¨å±€å˜é‡åŠ è½½
BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

# ==========================================
# 3. åˆå§‹åŒ– NLP
# ==========================================
@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    if not os.path.exists(nltk_data_dir):
        os.makedirs(nltk_data_dir)
    nltk.data.path.append(nltk_data_dir)
    for pkg in ['averaged_perceptron_tagger', 'punkt']:
        try: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
        except: pass

setup_nltk()

def smart_lemmatize(text):
    words = re.findall(r"[a-zA-Z']+", text)
    results = []
    for w in words:
        lemmas_dict = lemminflect.getAllLemmas(w)
        if not lemmas_dict:
            results.append(w.lower())
            continue
        if 'ADJ' in lemmas_dict: lemma = lemmas_dict['ADJ'][0]
        elif 'ADV' in lemmas_dict: lemma = lemmas_dict['ADV'][0]
        elif 'VERB' in lemmas_dict: lemma = lemmas_dict['VERB'][0]
        elif 'NOUN' in lemmas_dict: lemma = lemmas_dict['NOUN'][0]
        else: lemma = list(lemmas_dict.values())[0][0]
        results.append(lemma)
    return " ".join(results)

# ==========================================
# 4. è¯åº“åŠ è½½ (CSV)
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv"]

@st.cache_data
def load_vocab():
    vocab = {}
    file_path = next((f for f in POSSIBLE_FILES if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
            df = df.sort_values(r_col, ascending=True)
            df = df.drop_duplicates(subset=[w_col], keep='first')
            vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except: pass
    
    # æ³¨å…¥ JSON åŠ è½½çš„è¡¥ä¸
    for word, rank in BUILTIN_PATCH_VOCAB.items():
        if word not in vocab: vocab[word] = rank
        else:
            if vocab[word] > 20000: vocab[word] = rank
    return vocab

vocab_dict = load_vocab()

# ==========================================
# 5. AI æŒ‡ä»¤ç”Ÿæˆå™¨
# ==========================================
def generate_ai_prompt(word_list, output_format, def_mode="single", is_term_list=False):
    words_str = ", ".join(word_list)
    
    definition_instruction = ""
    if is_term_list or def_mode == "term":
        definition_instruction = "- **é¢†åŸŸé”å®š**ï¼šå•è¯å¸¦æœ‰ (Domain) æ ‡ç­¾ï¼Œ**å¿…é¡»**ä»…æä¾›ç¬¦åˆè¯¥é¢†åŸŸèƒŒæ™¯çš„ä¸“ä¸šé‡Šä¹‰ã€‚"
    elif def_mode == "split":
        definition_instruction = """- **ç†Ÿè¯æ·±æŒ– (Polymsey Splitting)**ï¼šè¿™äº›æ˜¯é«˜é¢‘å¸¸ç”¨è¯ï¼Œä¸ºäº†æŒæ¡å…¶ä¸åŒç”¨æ³•ï¼Œ**è¯·å°†ä¸åŒçš„å«ä¹‰æ‹†åˆ†ä¸ºå¤šæ¡ç‹¬ç«‹çš„æ•°æ®ï¼ˆå¤šå¼ å¡ç‰‡ï¼‰**ã€‚
    - ä¾‹å¦‚ 'fair' åº”æ‹†åˆ†ä¸ºï¼š
      1. fair (adj) - reasonable/impartial (å…¬å¹³çš„)
      2. fair (n) - gathering/market (é›†å¸‚)
    - ä¸è¦æŠŠæ‰€æœ‰æ„æ€æŒ¤åœ¨ä¸€å¼ å¡ç‰‡é‡Œã€‚"""
    else: # single
        definition_instruction = "- **æç®€é€Ÿè®° (Minimalist)**ï¼šè¿™äº›æ˜¯ç”Ÿè¯ï¼Œè¯·**ä»…æä¾› 1 ä¸ªæœ€æ ¸å¿ƒã€æœ€å¸¸ç”¨çš„é‡Šä¹‰**ã€‚ä¸¥ç¦ç½—åˆ—å¤šä¸ªä¹‰é¡¹ï¼Œå‡è½»è®°å¿†è´Ÿæ‹…ã€‚"

    if output_format == 'csv':
        format_req = "CSV Code Block (åç¼€å .csv)"
        format_desc = "è¯·ç›´æ¥è¾“å‡ºæ ‡å‡† CSV ä»£ç å—ã€‚"
    else:
        format_req = "TXT Code Block (åç¼€å .txt)"
        format_desc = "è¯·è¾“å‡ºçº¯æ–‡æœ¬ TXT ä»£ç å—ã€‚"

    prompt = f"""
è¯·æ‰®æ¼”ä¸€ä½ä¸“ä¸šçš„ Anki åˆ¶å¡ä¸“å®¶ã€‚è¿™æ˜¯æˆ‘æ•´ç†çš„å•è¯åˆ—è¡¨ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ã€é‡Šä¹‰ç­–ç•¥ã€‘ä¸ºæˆ‘ç”Ÿæˆå¯¼å…¥æ–‡ä»¶ã€‚

1. æ ¸å¿ƒåŸåˆ™ï¼šé‡Šä¹‰ç­–ç•¥
{definition_instruction}

2. å¡ç‰‡æ­£é¢ (Column 1: Front)
- å†…å®¹ï¼šæä¾›è‡ªç„¶çš„çŸ­è¯­æˆ–æ­é… (Phrase/Collocation)ã€‚
- æ ·å¼ï¼šçº¯æ–‡æœ¬ã€‚

3. å¡ç‰‡èƒŒé¢ (Column 2: Back)
- æ ¼å¼ï¼šHTML æ’ç‰ˆï¼ŒåŒ…å«ä¸‰éƒ¨åˆ†ï¼Œå¿…é¡»ä½¿ç”¨ <br><br> åˆ†éš”ã€‚
- ç»“æ„ï¼šè‹±æ–‡é‡Šä¹‰<br><br><em>æ–œä½“ä¾‹å¥</em><br><br>ã€è¯æº/è¯æ ¹è¯ç¼€ã€‘ä¸­æ–‡åŠ©è®° (è¯æºä¼˜å…ˆ)

4. è¾“å‡ºæ ¼å¼æ ‡å‡† ({format_req})
- {format_desc}
- å…³é”®æ ¼å¼ï¼šä½¿ç”¨è‹±æ–‡é€—å· (,) åˆ†éš”ï¼Œä¸”æ¯ä¸ªå­—æ®µå†…å®¹å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å· ("...") åŒ…è£¹ã€‚

å¾…å¤„ç†å•è¯ï¼š
{words_str}
"""
    return prompt

# ==========================================
# 6. é€šç”¨åˆ†æå‡½æ•°
# ==========================================
def analyze_text(raw_text, mode="auto"):
    raw_items = []
    if "æŒ‰è¡Œ" in mode:
        lines = raw_text.split('\n')
        for line in lines:
            if line.strip(): raw_items.append(line.strip())
    else:
        clean_text = re.sub(r'[,.\n\t]', ' ', raw_text)
        raw_items = clean_text.split()
    
    seen = set()
    unique_items = [] 
    JUNK_WORDS = {'s', 't', 'd', 'm', 'll', 've', 're'}
    
    for item in raw_items:
        item_cleaned = item.strip()
        item_lower = item_cleaned.lower()
        
        if item_lower in seen: continue
        if len(item_lower) < 2 and item_lower not in ['a', 'i']: continue
        if item_lower in JUNK_WORDS: continue
        
        # 1. æœ¯è¯­èº«ä»½
        if item_lower in BUILTIN_TECHNICAL_TERMS:
            domain = BUILTIN_TECHNICAL_TERMS[item_lower]
            unique_items.append({
                "word": f"{item_cleaned} ({domain})", 
                "rank": 0,
                "cat": "term",
                "raw": item_lower
            })
        
        # 2. ä¸“åèº«ä»½ (Rank 1, æ–¹ä¾¿è¿‡æ»¤)
        if item_lower in PROPER_NOUNS_DB or item_lower in AMBIGUOUS_WORDS:
            display = PROPER_NOUNS_DB.get(item_lower, item_cleaned.title())
            unique_items.append({
                "word": display,
                "rank": 1, 
                "cat": "proper",
                "raw": item_lower
            })
            
        # 3. æ™®é€šèº«ä»½
        rank = vocab_dict.get(item_lower, 99999)
        if rank != 99999:
            unique_items.append({
                "word": item_cleaned,
                "rank": rank,
                "cat": "general",
                "raw": item_lower
            })
        
        seen.add(item_lower)
        
    return pd.DataFrame(unique_items)

# ==========================================
# 7. ç•Œé¢å¸ƒå±€
# ==========================================
st.title("ğŸš€ Vocab Master Pro")

app_mode = st.radio("é€‰æ‹©åŠŸèƒ½æ¨¡å¼:", 
    ["ğŸ› ï¸ æ™ºèƒ½è¿˜åŸ", "ğŸ“Š å•è¯åˆ†çº§ (å…¨é‡)", "ğŸ¯ æ™ºèƒ½ç²¾é€‰ (Top N)"], 
    horizontal=True
)
st.divider()

# ---------------------------------------------------------
# æ¨¡å¼ A: æ™ºèƒ½è¿˜åŸ
# ---------------------------------------------------------
if "æ™ºèƒ½è¿˜åŸ" in app_mode:
    c1, c2 = st.columns(2)
    with c1:
        raw_text = st.text_area("è¾“å…¥åŸå§‹æ–‡ç« ", height=400, placeholder="He was excited.")
        if st.button("å¼€å§‹è¿˜åŸ", type="primary"):
            res = smart_lemmatize(raw_text)
            st.code(res, language='text')
            st.caption("ğŸ‘† ä¸€é”®å¤åˆ¶")

# ---------------------------------------------------------
# æ¨¡å¼ B: å•è¯åˆ†çº§ (å…¨é‡)
# ---------------------------------------------------------
elif "å•è¯åˆ†çº§" in app_mode:
    col_level1, col_level2, _ = st.columns([1, 1, 2])
    with col_level1: current_level = st.number_input("å½“å‰æ°´å¹³", 0, 30000, 9000, 500)
    with col_level2: target_level = st.number_input("ç›®æ ‡æ°´å¹³", 0, 30000, 15000, 500)
    
    g_col1, g_col2 = st.columns(2)
    with g_col1:
        input_mode = st.radio("è¯†åˆ«æ¨¡å¼:", ("è‡ªåŠ¨åˆ†è¯", "æŒ‰è¡Œå¤„ç†"), horizontal=True)
        grade_input = st.text_area("input_box", height=400, placeholder="motion\nenergy\nrun\nset", label_visibility="collapsed")
        btn_grade = st.button("å¼€å§‹åˆ†çº§", type="primary", use_container_width=True)

    with g_col2:
        if btn_grade and grade_input and vocab_dict:
            df = analyze_text(grade_input, input_mode)
            if not df.empty:
                def categorize(row):
                    if row['cat'] == 'term': return 'term'
                    if row['cat'] == 'proper': return 'proper'
                    r = row['rank']
                    if r <= current_level: return "known"
                    elif r <= target_level: return "target"
                    else: return "beyond"
                
                df['final_cat'] = df.apply(categorize, axis=1)
                df = df.sort_values(by='rank')

                t1, t2, t3, t4, t5 = st.tabs(["ğŸŸ£ ä¸“ä¸šæœ¯è¯­", "ğŸŸ¡ é‡ç‚¹", "ğŸ”µ ä¸“æœ‰åè¯", "ğŸ”´ è¶…çº²", "ğŸŸ¢ å·²æŒæ¡"])
                
                def render_tab(tab_obj, cat_key, label, def_mode):
                    with tab_obj:
                        sub = df[df['final_cat'] == cat_key]
                        st.caption(f"å…± {len(sub)} ä¸ª")
                        if not sub.empty:
                            words = sub['word'].tolist()
                            with st.expander("ğŸ‘ï¸ æŸ¥çœ‹åˆ—è¡¨", expanded=False): st.code("\n".join(words))
                            
                            st.markdown(f"**ğŸ¤– AI æŒ‡ä»¤ ({label})**")
                            has_term = (cat_key == 'term')
                            
                            p_csv = generate_ai_prompt(words, 'csv', def_mode, is_term_list=has_term)
                            p_txt = generate_ai_prompt(words, 'txt', def_mode, is_term_list=has_term)
                            
                            t_csv, t_txt = st.tabs(["ğŸ“‹ CSV æŒ‡ä»¤", "ğŸ“ TXT æŒ‡ä»¤"])
                            with t_csv: st.code(p_csv, language='markdown')
                            with t_txt: st.code(p_txt, language='markdown')
                        else: st.info("æ— ")

                render_tab(t1, "term", "æœ¯è¯­", def_mode="term")   
                render_tab(t2, "target", "é‡ç‚¹", def_mode="single") 
                render_tab(t3, "proper", "ä¸“å", def_mode="single")
                render_tab(t4, "beyond", "è¶…çº²", def_mode="single") 
                render_tab(t5, "known", "ç†Ÿè¯", def_mode="split")  

# ---------------------------------------------------------
# æ¨¡å¼ C: æ™ºèƒ½ç²¾é€‰ (Top N)
# ---------------------------------------------------------
elif "Top N" in app_mode:
    st.info("ğŸ’¡ æ­¤æ¨¡å¼è‡ªåŠ¨è¿‡æ»¤ç®€å•è¯ï¼ŒæŒ‰ **ç”±æ˜“åˆ°éš¾** æŒ‘é€‰ã€‚")
    
    c_set1, c_set2, c_set3 = st.columns([1, 1, 1])
    with c_set1: top_n = st.number_input("ğŸ¯ ç­›é€‰æ•°é‡", 10, 500, 50, 10)
    with c_set2: min_rank_threshold = st.number_input("ğŸ“‰ å¿½ç•¥å‰ N è¯", 0, 20000, 3000, 500)
    with c_set3: st.write("") 
        
    c_input, c_btn = st.columns([3, 1])
    with c_input:
        topn_input = st.text_area("è¾“å…¥", height=150, placeholder="motion\nenergy\nrun", label_visibility="collapsed")
    with c_btn:
        btn_topn = st.button("ğŸ² ç”Ÿæˆç²¾é€‰", type="primary", use_container_width=True)

    if btn_topn and topn_input and vocab_dict:
        df = analyze_text(topn_input, "è‡ªåŠ¨åˆ†è¯") 
        
        if not df.empty:
            df['rank'] = pd.to_numeric(df['rank'], errors='coerce').fillna(99999)
            
            term_mask = (df['cat'] == 'term')
            general_mask = (df['cat'].isin(['general', 'proper'])) & (df['rank'] >= min_rank_threshold)
            
            valid_candidates = df[term_mask | general_mask].copy()
            sorted_df = valid_candidates.sort_values(by='rank', ascending=True)
            top_df = sorted_df.head(top_n)
            
            all_ids = set(df.index)
            top_ids = set(top_df.index)
            rest_ids = all_ids - top_ids
            rest_df = df.loc[list(rest_ids)].sort_values(by='rank')
            
            st.divider()
            col_win, col_rest = st.columns(2)
            
            # === å·¦æ  ===
            with col_win:
                st.success(f"ğŸ”¥ ç²¾é€‰ Top {len(top_df)}")
                if not top_df.empty:
                    words = top_df['word'].tolist()
                    with st.expander("åˆ—è¡¨", expanded=True): st.code("\n".join(words))
                    
                    st.markdown("**ğŸ¤– AI æŒ‡ä»¤ (æ ¸å¿ƒå•ä¹‰)**")
                    has_term = any('(' in w for w in words)
                    mode = "single" if not has_term else "term"
                    
                    p_csv = generate_ai_prompt(words, 'csv', mode, is_term_list=has_term)
                    p_txt = generate_ai_prompt(words, 'txt', mode, is_term_list=has_term)
                    
                    t1, t2 = st.tabs(["CSV", "TXT"])
                    with t1: st.code(p_csv, language='markdown')
                    with t2: st.code(p_txt, language='markdown')
                else: st.warning("æ— ")

            # === å³æ  ===
            with col_rest:
                st.subheader(f"ğŸ’¤ å‰©ä½™ {len(rest_df)} ä¸ª")
                if not rest_df.empty:
                    words_rest = rest_df['word'].tolist()
                    with st.expander("åˆ—è¡¨", expanded=False): st.code("\n".join(words_rest))
                    
                    st.markdown("**ğŸ¤– AI æŒ‡ä»¤ (å¤‡ç”¨)**")
                    p_csv_r = generate_ai_prompt(words_rest, 'csv', "single")
                    p_txt_r = generate_ai_prompt(words_rest, 'txt', "single")
                    
                    rt1, rt2 = st.tabs(["CSV", "TXT"])
                    with rt1: st.code(p_csv_r, language='markdown')
                    with rt2: st.code(p_txt_r, language='markdown')