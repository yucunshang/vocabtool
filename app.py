import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import json
import time
import requests
import zipfile
import concurrent.futures

# å°è¯•å¯¼å…¥å¤šæ ¼å¼æ–‡æ¡£å¤„ç†åº“
try:
    import PyPDF2
    import docx
except ImportError:
    pass  # ç¨ååœ¨ä½¿ç”¨æ—¶æç¤ºï¼Œä¸é˜»æ–­ä¸»ç¨‹åºå¯åŠ¨

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro V5", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode { font-family: 'Consolas', 'Courier New', monospace !important; font-size: 16px !important; }
    header {visibility: hidden;} footer {visibility: hidden;}
    .block-container { padding-top: 1rem; }
    [data-testid="stMetricValue"] { font-size: 28px !important; color: #007bff !important; }
    .param-box { background-color: #f8f9fa; padding: 20px; border-radius: 10px; border: 1px solid #e9ecef; margin-bottom: 20px; }
    .copy-hint { color: #888; font-size: 14px; margin-bottom: 5px; margin-top: 10px; padding-left: 5px; }
    div[data-testid="stExpander"] div[role="button"] p { font-size: 1.1rem; font-weight: 600; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ•°æ®ä¸ NLP åˆå§‹åŒ– (å¥å£®ç‰ˆ)
# ==========================================
@st.cache_data
def load_knowledge_base():
    """åŠ è½½æœ¬åœ°çŸ¥è¯†åº“ï¼Œå…·æœ‰å®¹é”™èƒ½åŠ›"""
    data = {"terms": {}, "proper": {}, "patch": {}, "ambiguous": set()}
    try:
        if os.path.exists('data/terms.json'):
            with open('data/terms.json', 'r', encoding='utf-8') as f: data["terms"] = {k.lower(): v for k, v in json.load(f).items()}
        if os.path.exists('data/proper.json'):
            with open('data/proper.json', 'r', encoding='utf-8') as f: data["proper"] = {k.lower(): v for k, v in json.load(f).items()}
        if os.path.exists('data/patch.json'):
            with open('data/patch.json', 'r', encoding='utf-8') as f: data["patch"] = json.load(f)
        if os.path.exists('data/ambiguous.json'):
            with open('data/ambiguous.json', 'r', encoding='utf-8') as f: data["ambiguous"] = set(json.load(f))
    except Exception as e:
        st.error(f"âš ï¸ æ•°æ®åŠ è½½éƒ¨åˆ†å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å¼è¿è¡Œã€‚")
    return data["terms"], data["proper"], data["patch"], data["ambiguous"]

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

@st.cache_resource
def setup_nltk():
    """NLTK æ•°æ®ä¸‹è½½æ£€æŸ¥"""
    try:
        nltk.data.find('corpora/wordnet')
    except LookupError:
        root_dir = os.path.dirname(os.path.abspath(__file__))
        nltk_data_dir = os.path.join(root_dir, 'nltk_data')
        os.makedirs(nltk_data_dir, exist_ok=True)
        nltk.data.path.append(nltk_data_dir)
        for pkg in ['averaged_perceptron_tagger', 'punkt', 'wordnet']:
            try: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
            except: pass

setup_nltk()

def get_lemma(w):
    """è¯å½¢è¿˜åŸå°è£…"""
    if not w: return ""
    try:
        lemmas_dict = lemminflect.getAllLemmas(w)
        if not lemmas_dict: return w.lower()
        # ä¼˜å…ˆé¡ºåºï¼šåŠ¨è¯ -> åè¯ -> å½¢å®¹è¯ -> å‰¯è¯
        for pos in ['VERB', 'NOUN', 'ADJ', 'ADV']:
            if pos in lemmas_dict: return lemmas_dict[pos][0]
        return list(lemmas_dict.values())[0][0]
    except:
        return w.lower()

@st.cache_data
def load_vocab():
    """åŠ è½½è¯é¢‘è¡¨ï¼Œå…·å¤‡å›é€€æœºåˆ¶"""
    vocab = {}
    # å°è¯•åŠ è½½ CSV
    file_path = next((f for f in ["coca_cleaned.csv", "data.csv"] if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
            df = df.sort_values(r_col, ascending=True).drop_duplicates(subset=[w_col], keep='first')
            vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except Exception:
            pass # å¤±è´¥åˆ™ä¾é å†…ç½®è¡¥ä¸
    
    # åˆå¹¶è¡¥ä¸è¯åº“
    if BUILTIN_PATCH_VOCAB:
        for word, rank in BUILTIN_PATCH_VOCAB.items(): vocab[word] = rank
        
    # ç´§æ€¥ç¡¬ç¼–ç è¦†ç›– (Common Overrides)
    URGENT_OVERRIDES = {
        "china": 400, "usa": 200, "uk": 200, "google": 1000, "apple": 1000, 
        "january": 400, "february": 400, "march": 400, "april": 400, "may": 100, "june": 400,
        "monday": 300, "sunday": 300
    }
    for word, rank in URGENT_OVERRIDES.items(): vocab[word] = rank
    return vocab

vocab_dict = load_vocab()

# ==========================================
# 3. æ–‡æ¡£è§£æå¼•æ“
# ==========================================
def extract_text_from_file(uploaded_file):
    ext = uploaded_file.name.split('.')[-1].lower()
    uploaded_file.seek(0)
    try:
        if ext == 'txt':
            return uploaded_file.getvalue().decode("utf-8", errors="ignore")
        elif ext == 'pdf':
            reader = PyPDF2.PdfReader(uploaded_file)
            return " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif ext == 'docx':
            doc = docx.Document(uploaded_file)
            return " ".join([p.text for p in doc.paragraphs])
        elif ext == 'epub':
            text_blocks = []
            with zipfile.ZipFile(uploaded_file) as z:
                for filename in z.namelist():
                    if filename.endswith(('.html', '.xhtml', '.htm', '.xml')):
                        try:
                            content = z.read(filename).decode('utf-8', errors='ignore')
                            clean_text = re.sub(r'<[^>]+>', ' ', content)
                            text_blocks.append(clean_text)
                        except: pass
            return " ".join(text_blocks)
    except Exception as e:
        return f" [è§£æé”™è¯¯: {e}] "
    return ""

def get_base_prompt_template(export_format="TXT"):
    return f"""ã€è§’è‰²è®¾å®šã€‘ ä½ æ˜¯ä¸€ä½ç²¾é€šè¯æºå­¦ã€è®¤çŸ¥å¿ƒç†å­¦ä»¥åŠ Anki ç®—æ³•çš„â€œè‹±è¯­è¯æ±‡ä¸“å®¶ä¸é—ªå¡åˆ¶ä½œå¤§å¸ˆâ€ã€‚è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹æ ‡å‡†ï¼š

1. æ ¸å¿ƒåŸåˆ™ï¼šåŸå­æ€§ (Atomicity)
è‹¥ä¸€ä¸ªå•è¯æœ‰å¤šä¸ªå¸¸ç”¨å«ä¹‰ï¼Œå¿…é¡»æ‹†åˆ†ä¸ºå¤šæ¡ç‹¬ç«‹æ•°æ®ã€‚
2. å¡ç‰‡æ­£é¢ (Column 1)
æä¾›è‡ªç„¶çš„çŸ­è¯­æˆ–æ­é… (Phrase/Collocation)ã€‚
3. å¡ç‰‡èƒŒé¢ (Column 2 - æ•´åˆé¡µ)
ä½¿ç”¨ HTML æ ‡ç­¾æ’ç‰ˆï¼ŒåŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼Œç”¨ <br><br> åˆ†éš”ï¼š
è‹±æ–‡é‡Šä¹‰ <br><br> <em>æ–œä½“ä¾‹å¥</em> <br><br> ã€ä¸­æ–‡è¯æº/è®°å¿†æ³•ã€‘
4. è¾“å‡ºæ ¼å¼æ ‡å‡† ({export_format} æ ¼å¼)
çº¯æ–‡æœ¬ä»£ç å—ï¼Œæ—  Markdown åŒ…è£¹ã€‚
é€—å·åˆ†éš”ï¼Œå­—æ®µç”¨åŒå¼•å·åŒ…è£¹ã€‚
5. æ•°æ®æ¸…æ´—
è‡ªåŠ¨ä¿®æ­£æ‹¼å†™é”™è¯¯ã€‚

ğŸ’¡ æœ€ç»ˆè¾“å‡ºç¤ºä¾‹ï¼ˆ{export_format} å†…å®¹ï¼‰ï¼š
"run a business","to manage a company<br><br><em>He quit his job to run a business.</em><br><br>ã€è¯æºã€‘run æºè‡ªå¤è‹±è¯­ rinnanï¼ˆæµåŠ¨/è¿è½¬ï¼‰"
"""

# ==========================================
# 4. å¤šæ ¸å¹¶å‘ API å¼•æ“ (çº¿ç¨‹å®‰å…¨ç‰ˆ)
# ==========================================
def _fetch_deepseek_chunk_safe(batch_data):
    """
    çº¯å‡½æ•°ï¼Œä¸æ“ä½œ UIã€‚
    batch_data: (index, words_list, prompt_template, api_key)
    Return: (index, result_string, error_msg)
    """
    index, batch_words, prompt_template, api_key = batch_data
    
    url = "https://api.deepseek.com/chat/completions"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    system_enforcement = "\n\nã€ç³»ç»Ÿç»å¯¹å¼ºåˆ¶æŒ‡ä»¤ã€‘ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„æ•°æ®ä»£ç ï¼Œä¸è¦å›å¤â€œå¥½çš„â€ï¼Œä¸è¦ä½¿ç”¨ ```csv åŒ…è£¹ï¼"
    full_prompt = f"{prompt_template}{system_enforcement}\n\nå¾…å¤„ç†å•è¯åˆ—è¡¨ï¼š\n{', '.join(batch_words)}"
    
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": full_prompt}],
        "temperature": 0.3,
        "max_tokens": 4096
    }
    
    try:
        for attempt in range(3):
            resp = requests.post(url, json=payload, headers=headers, timeout=60)
            if resp.status_code == 429: 
                time.sleep(2 * (attempt + 1))
                continue
            if resp.status_code == 402: return (index, "", "ERROR_402_NO_BALANCE")
            elif resp.status_code == 401: return (index, "", "ERROR_401_INVALID_KEY")
            
            resp.raise_for_status()
            result = resp.json()['choices'][0]['message']['content'].strip()
            
            # æ¸…æ´— Markdown æ ‡è®°
            if result.startswith("```"):
                lines = result.split('\n')
                if lines[0].startswith("```"): lines = lines[1:]
                if lines and lines[-1].startswith("```"): lines = lines[:-1]
                result = '\n'.join(lines).strip()
            return (index, result, None)
            
        return (index, "", "TIMEOUT")
    except Exception as e:
        return (index, "", str(e))

def run_concurrent_api(words, prompt_template, api_key, progress_bar, status_text):
    """ä¸»çº¿ç¨‹æ§åˆ¶è¿›åº¦çš„å¹¶å‘å™¨"""
    MAX_WORDS = 300 # é™åˆ¶å•æ¬¡è¯·æ±‚é‡
    if len(words) > MAX_WORDS:
        st.warning(f"âš ï¸ ä¸ºä¿è¯å¹¶å‘ç¨³å®šï¼Œæœ¬æ¬¡ä»…æˆªå–å‰ **{MAX_WORDS}** ä¸ªå•è¯ã€‚")
        words = words[:MAX_WORDS]

    CHUNK_SIZE = 30
    chunks = [words[i:i + CHUNK_SIZE] for i in range(0, len(words), CHUNK_SIZE)]
    total_chunks = len(chunks)
    
    # å‡†å¤‡ä»»åŠ¡æ•°æ®
    tasks = [(i, chunk, prompt_template, api_key) for i, chunk in enumerate(chunks)]
    results_map = {}
    errors = []
    
    status_text.markdown("ğŸš€ **å¹¶å‘ä»»åŠ¡å·²å‘å°„ï¼** æ­£åœ¨å…¨é€Ÿç”Ÿæˆ...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_idx = {executor.submit(_fetch_deepseek_chunk_safe, task): task[0] for task in tasks}
        
        completed_count = 0
        for future in concurrent.futures.as_completed(future_to_idx):
            idx, res_str, err = future.result()
            
            if err:
                if "402" in err: return "âŒ é”™è¯¯ï¼šDeepSeek è´¦æˆ·ä½™é¢ä¸è¶³ã€‚"
                if "401" in err: return "âŒ é”™è¯¯ï¼šAPI Key æ— æ•ˆã€‚"
                errors.append(f"æ‰¹æ¬¡ {idx} å¤±è´¥: {err}")
            else:
                results_map[idx] = res_str
            
            completed_count += 1
            progress = completed_count / total_chunks
            progress_bar.progress(progress)
            status_text.markdown(f"**âš¡ AI å¤šæ ¸å¹¶å‘å…¨é€Ÿç¼–çº‚ä¸­ï¼š** `{completed_count}/{total_chunks}` æ‰¹æ¬¡å®Œæˆ")

    # æŒ‰åŸå§‹é¡ºåºæ‹¼æ¥
    final_output = []
    for i in range(total_chunks):
        if i in results_map:
            final_output.append(results_map[i])
    
    if errors:
        st.warning(f"âš ï¸ éƒ¨åˆ†æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥ ({len(errors)}ä¸ª)ï¼Œå·²è‡ªåŠ¨è·³è¿‡ã€‚")
        
    return "\n".join(final_output)

# ==========================================
# 5. åˆ†æå¼•æ“
# ==========================================
def analyze_words(unique_word_list):
    unique_items = [] 
    # åŸºç¡€åœç”¨è¯è¿‡æ»¤ (ç®€å•ç‰ˆ)
    STOP_WORDS = {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but', 'his', 'by', 'from', 'they', 'we', 'say', 'her', 'she', 'or', 'an', 'will', 'my', 'one', 'all', 'would', 'there', 'their', 'what', 'so', 'up', 'out', 'if', 'about', 'who', 'get', 'which', 'go', 'me'}
    
    for item_lower in unique_word_list:
        if len(item_lower) < 2 and item_lower != 'a': continue
        if item_lower in STOP_WORDS: continue
        
        actual_rank = vocab_dict.get(item_lower, 99999)
        
        if item_lower in BUILTIN_TECHNICAL_TERMS:
            domain = BUILTIN_TECHNICAL_TERMS[item_lower]
            term_rank = actual_rank if actual_rank != 99999 else 15000
            unique_items.append({"word": f"{item_lower} ({domain})", "rank": term_rank, "raw": item_lower})
            continue
        
        # ç®€å•è¿‡æ»¤çº¯æ•°å­—
        if item_lower.isdigit(): continue

        if actual_rank != 99999:
            unique_items.append({"word": item_lower, "rank": actual_rank, "raw": item_lower})
        elif item_lower in PROPER_NOUNS_DB: # ä¸“æœ‰åè¯å³ä½¿æ²¡æ’åä¹Ÿä¿ç•™
             unique_items.append({"word": PROPER_NOUNS_DB[item_lower], "rank": 99999, "raw": item_lower})
            
    return pd.DataFrame(unique_items)

# ==========================================
# 6. UI ä¸ çŠ¶æ€ç®¡ç†
# ==========================================
st.title("ğŸš€ Vocab Master Pro - V5")

# åˆå§‹åŒ– Session State
if "raw_input_text" not in st.session_state: st.session_state.raw_input_text = ""
if "uploader_key" not in st.session_state: st.session_state.uploader_key = 0 
if "is_processed" not in st.session_state: st.session_state.is_processed = False
if "generated_cards" not in st.session_state: st.session_state.generated_cards = {} # ç”¨äºå­˜å‚¨ AI ç”Ÿæˆç»“æœ

def clear_all_inputs():
    st.session_state.raw_input_text = ""
    st.session_state.uploader_key += 1 
    st.session_state.is_processed = False
    st.session_state.generated_cards = {} # æ¸…ç©ºç”Ÿæˆè®°å½•

# --- é¡¶éƒ¨é…ç½®é¢æ¿ (æ›¿ä»£ä¾§è¾¹æ ) ---
with st.expander("âš™ï¸ **å‚æ•°é…ç½®ä¸ API è®¾ç½®** (ç‚¹å‡»å±•å¼€)", expanded=False):
    col_k1, col_k2 = st.columns([1, 2])
    with col_k1:
        # å°è¯•è‡ªåŠ¨è·å– Secrets
        default_key = ""
        try: default_key = st.secrets["DEEPSEEK_API_KEY"]
        except: pass
        user_api_key = st.text_input("ğŸ”‘ DeepSeek API Key", value=default_key, type="password", help="å¦‚æœæ²¡æœ‰é…ç½® Secretsï¼Œè¯·åœ¨æ­¤å¤„è¾“å…¥")
    
    with col_k2:
        st.info("ğŸ’¡ å‚æ•°è¯´æ˜ï¼š**å¿½ç•¥å‰ N è¯** å¯è¿‡æ»¤æ‰ too, the ç­‰ç®€å•è¯ï¼›**Top N** é€‰å–æœ€é«˜é¢‘ç”Ÿè¯ã€‚")

    c1, c2, c3, c4 = st.columns(4)
    with c1: current_level = st.number_input("ğŸ¯ å½“å‰è¯æ±‡é‡ (èµ·)", 0, 30000, 4500, 500)
    with c2: target_level = st.number_input("ğŸ¯ ç›®æ ‡è¯æ±‡é‡ (æ­¢)", 0, 30000, 15000, 500)
    with c3: top_n = st.number_input("ğŸ”¥ ç²¾é€‰ Top N", 10, 500, 50, 10)
    with c4: min_rank_threshold = st.number_input("ğŸ“‰ å¿½ç•¥å‰ N è¯", 0, 20000, 1000, 500)
    
    show_rank = st.checkbox("ğŸ”¢ åœ¨åˆ—è¡¨ä¸­æ˜¾ç¤ºè¯é¢‘ Rank", value=True)

# --- è¾“å…¥åŒº ---
col_input1, col_input2 = st.columns([3, 2])
with col_input1:
    raw_text = st.text_area("ğŸ“¥ ç²˜è´´æ–‡æœ¬ / è¯è¡¨", height=150, key="raw_input_text", placeholder="åœ¨æ­¤ç²˜è´´è‹±æ–‡æ–‡ç« ã€è®ºæ–‡æˆ–å•è¯åˆ—è¡¨...")
with col_input2:
    st.markdown("#### ğŸ“‚ æ–‡æ¡£è§£æ")
    uploaded_file = st.file_uploader("æ”¯æŒ TXT, PDF, DOCX, EPUB", type=["txt", "pdf", "docx", "epub"], key=f"uploader_{st.session_state.uploader_key}")

col_btn1, col_btn2 = st.columns([5, 1])
with col_btn1: btn_process = st.button("ğŸš€ å¼€å§‹æé€Ÿè§£æ", type="primary", use_container_width=True)
with col_btn2: st.button("ğŸ—‘ï¸ æ¸…ç©º", on_click=clear_all_inputs, use_container_width=True)

st.divider()

# ==========================================
# 7. æ ¸å¿ƒå¤„ç†é€»è¾‘
# ==========================================
if btn_process:
    with st.spinner("ğŸ§  æ­£åœ¨è¿›è¡Œæ–‡æœ¬æ¸…æ´—ä¸è¯æºåˆ†æ..."):
        start_time = time.time()
        combined_text = raw_text
        if uploaded_file is not None: combined_text += "\n" + extract_text_from_file(uploaded_file)
            
        if not combined_text.strip():
            st.warning("âš ï¸ æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æœ¬ï¼")
            st.session_state.is_processed = False
        else:
            # ä¼˜åŒ–æ­£åˆ™ï¼šä¿ç•™è¿å­—ç¬¦å•è¯ (state-of-the-art)
            raw_words = re.findall(r"[a-zA-Z]+(?:[-'][a-zA-Z]+)*", combined_text)
            
            # è¯å½¢è¿˜åŸ
            lemmatized_words = [get_lemma(w) for w in raw_words]
            full_lemmatized_text = " ".join(lemmatized_words)
            unique_lemmas = list(set([w.lower() for w in lemmatized_words]))
            
            # è¯é¢‘åˆ†æ
            st.session_state.base_df = analyze_words(unique_lemmas)
            st.session_state.lemma_text = full_lemmatized_text
            st.session_state.stats = {
                "raw_count": len(raw_words),
                "unique_count": len(unique_lemmas),
                "valid_count": len(st.session_state.base_df),
                "time": time.time() - start_time
            }
            st.session_state.is_processed = True
            # æ–°åˆ†ææ—¶é‡ç½®ç”Ÿæˆç»“æœ
            st.session_state.generated_cards = {} 

# ==========================================
# 8. ç»“æœæ¸²æŸ“
# ==========================================
if st.session_state.get("is_processed", False):
    
    stats = st.session_state.stats
    # ä½¿ç”¨å®¹å™¨ç¾åŒ– Metrics
    with st.container():
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("ğŸ“ æ€»è¯æ•°", f"{stats['raw_count']:,}")
        c2.metric("âœ‚ï¸ å»é‡è¯æ±‡", f"{stats['unique_count']:,}")
        c3.metric("ğŸ¯ æœ‰æ•ˆå­¦ä¹ è¯", f"{stats['valid_count']:,}")
        c4.metric("âš¡ è€—æ—¶", f"{stats['time']:.2f}s")
    
    df = st.session_state.base_df.copy()
    
    if not df.empty:
        # åˆ†çº§é€»è¾‘
        def categorize(row):
            r = row['rank']
            if r <= current_level: return "known"
            elif r <= target_level: return "target"
            else: return "beyond"
        
        df['final_cat'] = df.apply(categorize, axis=1)
        df = df.sort_values(by='rank')
        
        # ç­›é€‰é€»è¾‘
        top_df = df[df['rank'] >= min_rank_threshold].sort_values(by='rank', ascending=True).head(top_n)
        target_df = df[df['final_cat']=='target']
        beyond_df = df[df['final_cat']=='beyond']
        
        # Tabs
        tab_list = ["ğŸ”¥ Topç²¾é€‰", "ğŸŸ¡ é‡ç‚¹è¯æ±‡", "ğŸ”´ è¶…çº²è¯æ±‡", "ğŸ“ åŸæ–‡ä¸‹è½½"]
        tabs = st.tabs(tab_list)
        
        # æ¸²æŸ“å‡½æ•° (å°è£…ä»¥å¤ç”¨)
        def render_word_tab(tab_obj, data_df, tab_key):
            with tab_obj:
                if data_df.empty:
                    st.info("è¯¥åŒºé—´æš‚æ— å•è¯")
                    return

                col_list, col_ai = st.columns([1, 2])
                
                # å·¦ä¾§ï¼šå•è¯åˆ—è¡¨
                with col_list:
                    st.markdown(f"**å•è¯é¢„è§ˆ ({len(data_df)})**")
                    display_text = []
                    for _, row in data_df.iterrows():
                        suffix = f" [{int(row['rank'])}]" if show_rank and row['rank'] != 99999 else ""
                        display_text.append(f"{row['word']}{suffix}")
                    st.text_area("åˆ—è¡¨", value="\n".join(display_text), height=400, label_visibility="collapsed")

                # å³ä¾§ï¼šAI ç”ŸæˆåŒº
                with col_ai:
                    st.markdown("#### ğŸ¤– AI å¡ç‰‡åˆ¶ä½œ")
                    export_fmt = st.radio("æ ¼å¼", ["TXT", "CSV"], horizontal=True, key=f"fmt_{tab_key}")
                    
                    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç”Ÿæˆç»“æœ (æŒä¹…åŒ–)
                    res_key = f"{tab_key}_{export_fmt}"
                    existing_result = st.session_state.generated_cards.get(res_key)
                    
                    if existing_result:
                        st.success("âœ… å¡ç‰‡å·²ç”Ÿæˆï¼")
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½ç”Ÿæˆç»“æœ",
                            data=existing_result.encode('utf-8-sig'),
                            file_name=f"anki_{tab_key}.{export_fmt.lower()}",
                            mime="text/plain",
                            type="primary"
                        )
                        with st.expander("æŸ¥çœ‹å·²ç”Ÿæˆå†…å®¹"):
                            st.code(existing_result, language="text")
                    else:
                        st.info("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ï¼Œè°ƒç”¨ AI ä¸ºå·¦ä¾§å•è¯ç”Ÿæˆè§£é‡Šã€ä¾‹å¥å’Œè¯æºã€‚")
                        if st.button(f"âš¡ ç”Ÿæˆ {tab_key} å¡ç‰‡", key=f"btn_{tab_key}"):
                            if not user_api_key:
                                st.error("âŒ è¯·å…ˆåœ¨é¡¶éƒ¨é…ç½®æ è¾“å…¥ DeepSeek API Key")
                            else:
                                pure_words = data_df['word'].tolist()
                                prompt = get_base_prompt_template(export_fmt)
                                
                                p_bar = st.progress(0)
                                s_text = st.empty()
                                
                                result_str = run_concurrent_api(pure_words, prompt, user_api_key, p_bar, s_text)
                                
                                if "âŒ" in result_str and len(result_str) < 100:
                                    st.error(result_str)
                                else:
                                    # ä¿å­˜ç»“æœåˆ° Session State
                                    st.session_state.generated_cards[res_key] = result_str
                                    s_text.success("ğŸ‰ ç”Ÿæˆå®Œæˆï¼")
                                    st.rerun() # å¼ºåˆ¶åˆ·æ–°ä»¥æ˜¾ç¤ºä¸‹è½½æŒ‰é’®

        render_word_tab(tabs[0], top_df, "top")
        render_word_tab(tabs[1], target_df, "target")
        render_word_tab(tabs[2], beyond_df, "beyond")
        
        with tabs[3]:
            st.download_button("ğŸ’¾ ä¸‹è½½è¯å½¢è¿˜åŸåçš„å…¨æ–‡ (.txt)", st.session_state.lemma_text, "lemmatized.txt")
            st.text_area("å…¨æ–‡é¢„è§ˆ", st.session_state.lemma_text[:5000], height=300)