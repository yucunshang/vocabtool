import streamlit as st
import pandas as pd
import re
import spacy
from collections import Counter
import io

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="COCA è¯æ±‡åˆ†çº§åˆ†æå™¨",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- ç¼“å­˜åŠ è½½ NLP æ¨¡å‹ (è§£å†³åŠ è½½æ…¢çš„é—®é¢˜) ---
@st.cache_resource
def load_nlp():
    # ä½¿ç”¨å°æ¨¡å‹è¿›è¡Œè¯å½¢è¿˜åŸ (run, running, ran -> run)
    try:
        return spacy.load("en_core_web_sm")
    except:
        # å¦‚æœäº‘ç«¯æ²¡æœ‰æ¨¡å‹ï¼Œè‡ªåŠ¨ä¸‹è½½çš„å‘½ä»¤åœ¨ packages.txt é‡Œå¤„ç†ï¼Œæˆ–è€…è¿™é‡ŒæŠ¥é”™æç¤º
        import subprocess
        subprocess.run(["python", "-m", "spacy", "download", "en_core_web_sm"])
        return spacy.load("en_core_web_sm")

nlp = load_nlp()

# --- æ ¸å¿ƒé€»è¾‘ ---
def process_text(text, vocab_df, user_vocab_limit):
    # 1. æ–‡æœ¬æ¸…æ´—ä¸è¯å½¢è¿˜åŸ (Word Family)
    doc = nlp(text)
    
    # æå–å•è¯åŸå‹ (Lemma)ï¼Œè¿‡æ»¤æ ‡ç‚¹ã€æ•°å­—ã€åœç”¨è¯
    # è¿™é‡Œçš„é€»è¾‘ï¼šåªä¿ç•™çº¯å­—æ¯å•è¯ï¼Œä¸”é•¿åº¦>1
    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop and len(token.text) > 1]
    
    # ç»Ÿè®¡è¯é¢‘ (æœ¬æ–‡ä¸­çš„é¢‘ç‡)
    text_word_counts = Counter(lemmas)
    
    # 2. ä¸ COCA æ•°æ®åº“æ¯”å¯¹
    # åˆ›å»ºä¸€ä¸ª DataFrame
    df_text = pd.DataFrame(text_word_counts.items(), columns=['word', 'text_freq'])
    
    # å‡è®¾ vocab_df æœ‰ 'word' å’Œ 'rank' ä¸¤åˆ—
    # åˆå¹¶æ•°æ®ï¼šæŠŠ COCA çš„æ’åä¿¡æ¯åˆå¹¶è¿›æ¥
    merged_df = pd.merge(df_text, vocab_df, on='word', how='left')
    
    # å¤„ç†æœªç™»å½•è¯ (COCAé‡Œæ²¡æœ‰çš„è¯ï¼Œè®¾ä¸ºè¶…çº²)
    merged_df['rank'] = merged_df['rank'].fillna(999999).astype(int)
    
    # 3. æ ¸å¿ƒåˆ†ç±»é€»è¾‘
    # æŒæ¡è¯æ±‡ (Within Vocabulary): æ’å <= ç”¨æˆ·è¯æ±‡é‡
    known_df = merged_df[merged_df['rank'] <= user_vocab_limit].copy()
    
    # ç”Ÿè¯ (Beyond Vocabulary): æ’å > ç”¨æˆ·è¯æ±‡é‡
    unknown_df = merged_df[merged_df['rank'] > user_vocab_limit].copy()
    
    return known_df, unknown_df

# --- ä¾§è¾¹æ ï¼šè®¾ç½®ä¸æ•°æ®æº ---
st.sidebar.header("ğŸ› ï¸ è®¾ç½®é¢æ¿")

# 1. å¿…é¡»ä¸Šä¼  COCA è¡¨ (ä¸ºäº†ç‰ˆæƒå®‰å…¨ï¼Œç”±ç”¨æˆ·ä¸Šä¼ )
st.sidebar.subheader("1. æ•°æ®æº (Frequency List)")
vocab_file = st.sidebar.file_uploader("ä¸Šä¼  COCA 20000 è¯è¡¨ (.csv)", type=['csv'])
st.sidebar.caption("æ ¼å¼è¦æ±‚ï¼šåŒ…å«ä¸¤åˆ— `word` å’Œ `rank` çš„ CSV æ–‡ä»¶ã€‚")

# 2. ç”¨æˆ·å‚æ•°
st.sidebar.subheader("2. ä½ çš„è¯æ±‡é‡ (Word Families)")
user_vocab = st.sidebar.number_input(
    "è¾“å…¥ä½ çš„è¯æ±‡é‡é˜ˆå€¼:", 
    min_value=1000, 
    max_value=30000, 
    value=5000, 
    step=500,
    help="ä¾‹å¦‚è¾“å…¥ 5000ï¼Œç³»ç»Ÿä¼šå°† COCA æ’å 5000 åçš„è¯ç®—ä½œç”Ÿè¯ã€‚"
)

sort_option = st.sidebar.radio("æ’åºæ–¹å¼:", ["æŒ‰ COCA è¯é¢‘ (ç”±éš¾åˆ°æ˜“)", "æŒ‰å­—æ¯é¡ºåº (A-Z)"])

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ“š COCA æ–‡æœ¬è¯æ±‡åˆ†çº§å·¥å…·")
st.markdown(f"**é€»è¾‘ï¼š** è‡ªåŠ¨è¿˜åŸå•è¯åŸå‹ (Word Family)ï¼Œå¯¹æ¯” **COCA æ’å**ï¼Œé€šè¿‡ä½ çš„è¯æ±‡é‡ **{user_vocab}** è¿›è¡Œåˆ‡å‰²ã€‚")

# è¾“å…¥åŒºåŸŸ
input_method = st.radio("é€‰æ‹©è¾“å…¥æ–¹å¼:", ["ç›´æ¥ç²˜è´´æ–‡æœ¬", "ä¸Šä¼  TXT æ–‡ä»¶"], horizontal=True)

raw_text = ""
if input_method == "ç›´æ¥ç²˜è´´æ–‡æœ¬":
    raw_text = st.text_area("åœ¨æ­¤ç²˜è´´æ–‡æœ¬:", height=200, placeholder="Paste your English text here...")
else:
    uploaded_txt = st.file_uploader("ä¸Šä¼  .txt æ–‡ä»¶", type=['txt'])
    if uploaded_txt is not None:
        raw_text = uploaded_txt.read().decode("utf-8")

# --- å¼€å§‹åˆ†æ ---
if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
    if not vocab_file:
        st.error("è¯·å…ˆåœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼  COCA è¯é¢‘æ•°æ®åº“ (CSV)ï¼")
    elif not raw_text.strip():
        st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹ã€‚")
    else:
        # åŠ è½½æ•°æ®åº“
        try:
            # è¯»å– CSVï¼Œæ ‡å‡†åŒ–åˆ—å
            vocab_db = pd.read_csv(vocab_file)
            # ç¡®ä¿åˆ—åéƒ½æ˜¯å°å†™ï¼Œé˜²æ­¢ç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶åˆ—åæ˜¯å¤§å†™
            vocab_db.columns = [c.lower() for c in vocab_db.columns]
            
            if 'word' not in vocab_db.columns or 'rank' not in vocab_db.columns:
                st.error("CSV æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼å¿…é¡»åŒ…å« 'word' å’Œ 'rank' ä¸¤åˆ—ã€‚")
            else:
                vocab_db['word'] = vocab_db['word'].astype(str).str.lower().str.strip()
                
                with st.spinner("æ­£åœ¨è¿›è¡Œ NLP åˆ†æä¸åˆ†çº§..."):
                    known, unknown = process_text(raw_text, vocab_db, user_vocab)
                
                # --- æ’åºé€»è¾‘ ---
                if sort_option == "æŒ‰ COCA è¯é¢‘ (ç”±éš¾åˆ°æ˜“)":
                    # éš¾è¯æ’å‰é¢ (rank è¶Šå¤§è¶Šéš¾) -> é™åº
                    unknown = unknown.sort_values(by='rank', ascending=False)
                    known = known.sort_values(by='rank', ascending=False)
                else:
                    unknown = unknown.sort_values(by='word', ascending=True)
                    known = known.sort_values(by='word', ascending=True)

                # --- ç»“æœå±•ç¤º ---
                st.success("åˆ†æå®Œæˆï¼")
                
                tab1, tab2 = st.tabs([f"ğŸ”´ ç”Ÿè¯è¡¨ / è¯æ±‡é‡å¤– ({len(unknown)})", f"ğŸŸ¢ ç†Ÿè¯è¡¨ / è¯æ±‡é‡å†… ({len(known)})"])
                
                with tab1:
                    st.dataframe(
                        unknown[['word', 'rank', 'text_freq']], 
                        column_config={
                            "word": "å•è¯ (åŸå‹)",
                            "rank": "COCA æ’å",
                            "text_freq": "æ–‡ä¸­å‡ºç°æ¬¡æ•°"
                        },
                        use_container_width=True
                    )
                    # ä¸‹è½½æŒ‰é’®
                    csv_unknown = unknown.to_csv(index=False).encode('utf-8')
                    st.download_button("ğŸ“¥ ä¸‹è½½ç”Ÿè¯è¡¨ (CSV)", csv_unknown, "unknown_words.csv", "text/csv")
                    
                    # ç”Ÿæˆ TXT æ ¼å¼å­—ç¬¦ä¸²
                    txt_unknown = "\n".join([f"{row['word']} (Rank: {row['rank']})" for _, row in unknown.iterrows()])
                    st.download_button("ğŸ“„ ä¸‹è½½ç”Ÿè¯è¡¨ (TXT)", txt_unknown, "unknown_words.txt", "text/plain")

                with tab2:
                    st.dataframe(
                        known[['word', 'rank', 'text_freq']],
                        column_config={
                            "word": "å•è¯ (åŸå‹)",
                            "rank": "COCA æ’å",
                            "text_freq": "æ–‡ä¸­å‡ºç°æ¬¡æ•°"
                        },
                        use_container_width=True
                    )
                    csv_known = known.to_csv(index=False).encode('utf-8')
                    st.download_button("ğŸ“¥ ä¸‹è½½ç†Ÿè¯è¡¨ (CSV)", csv_known, "known_words.csv", "text/csv")

        except Exception as e:
            st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

# --- é¡µè„š ---
st.markdown("---")
st.markdown("Powered by **Streamlit** | NLP by **Spacy**")