"""
Vocab Flow Ultra â€“ Streamlit entry point.
UI and session-state wiring; logic lives in constants, errors, utils,
resources, extraction, vocab, ai, anki_parse, tts, anki_package, state.
"""
import html
import logging
import os
import re
import time
from typing import Optional

import streamlit as st

import constants
import resources
from ai import (
    CardFormat,
    get_word_quick_definition,
    process_ai_in_batches,
)
from anki_package import cleanup_old_apkg_files, generate_anki_package
from anki_parse import parse_anki_data
from config import get_config
from errors import ErrorHandler
from extraction import (
    extract_text_from_file,
    extract_text_from_url,
    is_upload_too_large,
)
from rate_limiter import (
    check_batch_limit, check_lookup_limit, check_url_limit,
    record_batch, record_lookup, record_url,
)
from state import set_generated_words_state
from ui_styles import APP_STYLES_HTML
from utils import get_beijing_time_str, render_copy_button, run_gc
from vocab import analyze_logic, get_lemma_for_word

logger = logging.getLogger(__name__)

# ==========================================
# Page Configuration
# ==========================================
st.set_page_config(
    page_title="Vocab Flow Ultra Â· è¯æ±‡åŠ©æ‰‹",
    page_icon="âš¡ï¸",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# Logging: ensure root logger has a handler when running as main app
if not logging.getLogger().handlers:
    logging.basicConfig(
        level=logging.INFO,
        format="%(levelname)s:%(name)s:%(message)s",
    )

# Load vocab data for use in mode_rank tab and the vocab error check below.
VOCAB_DICT, FULL_DF = resources.load_vocab_data()

# Clean old .apkg files from our temp subdir (e.g. from previous sessions)
cleanup_old_apkg_files()

# Initialize Session State
for key, default_value in constants.DEFAULT_SESSION_STATE.items():
    if key not in st.session_state:
        st.session_state[key] = default_value

# Handle clickable word lookup via query params
_qp = st.query_params
_clicked_word = _qp.get("lookup_word", "")
if _clicked_word:
    st.query_params.clear()
    st.session_state["quick_lookup_word"] = _clicked_word
    st.session_state["_auto_lookup_word"] = _clicked_word

# Custom CSS â€“ app-like design (see ui_styles.py)
st.markdown(APP_STYLES_HTML, unsafe_allow_html=True)


def set_anki_pkg(file_path: str, deck_name: str) -> None:
    """Store Anki package path in session state and clean previous file."""
    if not file_path or not os.path.exists(file_path):
        raise FileNotFoundError("Generated Anki package file not found.")

    prev_path = st.session_state.get('anki_pkg_path')
    if prev_path and prev_path != file_path:
        try:
            if os.path.exists(prev_path):
                os.remove(prev_path)
        except OSError as e:
            logger.warning("Could not remove previous anki package: %s", e)

    st.session_state['anki_pkg_path'] = file_path
    st.session_state['anki_pkg_name'] = f"{deck_name}.apkg"


def render_anki_download_button(
    label: str,
    *,
    button_type: str = "primary",
    use_container_width: bool = False,
    key: str | None = None,
) -> None:
    """Safely render Anki package download button if file exists."""
    file_path = st.session_state.get('anki_pkg_path')
    file_name = st.session_state.get('anki_pkg_name', "deck.apkg")

    if not file_path:
        return
    if not os.path.exists(file_path):
        st.warning("âš ï¸ ä¸‹è½½æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°ç”Ÿæˆã€‚")
        st.session_state['anki_pkg_path'] = ""
        return

    try:
        with open(file_path, "rb") as f:
            kwargs = dict(
                label=label,
                data=f.read(),
                file_name=file_name,
                mime="application/octet-stream",
                type=button_type,
                use_container_width=use_container_width,
            )
            if key is not None:
                kwargs["key"] = key
            st.download_button(**kwargs)
        st.caption("ğŸ’¡ å¦‚ä¸‹è½½æ— ååº”ï¼Œè¯·åœ¨æµè§ˆå™¨ï¼ˆSafari / Chromeï¼‰ä¸­æ‰“å¼€æœ¬é¡µé¢å†ç‚¹å‡»ä¸‹è½½ã€‚")
    except OSError as e:
        logger.error("Failed to open package for download: %s", e)
        st.error("âŒ ä¸‹è½½æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œè¯·é‡æ–°ç”Ÿæˆã€‚")


# ==========================================
# UI Components
# ==========================================


def _rank_badge_style(rank: int) -> tuple[str, str]:
    """Map numeric rank to badge color and label."""
    if rank <= 2809:
        return "#10b981", "æ ¸å¿ƒ"
    if rank <= 6000:
        return "#22c55e", "åŸºç¡€"
    if rank <= 10000:
        return "#3b82f6", "å¸¸ç”¨"
    if rank <= 15000:
        return "#f59e0b", "è¿›é˜¶è¯"
    if rank <= 20000:
        return "#f97316", "é«˜çº§è¯"
    if rank < 99999:
        return "#ef4444", "ä½é¢‘è¯"
    return "#6b7280", "æœªæ”¶å½•"


@st.cache_data(show_spinner=False)
def _cached_analyze(text: str, min_rank: int, max_rank: int, include_unknown: bool):
    return analyze_logic(text, min_rank, max_rank, include_unknown)


@st.cache_data(ttl=3600, show_spinner=False)
def _cached_extract_url(url: str) -> str:
    return extract_text_from_url(url)


def _analyze_and_set_words(raw_text: str, min_rank: int, max_rank: int) -> bool:
    """Run rank-based analysis and update session state. Returns success."""
    if len(raw_text) <= 2:
        return False
    final_data, raw_count, stats_info = _cached_analyze(raw_text, min_rank, max_rank, False)
    set_generated_words_state(final_data, raw_count, stats_info)
    if not final_data:
        st.info(
            f"ğŸ“­ å…±æå– {raw_count} ä¸ªåŸå§‹è¯ï¼Œç» rank {min_rank}â€“{max_rank} ç­›é€‰åæ— å‰©ä½™å•è¯ã€‚"
            " å¯å°è¯•æ‰©å¤§ rank èŒƒå›´ï¼Œæˆ–ä½¿ç”¨ã€Œè¯è¡¨ã€æ¨¡å¼ç›´æ¥ç²˜è´´ã€‚"
        )
        return False
    return True


st.markdown("""
<div class="app-hero">
    <h1>è¯æ±‡åŠ©æ‰‹</h1>
    <p>æŸ¥è¯ã€ç­›è¯ã€åˆ¶å¡ä¸€ä½“åŒ–</p>
</div>
""", unsafe_allow_html=True)


def _render_extraction_stats(data: list) -> None:
    """Show 4 metrics: coverage, target density, raw count, filtered count."""
    original_count = len(data)
    if st.session_state.get('stats_info'):
        stats = st.session_state['stats_info']
        col_s1, col_s2 = st.columns(2)
        with col_s1:
            st.metric("ğŸ“Š è¯æ±‡è¦†ç›–ç‡", f"{stats['coverage']*100:.1f}%")
        with col_s2:
            st.metric("ğŸ¯ ç›®æ ‡è¯å¯†åº¦", f"{stats['target_density']*100:.1f}%")

    raw_count = st.session_state.get('raw_count', 0)
    if not raw_count:
        raw_count = original_count
    col_t1, col_t2 = st.columns(2)
    with col_t1:
        st.metric("ğŸ“¦ æå–çš„å•è¯æ€»æ•°", raw_count)
    with col_t2:
        st.metric("âœ… ç­›é€‰åå•è¯æ€»æ•°", original_count)


def _render_word_editor(data: list) -> list:
    """Render editable word list textarea with copy button. Returns current words_only."""
    words_only = [w for w, r in data]
    words_text = "\n".join(words_only)
    original_count = len(data)
    if "word_list_editor" not in st.session_state:
        st.session_state["word_list_editor"] = words_text

    col_title, col_copy_btn = st.columns([5, 1])
    with col_title:
        st.markdown("### ğŸ“ å•è¯åˆ—è¡¨")
    with col_copy_btn:
        current_words_text = st.session_state.get("word_list_editor", words_text)
        render_copy_button(current_words_text, key="copy_words_btn")
    st.caption("ğŸ’¡ å¯ä»¥åœ¨ä¸‹æ–¹æ–‡æœ¬æ¡†ä¸­ç¼–è¾‘ã€æ–°å¢æˆ–åˆ é™¤å•è¯ï¼Œæ¯è¡Œä¸€ä¸ªå•è¯")

    edited_words = st.text_area(
        f"âœï¸ å•è¯åˆ—è¡¨ (å…± {original_count} ä¸ª)",
        height=300,
        key="word_list_editor",
        label_visibility="collapsed",
        help="æ¯è¡Œä¸€ä¸ªå•è¯"
    )

    if edited_words != words_text:
        result_words = [w.strip() for w in edited_words.split('\n') if w.strip()]
        st.info(f"ğŸ“ å·²ç¼–è¾‘ï¼šå½“å‰å…± {len(result_words)} ä¸ªå•è¯")
    else:
        result_words = words_only

    return result_words


def _render_audio_settings(key_prefix: str) -> tuple[bool, str, bool]:
    """Render audio toggle + voice selector. Example audio always on when TTS enabled.
    Returns (enable_audio, voice_code, enable_example_audio)."""
    enable_audio = st.checkbox("å¯ç”¨è¯­éŸ³", value=True, key=f"chk_audio_{key_prefix}")
    if enable_audio:
        selected_voice_label = st.radio(
            "ğŸ™ï¸ å‘éŸ³äºº",
            options=list(constants.VOICE_MAP.keys()),
            index=0,
            horizontal=False,
            key=f"sel_voice_{key_prefix}",
        )
        voice_code = constants.VOICE_MAP[selected_voice_label]
    else:
        voice_code = list(constants.VOICE_MAP.values())[0]
    return enable_audio, voice_code, True  # ä¾‹å¥é»˜è®¤å‘éŸ³ï¼Œä¸æä¾›é€‰é¡¹


def _render_builtin_ai_section(
    words_only: list, enable_audio: bool, voice_code: str, enable_example_audio: bool,
    card_format: Optional[CardFormat] = None,
) -> None:
    """Builtin AI generation flow: button â†’ progress â†’ parse â†’ edit â†’ package â†’ download."""
    ai_model_label = get_config().get("openai_model_display", constants.AI_MODEL_DISPLAY)

    # Fix 1: Deck name input
    deck_name = st.text_input(
        "ğŸ·ï¸ ç‰Œç»„åç§°",
        value=f"Vocab_{get_beijing_time_str()}",
        key="builtin_deck_name",
        help="ç”Ÿæˆçš„ .apkg æ–‡ä»¶å’Œ Anki ç‰Œç»„åç§°",
    )

    # Fix 7: Pagination â€” slice words into pages of MAX_AUTO_LIMIT
    total_word_count = len(words_only)
    page_size = constants.MAX_AUTO_LIMIT
    page = st.session_state.get("_builtin_gen_page", 0)
    start_idx = page * page_size
    end_idx = (page + 1) * page_size
    words_for_auto_ai = words_only[start_idx:min(end_idx, total_word_count)]
    remaining_words = max(0, total_word_count - end_idx)

    if total_word_count > page_size:
        if page == 0:
            st.caption(
                f"âš ï¸ å…± {total_word_count} è¯ï¼Œé¦–æ‰¹å¤„ç†å‰ {len(words_for_auto_ai)} è¯ã€‚"
                " ç”Ÿæˆå®Œæˆåå¯ç»§ç»­å¤„ç†å‰©ä½™è¯æ±‡ã€‚"
            )
        else:
            st.caption(
                f"âš ï¸ ç¬¬ {page + 1} æ‰¹ï¼Œç¬¬ {start_idx + 1}â€“{min(end_idx, total_word_count)} è¯"
                f"ï¼ˆå…± {total_word_count} è¯ï¼‰ã€‚"
            )

    # Handle auto-generate flag set by "ç»§ç»­ä¸‹ä¸€æ‰¹" button
    auto_generate = st.session_state.pop("_builtin_auto_generate", False)

    clicked = st.button(
        f"ğŸš€ ä½¿ç”¨ {ai_model_label} ä¸€é”®åˆ¶å¡",
        type="primary",
        key="btn_builtin_gen",
        use_container_width=True,
    )

    if clicked:
        # Fresh generate: reset page and clear previous cards
        st.session_state["_builtin_gen_page"] = 0
        st.session_state.pop("_builtin_parsed_cards", None)
        st.session_state.pop("_builtin_ai_partial_result", None)
        st.session_state.pop("_builtin_ai_failed_words", None)
        st.session_state.pop("card_editor", None)
        # Re-compute pagination for page 0
        words_for_auto_ai = words_only[:page_size]
        remaining_words = max(0, total_word_count - page_size)

    if clicked or auto_generate:
        batch_allowed, batch_msg = check_batch_limit()
        if not batch_allowed:
            st.warning(batch_msg)
            st.stop()
        record_batch()

        progress_title = st.empty()
        card_text = st.empty()
        card_bar = st.progress(0.0)
        audio_text = st.empty()
        audio_bar = st.progress(0.0)

        total_words = len(words_for_auto_ai)
        batch_size = constants.AI_BATCH_SIZE
        first_end = min(batch_size, total_words)
        progress_title.markdown("#### â³ å†…ç½® AI åˆ¶å¡è¿›åº¦")
        st.caption("ğŸ’¡ ç¬¬ä¸€ç»„å¯èƒ½ç¨æ…¢ï¼ˆå»ºç«‹è¿æ¥ï¼‰ï¼Œåç»­ç»„å¹¶å‘è¿›è¡Œã€‚")
        card_text.markdown(f"**åˆ¶å¡è¿›åº¦**ï¼šç¬¬ 1 ç»„ï¼ˆ1â€“{first_end}/{total_words}ï¼‰AI ç”Ÿæˆä¸­...")
        audio_text.markdown("**éŸ³é¢‘è¿›åº¦**ï¼šç­‰å¾…åˆ¶å¡å®Œæˆ...")

        def update_ai_progress(current: int, total: int) -> None:
            ratio = current / total if total > 0 else 0.0
            card_bar.progress(min(0.9, ratio * 0.9))
            batch_idx = (current + batch_size - 1) // batch_size
            start = (batch_idx - 1) * batch_size + 1
            end = min(batch_idx * batch_size, total)
            card_text.markdown(f"**åˆ¶å¡è¿›åº¦**ï¼šç¬¬ {batch_idx} ç»„ï¼ˆ{start}â€“{end}/{total}ï¼‰AI ç”Ÿæˆä¸­...")

        # Fix 3: Unpack (content, failed_words) tuple; pass voice for IPA style (BrE/AmE)
        _fmt = dict(card_format) if card_format else {}
        _fmt["voice_code"] = voice_code
        current_card_type = card_format.get("card_type", "standard") if card_format else "standard"
        ai_result, failed_words = process_ai_in_batches(
            words_for_auto_ai,
            progress_callback=update_ai_progress,
            card_format=_fmt,
        )

        st.session_state["_builtin_ai_failed_words"] = failed_words or []

        # Fix 3: Report failed words
        if failed_words:
            st.warning(
                f"âš ï¸ {len(failed_words)} ä¸ªè¯ç”Ÿæˆå¤±è´¥ï¼Œå·²è·³è¿‡ï¼š"
                f"{', '.join(failed_words[:20])}"
                + ("..." if len(failed_words) > 20 else "")
            )

        if ai_result:
            card_text.markdown("**åˆ¶å¡è¿›åº¦**ï¼šæ­£åœ¨è§£æ AI ç»“æœ...")
            parsed_data = parse_anki_data(ai_result, expected_card_type=current_card_type)

            if parsed_data:
                card_bar.progress(1.0)
                req = len(words_for_auto_ai)
                got = len(parsed_data)
                count_msg = f"å…± {got} å¼ " + (f"ï¼ˆè¯·æ±‚ {req} è¯ï¼‰" if got != req else "")
                card_text.markdown(f"**åˆ¶å¡è¿›åº¦**ï¼šâœ… å®Œæˆï¼ˆ{count_msg}ï¼‰")
                audio_text.markdown("**éŸ³é¢‘è¿›åº¦**ï¼šè¿›è¡Œä¸­...")

                # Fix 4: Checkpoint â€” save raw AI text in case packaging fails
                st.session_state["_builtin_ai_partial_result"] = ai_result

                # Fix 5: Save cards; append if continuing a batch
                if auto_generate:
                    existing = st.session_state.get("_builtin_parsed_cards") or []
                    st.session_state["_builtin_parsed_cards"] = existing + parsed_data
                    st.session_state.pop("card_editor", None)
                else:
                    st.session_state["_builtin_parsed_cards"] = parsed_data

                edited_data = st.session_state["_builtin_parsed_cards"]

                def update_pkg_progress(ratio: float, text: str) -> None:
                    audio_bar.progress(ratio)
                    audio_text.markdown(f"**éŸ³é¢‘è¿›åº¦**ï¼š{text}")

                try:
                    current_deck_name = st.session_state.get("builtin_deck_name", deck_name)
                    file_path, audio_failed, failed_phrases = generate_anki_package(
                        edited_data,
                        current_deck_name,
                        card_type=current_card_type,
                        enable_tts=enable_audio,
                        tts_voice=voice_code,
                        enable_example_tts=enable_example_audio,
                        progress_callback=update_pkg_progress,
                    )
                    set_anki_pkg(file_path, current_deck_name)
                    st.session_state["anki_cards_cache"] = edited_data
                    st.session_state["_builtin_card_type"] = current_card_type
                    st.session_state["_builtin_audio_failed_count"] = audio_failed
                    st.session_state["_builtin_audio_failed_phrases"] = failed_phrases or []
                    audio_bar.progress(1.0)
                    suffix = f"ï¼ˆ{audio_failed} æ¡å¤±è´¥ï¼Œå¯ç‚¹å‡»ä¸‹æ–¹é‡è¯•ï¼‰" if audio_failed else ""
                    audio_text.markdown(f"**éŸ³é¢‘è¿›åº¦**ï¼šâœ… å®Œæˆ{suffix}")
                    req = len(words_for_auto_ai)
                    got = len(edited_data)
                    st.info(f"âœ… è§£æå®Œæˆï¼Œå…± {got} å¼ å¡ç‰‡ã€‚" + (f"ï¼ˆè¯·æ±‚ {req} è¯ï¼Œ{req - got} è¯æœªè§£ææˆåŠŸï¼‰" if got < req else ""))
                    st.balloons()
                    run_gc()
                except Exception as e:
                    audio_text.markdown("**éŸ³é¢‘è¿›åº¦**ï¼šâŒ ç”Ÿæˆå¤±è´¥")
                    ErrorHandler.handle(e, "ç”Ÿæˆ .apkg å‡ºé”™")
            else:
                card_text.markdown("**åˆ¶å¡è¿›åº¦**ï¼šâŒ è§£æå¤±è´¥")
                audio_text.markdown("**éŸ³é¢‘è¿›åº¦**ï¼šæœªå¼€å§‹")
                st.error("è§£æå¤±è´¥ï¼ŒAI è¿”å›å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯ã€‚")
        else:
            card_text.markdown("**åˆ¶å¡è¿›åº¦**ï¼šâŒ AI ç”Ÿæˆå¤±è´¥")
            audio_text.markdown("**éŸ³é¢‘è¿›åº¦**ï¼šæœªå¼€å§‹")
            st.error("AI ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key æˆ–ç½‘ç»œè¿æ¥ã€‚")

    # Fix 5: Next batch button â€” shown whenever cards are in session state
    if st.session_state.get("_builtin_parsed_cards") and remaining_words > 0:
        if st.button(
            f"â–¶ ä¸‹ä¸€æ‰¹ï¼ˆå‰©ä½™ {remaining_words} è¯ï¼‰",
            key="btn_gen_next_page",
            use_container_width=True,
        ):
            st.session_state["_builtin_gen_page"] = page + 1
            st.session_state["_builtin_auto_generate"] = True
            st.rerun()

    render_anki_download_button(
        f"ğŸ“¥ ä¸‹è½½ {st.session_state.get('anki_pkg_name', 'deck.apkg')}",
        button_type="primary",
        use_container_width=True,
        key="builtin_download_btn",
    )

    ai_failed_words = st.session_state.get("_builtin_ai_failed_words", [])
    if ai_failed_words and st.session_state.get("_builtin_parsed_cards"):
        st.warning(
            f"âš ï¸ ä»æœ‰ {len(ai_failed_words)} ä¸ªè¯æœªå®Œæˆåˆ¶å¡ï¼š"
            f"{', '.join(ai_failed_words[:20])}"
            + ("..." if len(ai_failed_words) > 20 else "")
        )
        if st.button("ğŸ”„ é‡è¯•å¤±è´¥è¯åˆ¶å¡", key="btn_retry_failed_words_builtin", use_container_width=True):
            batch_allowed, batch_msg = check_batch_limit()
            if not batch_allowed:
                st.warning(batch_msg)
            else:
                record_batch()

                _fmt = dict(card_format) if card_format else {}
                _fmt["voice_code"] = voice_code
                current_card_type = st.session_state.get("_builtin_card_type") or (
                    card_format.get("card_type", "standard") if card_format else "standard"
                )
                with st.spinner(f"â³ é‡è¯• {len(ai_failed_words)} ä¸ªå¤±è´¥è¯..."):
                    try:
                        retry_result, retry_failed_words = process_ai_in_batches(
                            ai_failed_words,
                            card_format=_fmt,
                        )
                        retry_parsed = (
                            parse_anki_data(retry_result, expected_card_type=current_card_type)
                            if retry_result else []
                        )

                        existing_cards = st.session_state.get("_builtin_parsed_cards") or []
                        seen_words = {str(c.get("w", "")).strip().lower() for c in existing_cards if c.get("w")}
                        added_cards = []
                        for card in retry_parsed:
                            card_word = str(card.get("w", "")).strip().lower()
                            if not card_word or card_word in seen_words:
                                continue
                            seen_words.add(card_word)
                            added_cards.append(card)

                        merged_cards = existing_cards + added_cards
                        st.session_state["_builtin_parsed_cards"] = merged_cards
                        st.session_state.pop("card_editor", None)

                        unresolved_after_parse = []
                        if current_card_type in ("standard", "cloze", "audio"):
                            parsed_heads = {str(c.get("w", "")).strip().lower() for c in retry_parsed if c.get("w")}
                            unresolved_after_parse = [
                                w for w in ai_failed_words
                                if w.strip().lower() not in parsed_heads
                            ]
                        remaining_failed = list(dict.fromkeys((retry_failed_words or []) + unresolved_after_parse))
                        st.session_state["_builtin_ai_failed_words"] = remaining_failed

                        if added_cards:
                            current_deck_name = st.session_state.get("builtin_deck_name", deck_name)
                            new_file, audio_failed, failed_phrases = generate_anki_package(
                                merged_cards,
                                current_deck_name,
                                card_type=current_card_type,
                                enable_tts=enable_audio,
                                tts_voice=voice_code,
                                enable_example_tts=enable_example_audio,
                            )
                            set_anki_pkg(new_file, current_deck_name)
                            st.session_state["anki_cards_cache"] = merged_cards
                            st.session_state["_builtin_card_type"] = current_card_type
                            st.session_state["_builtin_audio_failed_count"] = audio_failed
                            st.session_state["_builtin_audio_failed_phrases"] = failed_phrases or []

                        if added_cards:
                            st.success(f"âœ… é‡è¯•è¡¥å…¨ {len(added_cards)} å¼ å¡ç‰‡ã€‚")
                        else:
                            st.info("æœ¬æ¬¡é‡è¯•æœªè§£æå‡ºæ–°å¡ç‰‡ã€‚")
                        if remaining_failed:
                            st.warning(
                                f"âš ï¸ ä»å¤±è´¥ {len(remaining_failed)} è¯ï¼š"
                                + ", ".join(remaining_failed[:20])
                                + ("..." if len(remaining_failed) > 20 else "")
                            )
                        run_gc()
                    except Exception as e:
                        ErrorHandler.handle(e, "é‡è¯•å¤±è´¥è¯åˆ¶å¡å¤±è´¥")
                st.rerun()

    # Audio retry: only re-run TTS for files still missing (cache skips successful ones)
    audio_failed = st.session_state.get("_builtin_audio_failed_count", 0)
    failed_phrases = st.session_state.get("_builtin_audio_failed_phrases", [])
    if audio_failed > 0 and st.session_state.get("_builtin_parsed_cards") and st.session_state.get("anki_pkg_path"):
        st.warning(
            f"âš ï¸ {audio_failed} æ¡éŸ³é¢‘ç”Ÿæˆå¤±è´¥ï¼Œæ¶‰åŠ {len(failed_phrases)} ä¸ªè¯ã€‚"
            " ç‚¹å‡»é‡è¯•ï¼Œå·²æˆåŠŸçš„éŸ³é¢‘ä¼šç›´æ¥å¤ç”¨ï¼Œåªè¡¥å…¨ç¼ºå¤±éƒ¨åˆ†ã€‚"
        )
        with st.expander("â“ éŸ³é¢‘å¤±è´¥å¯èƒ½åŸå› ", expanded=False):
            st.markdown("""
- **ç½‘ç»œè¶…æ—¶**ï¼šedge-tts éœ€è”ç½‘ï¼Œç½‘ç»œä¸ç¨³æˆ–è¶…æ—¶ä¼šå¯¼è‡´å¤±è´¥
- **æ–‡æœ¬è¿‡çŸ­**ï¼šä¾‹å¥ â‰¤3 å­—ç¬¦å¯èƒ½è¢« TTS å¿½ç•¥æˆ–ç”Ÿæˆå¤±è´¥
- **é ASCII æ‹¬å·**ï¼šä¾‹å¥å«ä¸­æ–‡æ‹¬å· `ï¼ˆï¼‰` ç­‰ï¼ŒTTS å¯èƒ½æ”¶åˆ°é”™è¯¯æ–‡æœ¬
- **é€Ÿç‡é™åˆ¶**ï¼šå¤§æ‰¹é‡æ—¶æœåŠ¡ç«¯å¯èƒ½é™æµ
- **ä¸´æ—¶ç›®å½•**ï¼šæƒé™æˆ–ç£ç›˜ç©ºé—´ä¸è¶³å¯¼è‡´å†™å…¥å¤±è´¥
- **è¯­éŸ³ä¸æ”¯æŒ**ï¼šæå°‘æ•°å­—ç¬¦æˆ–è¯­è¨€ç»„åˆä¸è¢«æ‰€é€‰è¯­éŸ³æ”¯æŒ
""")
        if failed_phrases:
            st.caption("å¤±è´¥è¯æ±‡ï¼š" + "ã€".join(failed_phrases[:20]) + (" â€¦" if len(failed_phrases) > 20 else ""))
        if st.button("ğŸ”„ é‡è¯•å¤±è´¥éŸ³é¢‘", key="btn_retry_audio_builtin"):
            cards = st.session_state["_builtin_parsed_cards"]
            with st.spinner(f"â³ é‡è¯• {audio_failed} æ¡éŸ³é¢‘..."):
                try:
                    current_deck_name = st.session_state.get("builtin_deck_name", deck_name)
                    new_file, new_failed, new_phrases = generate_anki_package(
                        cards, current_deck_name,
                        card_type=st.session_state.get("_builtin_card_type", "standard"),
                        enable_tts=enable_audio, tts_voice=voice_code,
                        enable_example_tts=enable_example_audio,
                    )
                    set_anki_pkg(new_file, current_deck_name)
                    st.session_state["_builtin_audio_failed_count"] = new_failed
                    st.session_state["_builtin_audio_failed_phrases"] = new_phrases or []
                    run_gc()
                    st.rerun()
                except Exception as e:
                    ErrorHandler.handle(e, "é‡è¯•éŸ³é¢‘å¤±è´¥")

    st.caption("âš ï¸ AI ç»“æœè¯·äººå·¥å¤æ ¸åå†å­¦ä¹ ã€‚")


def _render_extract_results() -> None:
    """Render the extracted words results, AI card generation, and third-party prompt."""
    if not st.session_state.get('gen_words_data'):
        return

    data = st.session_state['gen_words_data']

    _render_extraction_stats(data)

    st.markdown("### âœ… æå–æˆåŠŸï¼")

    words_only = _render_word_editor(data)

    st.markdown("---")
    st.markdown("### ğŸ¤– AI ç”Ÿæˆ Anki å¡ç‰‡")

    enable_audio, voice_code, enable_example_audio = _render_audio_settings("auto")

    st.markdown("#### å†…ç½® AI ä¸€é”®åˆ¶å¡")
    card_type = st.radio(
        "å¡ç‰‡ç±»å‹",
        options=constants.CARD_TYPES,
        format_func=lambda x: {
            "standard":    "ğŸ“– æ ‡å‡†å¡ï¼ˆæ­£é¢å•è¯ï¼Œåé¢ä¸­è‹±é‡Šä¹‰+ä¾‹å¥ï¼‰",
            "cloze":       "ğŸ“– é˜…è¯»å¡ï¼ˆæ­£é¢æŒ–ç©ºå¥ï¼Œåé¢å•è¯+é‡Šä¹‰+ä¾‹å¥ï¼‰",
            "production":  "âœï¸ å£è¯­è¡¨è¾¾å¡ï¼ˆæ­£é¢ä¸­æ–‡åœºæ™¯ï¼Œåé¢è‹±æ–‡è¯å—+ä¾‹å¥ï¼‰",
            "translation": "ğŸ“ äº’è¯‘å¡ï¼ˆæ­£é¢ä¸­æ–‡é‡Šä¹‰ï¼Œåé¢è‹±æ–‡å•è¯+éŸ³æ ‡ï¼‰",
            "audio":       "ğŸ”Š å¬éŸ³å¡ï¼ˆæ­£é¢éŸ³é¢‘ï¼Œåé¢å•è¯+é‡Šä¹‰ï¼‰",
        }.get(x, x),
        index=1,
        horizontal=False,
        key="builtin_card_type",
    )
    _render_builtin_ai_section(
        words_only, enable_audio, voice_code, enable_example_audio,
        {"card_type": card_type},
    )


def _do_lookup(query_word: str) -> None:
    """Execute AI lookup for a word, populating session state cache and result."""
    # Input length guard
    if len(query_word) > constants.MAX_LOOKUP_INPUT_LENGTH:
        st.warning(f"âš ï¸ è¾“å…¥è¿‡é•¿ï¼ˆæœ€å¤š {constants.MAX_LOOKUP_INPUT_LENGTH} å­—ç¬¦ï¼‰ã€‚")
        return

    # Rate limit check
    allowed, msg = check_lookup_limit()
    if not allowed:
        st.warning(msg)
        return

    st.session_state["quick_lookup_is_loading"] = True
    try:
        # Case-sensitive cache so "China" vs "china", "May" vs "may" get different results
        cache_key = f"lookup_cache_{query_word.strip()}"
        if cache_key not in st.session_state:
            stream_box = st.empty()

            def _on_stream(text: str) -> None:
                safe = html.escape(text).replace("\n", "<br>")
                stream_box.markdown(
                    f'<div class="ql-stream">{safe}</div>',
                    unsafe_allow_html=True,
                )

            with st.spinner("ğŸ” æŸ¥è¯¢ä¸­..."):
                st.session_state[cache_key] = get_word_quick_definition(
                    query_word,
                    stream_callback=_on_stream,
                )
            stream_box.empty()
            keys = st.session_state["quick_lookup_cache_keys"]
            keys.append(cache_key)
            while len(keys) > constants.QUICK_LOOKUP_CACHE_MAX:
                old_key = keys.pop(0)
                if old_key in st.session_state:
                    del st.session_state[old_key]
            st.session_state["quick_lookup_cache_keys"] = keys
        st.session_state["quick_lookup_last_query"] = query_word
        st.session_state["quick_lookup_last_result"] = st.session_state.get(cache_key)
        record_lookup()
    finally:
        st.session_state["quick_lookup_is_loading"] = False
        st.session_state["quick_lookup_block_until"] = time.time() + constants.QUICK_LOOKUP_COOLDOWN_SECONDS


def render_quick_lookup() -> None:
    # Guard against sessions that predate DEFAULT_SESSION_STATE centralization.
    for _k, _v in {
        "quick_lookup_last_query": "",
        "quick_lookup_last_result": None,
        "quick_lookup_is_loading": False,
        "quick_lookup_block_until": 0.0,
        "quick_lookup_cache_keys": [],
    }.items():
        if _k not in st.session_state:
            st.session_state[_k] = _v

    now_ts = time.time()
    in_cooldown = now_ts < st.session_state["quick_lookup_block_until"]
    lookup_disabled = st.session_state["quick_lookup_is_loading"] or in_cooldown

    pending_word = st.session_state.pop("_quick_lookup_pending_word", "")
    if pending_word:
        st.session_state["quick_lookup_word"] = pending_word
        st.session_state["_auto_lookup_word"] = pending_word

    # Update input to lemma only (no lookup) - must run before widget creation
    update_input_only = st.session_state.pop("_quick_lookup_update_input_only", "")
    if update_input_only:
        st.session_state["quick_lookup_word"] = update_input_only

    # Handle pending clear (must happen before text_input widget is created)
    if st.session_state.pop("_quick_lookup_pending_clear", False):
        st.session_state["quick_lookup_word"] = ""
        st.session_state["quick_lookup_last_query"] = ""
        st.session_state["quick_lookup_last_result"] = None
        st.toast("å·²æ¸…ç©º", icon="ğŸ—‘ï¸")

    auto_word = st.session_state.pop("_auto_lookup_word", "")
    if auto_word and not in_cooldown:
        lemma = get_lemma_for_word(auto_word)
        st.session_state["quick_lookup_word"] = lemma
        _do_lookup(lemma)

    lookup_model_label = get_config().get("openai_model_display", constants.AI_MODEL_DISPLAY)
    _btn_label = "æŸ¥è¯¢ä¸­..." if st.session_state["quick_lookup_is_loading"] else f"ğŸ” {lookup_model_label}"
    _has_content = bool(st.session_state.get("quick_lookup_word") or st.session_state.get("quick_lookup_last_result"))

    with st.form("quick_lookup_form", clear_on_submit=False, border=False):
        # Always render 3 columns so the form structure stays constant across
        # reruns.  A changing column count (2 vs 3) shifts the submit button's
        # element ID, causing Streamlit to lose the "clicked" signal and making
        # the lookup button silently do nothing on the first submission.
        col_word, col_btn, col_clear = st.columns([4, 2, 1.2])
        with col_word:
            lookup_word = st.text_input(
                "è¾“å…¥å•è¯æˆ–çŸ­è¯­",
                placeholder="è¾“å…¥å•è¯æˆ–çŸ­è¯­ï¼Œå›è½¦æŸ¥è¯¢ â€¦",
                key="quick_lookup_word",
                label_visibility="collapsed",
                autocomplete="off",
            )
        with col_btn:
            lookup_submit = st.form_submit_button(
                _btn_label,
                type="primary",
                use_container_width=True,
                disabled=lookup_disabled
            )
        with col_clear:
            clear_submit = st.form_submit_button("æ¸…ç©º", use_container_width=True, disabled=not _has_content)
        if clear_submit:
            st.session_state["_quick_lookup_pending_clear"] = True
            st.rerun()

    if in_cooldown:
        wait_seconds = max(0.0, st.session_state["quick_lookup_block_until"] - now_ts)
        st.caption(f"â±ï¸ è¯·ç¨å€™ {wait_seconds:.1f}s å†æ¬¡æŸ¥è¯¢")

    if lookup_submit and not clear_submit:
        query_word = lookup_word.strip()
        if not query_word:
            st.warning("âš ï¸ è¯·è¾“å…¥å•è¯æˆ–çŸ­è¯­ã€‚")
        else:
            if st.session_state["quick_lookup_is_loading"]:
                st.info("â³ æŸ¥è¯¢è¿›è¡Œä¸­ï¼Œè¯·ç¨å€™ã€‚")
            elif time.time() < st.session_state["quick_lookup_block_until"]:
                st.info("â±ï¸ è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•ã€‚")
            else:
                lemma = get_lemma_for_word(query_word)
                _do_lookup(lemma)
                st.session_state["_quick_lookup_update_input_only"] = lemma
                st.rerun()

    result = st.session_state.get("quick_lookup_last_result")
    if result and 'error' not in result:
        raw_content = result['result']
        rank = result.get('rank', 99999)

        rank_color, rank_label = _rank_badge_style(rank)

        # Build styled HTML lines (no iframe needed)
        lines = raw_content.split('\n')
        formatted_lines = []

        for line in lines:
            line = line.strip()
            if not line:
                continue
            safe = html.escape(line)

            if line.startswith("ğŸŒ±"):
                formatted_lines.append(f'<div class="ql-etym">{safe}</div>')
            elif "|" in line and len(line) < 50:
                formatted_lines.append(f'<div class="ql-def">{safe}</div>')
            elif line.startswith("â€¢"):
                formatted_lines.append(f'<div class="ql-ex">{safe}</div>')
            else:
                formatted_lines.append(f'<div class="ql-misc">{safe}</div>')

        display_html = "".join(formatted_lines)
        rank_badge = f'<span style="display:inline-block;background:{rank_color};color:white;padding:3px 10px;border-radius:5px;font-size:13px;font-weight:600;">ğŸ“Š {rank} Â· {rank_label}</span>'

        st.markdown(f'<div class="ql-result">{display_html}<div style="margin-top:10px;">{rank_badge}</div></div>', unsafe_allow_html=True)

    elif result and 'error' in result:
        # Avoid flashing red error blocks on reruns/refresh.
        st.toast(f"æŸ¥è¯å¤±è´¥ï¼š{result.get('error', 'æœªçŸ¥é”™è¯¯')}", icon="âš ï¸")
        st.session_state["quick_lookup_last_result"] = None

    st.markdown("---")


if hasattr(st, "fragment"):
    render_quick_lookup = st.fragment(render_quick_lookup)
    _render_extract_results = st.fragment(_render_extract_results)

if not VOCAB_DICT:
    st.error("âš ï¸ ç¼ºå¤± `ngsl_word_rank.csv` è¯åº“æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ç›®å½•ã€‚")

with st.expander("ä½¿ç”¨æŒ‡å— & æ”¯æŒæ ¼å¼", expanded=False):
    st.markdown("""
    **æé€Ÿå·¥ä½œæµ**
    1. **æŸ¥è¯** â€” é¡¶éƒ¨ AI æŸ¥è¯ï¼Œç§’é€Ÿè·å–ç²¾å‡†é‡Šä¹‰ã€è¯æºæ‹†è§£å’ŒåŒè¯­ä¾‹å¥
    2. **æå–** â€” æ”¯æŒ PDF / ePub / Docx / TXT / CSV / Excel ç­‰æ ¼å¼
    3. **ç”Ÿæˆ** â€” AI é‡Šä¹‰ + å¹¶å‘è¯­éŸ³åˆæˆï¼Œä¸€é”®æ‰“åŒ…ä¸‹è½½

    **æ”¯æŒçš„æ–‡ä»¶æ ¼å¼**
    TXT Â· PDF Â· DOCX Â· EPUB Â· CSV Â· XLSX Â· XLS Â· DB Â· SQLite Â· Anki å¯¼å‡º (.txt)
    """)

tab_lookup, tab_extract_anki = st.tabs([
    "AIæŸ¥è¯",
    "ç­›é€‰å•è¯&åˆ¶å¡",
])

with tab_lookup:
    render_quick_lookup()

# ==========================================
# Tab 1: Word Extractionï¼ˆç­›é€‰å•è¯ï¼‰
# ==========================================
def _render_shared_rank_selection() -> tuple[int, int]:
    """æ¸²æŸ“é€šç”¨è¯æ±‡é‡ rank é€‰æ‹©ï¼ˆæ–‡æœ¬/é“¾æ¥/æ–‡ä»¶/è¯åº“å…±ç”¨ï¼Œè¯è¡¨ä¸é€‚ç”¨ï¼‰ã€‚è¿”å› (min_rank, max_rank)ã€‚"""
    st.markdown("#### è¯æ±‡é‡ rank é€‰æ‹©ï¼ˆæ–‡æœ¬ / é“¾æ¥ / æ–‡ä»¶ / è¯åº“ é€šç”¨ï¼‰")
    preset_choices = [f"{label} ({min_r}â€“{max_r})" for label, min_r, max_r in constants.RANK_PRESETS] + ["è‡ªå®šä¹‰"]
    selected = st.radio(
        "è¯æ±‡é‡åŒºé—´",
        preset_choices,
        key="extract_rank_preset",
        horizontal=True,
        format_func=lambda x: x,
    )
    if selected != "è‡ªå®šä¹‰":
        for _label, min_r, max_r in constants.RANK_PRESETS:
            if selected == f"{_label} ({min_r}â€“{max_r})":
                st.session_state["extract_min_rank"] = min_r
                st.session_state["extract_max_rank"] = max_r
                break
        min_rank = st.session_state["extract_min_rank"]
        max_rank = st.session_state["extract_max_rank"]
    else:
        col1, col2 = st.columns(2)
        min_rank = col1.number_input(
            "å¿½ç•¥å‰ N é«˜é¢‘è¯ (Min Rank)", 1, 50000, st.session_state["extract_min_rank"], step=100,
            key="extract_min_rank"
        )
        max_rank = col2.number_input(
            "å¿½ç•¥å N ä½é¢‘è¯ (Max Rank)", 2000, 50000, st.session_state["extract_max_rank"], step=500,
            key="extract_max_rank"
        )
        if max_rank < min_rank:
            st.warning("âš ï¸ Max Rank å°äº Min Rankï¼Œå·²è‡ªåŠ¨äº¤æ¢ä¸¤è€…ã€‚")
            min_rank, max_rank = max_rank, min_rank
            st.session_state["extract_min_rank"] = min_rank
            st.session_state["extract_max_rank"] = max_rank
    return min_rank, max_rank


with tab_extract_anki:
    # é€šç”¨ rank åŒºé—´ï¼ˆæ–‡æœ¬/é“¾æ¥/æ–‡ä»¶/è¯åº“å…±ç”¨ï¼›è¯åº“ç›´æ¥åˆ¶å¡ä¸ç­› rankï¼‰
    shared_min_rank, shared_max_rank = _render_shared_rank_selection()
    st.markdown("---")

    mode_input_text, mode_rank, mode_direct = st.tabs([
        "è¾“å…¥æ–‡æœ¬",
        "è¯åº“",
        "ç›´æ¥åˆ¶å¡",
    ])

    with mode_input_text:
        sub_paste, sub_upload, sub_url = st.tabs(["ç²˜è´´æ–‡æœ¬", "ä¸Šä¼ æ–‡ä»¶", "é“¾æ¥"])
        with sub_paste:
            pasted_text = st.text_area(
                "ç²˜è´´æ–‡ç« æ–‡æœ¬",
                height=100,
                key="paste_key_2_1",
                placeholder="æ”¯æŒç›´æ¥ç²˜è´´æ–‡ç« å†…å®¹..."
            )
            col_gen_p, col_clr_p = st.columns([4, 1])
            with col_gen_p:
                btn_paste = st.button("ğŸš€ ä»æ–‡æœ¬ç”Ÿæˆé‡ç‚¹è¯", type="primary", key="btn_mode_2_1", use_container_width=True)
            with col_clr_p:
                st.button("æ¸…ç©º", key="clr_paste", use_container_width=True,
                          on_click=lambda: st.session_state.update({"paste_key_2_1": ""}))
            if btn_paste:
                if shared_max_rank < shared_min_rank:
                    st.error("âŒ Max Rank å¿…é¡»å¤§äºç­‰äº Min Rankï¼Œè¯·åœ¨ä¸Šæ–¹ä¿®æ­£åé‡è¯•ã€‚")
                elif len(pasted_text) > constants.MAX_PASTE_TEXT_LENGTH:
                    st.error(f"âŒ æ–‡æœ¬è¿‡é•¿ï¼ˆæœ€å¤§çº¦ {constants.MAX_PASTE_TEXT_LENGTH // 1000}K å­—ç¬¦ï¼‰ï¼Œè¯·ç¼©çŸ­åé‡è¯•ã€‚")
                else:
                    with st.status("ğŸ” æ­£åœ¨åŠ è½½èµ„æºå¹¶åˆ†ææ–‡æœ¬...", expanded=True) as status:
                        start_time = time.time()
                        raw_text = pasted_text
                        status.write("ğŸ§  æ­£åœ¨è¿›è¡Œ NLP è¯å½¢è¿˜åŸä¸åˆ†çº§...")
                        if _analyze_and_set_words(raw_text, shared_min_rank, shared_max_rank):
                            st.session_state['process_time'] = time.time() - start_time
                            run_gc()
                            status.update(label="âœ… åˆ†æå®Œæˆ", state="complete", expanded=False)
                        else:
                            status.update(label="âš ï¸ å†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­", state="error")
        with sub_upload:
            uploaded_file = st.file_uploader(
                "ä¸Šä¼ æ–‡ä»¶ï¼ˆTXT/PDF/DOCX/EPUB/CSV/Excel/DBï¼‰",
                type=['txt', 'pdf', 'docx', 'epub', 'csv', 'xlsx', 'xls', 'db', 'sqlite'],
                key="upload_2_3",
            )
            if uploaded_file and is_upload_too_large(uploaded_file):
                st.error(f"âŒ æ–‡ä»¶è¿‡å¤§ï¼Œå·²é™åˆ¶ä¸º {constants.MAX_UPLOAD_MB}MBã€‚è¯·ç¼©å°æ–‡ä»¶åé‡è¯•ã€‚")
                uploaded_file = None
            if st.button("ğŸ“ ä»æ–‡ä»¶ç”Ÿæˆé‡ç‚¹è¯", type="primary", key="btn_mode_2_3"):
                if shared_max_rank < shared_min_rank:
                    st.error("âŒ Max Rank å¿…é¡»å¤§äºç­‰äº Min Rankï¼Œè¯·åœ¨ä¸Šæ–¹ä¿®æ­£åé‡è¯•ã€‚")
                elif uploaded_file is None:
                    st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ–‡ä»¶ã€‚")
                else:
                    with st.status("ğŸ“„ æ­£åœ¨è§£ææ–‡ä»¶å¹¶æå–é‡ç‚¹è¯...", expanded=True) as status:
                        start_time = time.time()
                        raw_text = extract_text_from_file(uploaded_file)
                        if _analyze_and_set_words(raw_text, shared_min_rank, shared_max_rank):
                            st.session_state['process_time'] = time.time() - start_time
                            run_gc()
                            status.update(label="âœ… ç”Ÿæˆå®Œæˆ", state="complete", expanded=False)
                        else:
                            status.update(label="âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­", state="error")
        with sub_url:
            input_url = st.text_input(
                "ğŸ”— è¾“å…¥æ–‡ç«  URLï¼ˆè‡ªåŠ¨æŠ“å–ï¼‰",
                placeholder="https://www.economist.com/...",
                key="url_input_key_2_2"
            )
            col_gen_u, col_clr_u = st.columns([4, 1])
            with col_gen_u:
                btn_url = st.button("ğŸŒ ä»é“¾æ¥ç”Ÿæˆé‡ç‚¹è¯", type="primary", key="btn_mode_2_2", use_container_width=True)
            with col_clr_u:
                st.button("æ¸…ç©º", key="clr_url", use_container_width=True,
                          on_click=lambda: st.session_state.update({"url_input_key_2_2": ""}))
            if btn_url:
                if shared_max_rank < shared_min_rank:
                    st.error("âŒ Max Rank å¿…é¡»å¤§äºç­‰äº Min Rankï¼Œè¯·åœ¨ä¸Šæ–¹ä¿®æ­£åé‡è¯•ã€‚")
                elif not input_url.strip():
                    st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé“¾æ¥ã€‚")
                elif len(input_url) > constants.MAX_URL_LENGTH:
                    st.error(f"âŒ URL è¿‡é•¿ï¼ˆæœ€å¤§ {constants.MAX_URL_LENGTH} å­—ç¬¦ï¼‰ã€‚")
                elif not re.match(r'^https?://', input_url.strip()):
                    st.error("âŒ è¯·è¾“å…¥ä»¥ http:// æˆ– https:// å¼€å¤´çš„æœ‰æ•ˆé“¾æ¥ã€‚")
                else:
                    url_allowed, url_msg = check_url_limit()
                    if not url_allowed:
                        st.warning(url_msg)
                    else:
                        record_url()
                        with st.status("ğŸŒ æ­£åœ¨æŠ“å–å¹¶åˆ†æç½‘é¡µæ–‡æœ¬...", expanded=True) as status:
                            start_time = time.time()
                            status.write(f"æ­£åœ¨æŠ“å–ï¼š{input_url}")
                            raw_text = _cached_extract_url(input_url)
                            if raw_text.startswith("Error:"):
                                st.error(f"âŒ {raw_text}")
                                status.update(label="âŒ æŠ“å–å¤±è´¥", state="error", expanded=False)
                            elif _analyze_and_set_words(raw_text, shared_min_rank, shared_max_rank):
                                st.session_state['process_time'] = time.time() - start_time
                                run_gc()
                                status.update(label="âœ… ç”Ÿæˆå®Œæˆ", state="complete", expanded=False)
                            else:
                                status.update(label="âš ï¸ æŠ“å–å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­", state="error")

    with mode_direct:
        st.markdown("#### ç›´æ¥ç²˜è´´è¯è¡¨ï¼ˆä¸åš rank ç­›é€‰ï¼‰")
        st.caption("ğŸ’¡ è¯è¡¨æ¨¡å¼ï¼šæ— éœ€ç­›é€‰ï¼Œç›´æ¥ç”Ÿæˆã€‚")
        st.caption("æ”¯æŒä»»æ„æ ¼å¼ï¼šå¯ç›´æ¥ç²˜è´´æ–‡ç« ã€åˆ—è¡¨ã€å¸¦åºå·æˆ–ç¬¦å·çš„æ–‡æœ¬ï¼Œå°†è‡ªåŠ¨æå–å…¶ä¸­æ‰€æœ‰è‹±æ–‡å•è¯ã€‚")
        manual_words_text = st.text_area(
            "âœï¸ ç²˜è´´ä»»æ„åŒ…å«è‹±æ–‡å•è¯çš„æ–‡æœ¬",
            height=220,
            key="manual_words_2_5",
            placeholder="å¦‚ï¼š1. altruism 2. hectic  æˆ– ç›´æ¥ç²˜è´´æ•´æ®µè‹±æ–‡â€¦",
        )

        col_gen_m, col_clr_m = st.columns([4, 1])
        with col_gen_m:
            btn_gen_manual = st.button("ğŸ§¾ ç”Ÿæˆè¯è¡¨ï¼ˆä¸ç­› rankï¼‰", key="btn_mode_2_5", type="primary", use_container_width=True)
        with col_clr_m:
            st.button("æ¸…ç©º", key="clr_manual_words", use_container_width=True,
                       on_click=lambda: st.session_state.update({"manual_words_2_5": ""}))

        if btn_gen_manual:
            with st.spinner("æ­£åœ¨è§£æåˆ—è¡¨..."):
                if manual_words_text.strip():
                    # å¼±åŒ–æ ¼å¼è¦æ±‚ï¼šä»æ•´æ®µæ–‡æœ¬ä¸­æå–æ‰€æœ‰è‹±æ–‡å•è¯ï¼Œè‡ªåŠ¨å»é™¤ç¬¦å·ã€ç©ºæ ¼ç­‰
                    valid_words = re.findall(r"[a-zA-Z]+(?:[-'][a-zA-Z]+)*", manual_words_text)
                    valid_words = [
                        w for w in valid_words
                        if constants.MIN_WORD_LENGTH <= len(w) <= constants.MAX_WORD_LENGTH
                    ]

                    if not valid_words:
                        st.error("âŒ æ²¡æœ‰è¯†åˆ«åˆ°æœ‰æ•ˆçš„è‹±æ–‡å•è¯ï¼ˆ2â€“25 å­—æ¯ï¼‰ï¼Œè¯·æ£€æŸ¥è¾“å…¥å†…å®¹ã€‚")
                    else:
                        seen_lemmas = set()
                        data_list = []
                        for word in valid_words:
                            w = word.strip()
                            if not w:
                                continue
                            lemma = get_lemma_for_word(w)
                            if lemma in seen_lemmas:
                                continue
                            seen_lemmas.add(lemma)
                            data_list.append((lemma, resources.get_rank_for_word(lemma)))

                        raw_count = len(valid_words)
                        set_generated_words_state(data_list, raw_count, None)
                        unique_count = len(data_list)
                        duplicated = raw_count - unique_count
                        msg = f"âœ… å·²åŠ è½½ {unique_count} ä¸ªå•è¯ï¼ˆä¸ç­› rankï¼Œå·²ç»Ÿä¸€ä¸º lemmaï¼‰"
                        if duplicated > 0:
                            msg += f"ï¼ˆå»é‡ {duplicated} ä¸ªï¼‰"
                        st.toast(msg, icon="ğŸ‰")
                else:
                    st.warning("âš ï¸ å†…å®¹ä¸ºç©ºã€‚")

    with mode_rank:
        st.caption("ä½¿ç”¨ä¸Šæ–¹ã€Œè¯æ±‡é‡ rank é€‰æ‹©ã€çš„åŒºé—´ï¼›é¡ºåºç”Ÿæˆä»åŒºé—´èµ·ç‚¹èµ·å–ï¼ŒéšæœºæŠ½å–åœ¨åŒºé—´å†…éšæœºå–ã€‚")
        gen_type = st.radio("ç”Ÿæˆæ¨¡å¼", ["ğŸ”¢ é¡ºåºç”Ÿæˆ", "ğŸ”€ éšæœºæŠ½å–"], horizontal=True)

        if "é¡ºåºç”Ÿæˆ" in gen_type:
            # å½“ rank é€‰æ‹©å˜åŒ–æ—¶ï¼Œèµ·å§‹æ’åè·Ÿéš shared_min_rank æ›´æ–°
            if st.session_state.get("rank_start_sync_min") != shared_min_rank:
                st.session_state["rank_start_2_4"] = shared_min_rank
                st.session_state["rank_start_sync_min"] = shared_min_rank
            col_a, col_b = st.columns(2)
            start_rank = col_a.number_input("èµ·å§‹æ’å", 1, 50000, shared_min_rank, step=100, key="rank_start_2_4")
            count = col_b.number_input("æ•°é‡", 10, 5000, 10, step=10)

            if st.button("ğŸš€ ç”Ÿæˆåˆ—è¡¨"):
                with st.spinner("æ­£åœ¨æå–..."):
                    if FULL_DF is not None:
                        rank_col = next((c for c in FULL_DF.columns if 'rank' in c), None)
                        word_col = next((c for c in FULL_DF.columns if 'word' in c), None)
                        if rank_col is None or word_col is None:
                            st.error("âŒ è¯åº“CSVæ ¼å¼å¼‚å¸¸ï¼šç¼ºå°‘ rank æˆ– word åˆ—")
                        else:
                            subset = FULL_DF[FULL_DF[rank_col] >= start_rank].sort_values(rank_col).head(count)
                            set_generated_words_state(
                                list(zip(subset[word_col], subset[rank_col])),
                                0,
                                None
                            )
        else:
            random_count = st.number_input("æŠ½å–æ•°é‡", 10, 5000, 10, step=10, key="rank_random_count_2_4")
            st.caption(f"å½“å‰åŒºé—´ï¼š{shared_min_rank} â€“ {shared_max_rank}ï¼ˆåœ¨ä¸Šæ–¹ä¿®æ”¹ï¼‰")

            if st.button("ğŸ² éšæœºæŠ½å–"):
                if shared_max_rank < shared_min_rank:
                    st.error("âŒ æœ€å¤§æ’åå¿…é¡»å¤§äºç­‰äºæœ€å°æ’åï¼Œè¯·åœ¨ä¸Šæ–¹ä¿®æ­£åé‡è¯•ã€‚")
                else:
                    with st.spinner("æ­£åœ¨æŠ½å–..."):
                        if FULL_DF is not None:
                            rank_col = next((c for c in FULL_DF.columns if 'rank' in c), None)
                            word_col = next((c for c in FULL_DF.columns if 'word' in c), None)
                            if rank_col is None or word_col is None:
                                st.error("âŒ è¯åº“CSVæ ¼å¼å¼‚å¸¸ï¼šç¼ºå°‘ rank æˆ– word åˆ—")
                            else:
                                pool = FULL_DF[(FULL_DF[rank_col] >= shared_min_rank) & (FULL_DF[rank_col] <= shared_max_rank)]
                                if len(pool) < random_count:
                                    st.warning(f"âš ï¸ è¯¥èŒƒå›´åªæœ‰ {len(pool)} ä¸ªå•è¯ï¼Œå·²å…¨éƒ¨é€‰ä¸­")
                                sample = pool.sample(n=min(random_count, len(pool)))
                                set_generated_words_state(
                                    list(zip(sample[word_col], sample[rank_col])),
                                    0,
                                    None
                                )

    # Display results (shared across all modes)
    _render_extract_results()

st.markdown(
    '<div class="app-footer">Vocab Flow Ultra &nbsp;Â·&nbsp; Built for learners</div>',
    unsafe_allow_html=True
)
