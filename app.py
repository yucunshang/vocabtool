import streamlit as st
import pandas as pd
import re
import os
import time
from datetime import datetime, timedelta, timezone
import lemminflect
import nltk
import genanki
import random
import tempfile
from bs4 import BeautifulSoup

# --- æ–‡ä»¶å¤„ç†åº“ ---
import pypdf
import docx
import ebooklib
from ebooklib import epub

# ==========================================
# 0. é¡µé¢é…ç½®
# ==========================================
st.set_page_config(page_title="Vocab Flow Ultra", page_icon="âš¡ï¸", layout="centered", initial_sidebar_state="collapsed")

st.markdown("""
<style>
    .stTextArea textarea { font-family: 'Consolas', monospace; font-size: 14px; }
    .stButton>button { border-radius: 8px; font-weight: 600; width: 100%; margin-top: 5px; }
    .stat-box { padding: 15px; background-color: #f0fdf4; border: 1px solid #bbf7d0; border-radius: 8px; text-align: center; color: #166534; margin-bottom: 20px; }
    .preview-table { font-size: 12px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºåŠ è½½
# ==========================================
@st.cache_resource
def setup_nltk():
    try:
        root_dir = os.path.dirname(os.path.abspath(__file__))
        nltk_data_dir = os.path.join(root_dir, 'nltk_data')
        os.makedirs(nltk_data_dir, exist_ok=True)
        nltk.data.path.append(nltk_data_dir)
        for pkg in ['averaged_perceptron_tagger', 'punkt', 'punkt_tab']:
            try: nltk.data.find(f'tokenizers/{pkg}')
            except LookupError: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
    except: pass
setup_nltk()

@st.cache_data
def load_vocab_data():
    """åŠ è½½è¯é¢‘è¡¨"""
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            df.columns = [c.strip().lower() for c in df.columns]
            w_col = next((c for c in df.columns if 'word' in c), df.columns[0])
            r_col = next((c for c in df.columns if 'rank' in c), df.columns[1])
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            df = df.sort_values(r_col).drop_duplicates(subset=[w_col], keep='first')
            return pd.Series(df[r_col].values, index=df[w_col]).to_dict(), df
        except: return {}, None
    return {}, None

VOCAB_DICT, FULL_DF = load_vocab_data()

def get_lemma(word):
    try: return lemminflect.getLemma(word, upos='VERB')[0]
    except: return word

def get_beijing_time_str():
    utc_now = datetime.now(timezone.utc)
    beijing_now = utc_now + timedelta(hours=8)
    return beijing_now.strftime('%m%d_%H%M')

def clear_all_state():
    st.session_state.clear()

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘: æ–‡æœ¬è§£æ (Smart Parsing)
# ==========================================
def extract_text_from_file(uploaded_file):
    text = ""
    file_type = uploaded_file.name.split('.')[-1].lower()
    try:
        if file_type == 'txt':
            text = uploaded_file.getvalue().decode("utf-8", errors='ignore')
        elif file_type == 'pdf':
            reader = pypdf.PdfReader(uploaded_file)
            text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif file_type == 'docx':
            doc = docx.Document(uploaded_file)
            text = "\n".join([p.text for p in doc.paragraphs])
        elif file_type == 'epub':
            with tempfile.NamedTemporaryFile(delete=False, suffix='.epub') as tmp:
                tmp.write(uploaded_file.getvalue())
                tmp_path = tmp.name
            book = epub.read_epub(tmp_path)
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    text += soup.get_text() + " "
            os.remove(tmp_path)
    except Exception as e:
        return f"Error: {e}"
    return text

def analyze_logic(text, current_lvl, target_lvl):
    raw_tokens = re.findall(r"[a-z]+", text.lower())
    total_words = len(raw_tokens)
    unique_tokens = set(raw_tokens)
    
    target_words = []
    for w in unique_tokens:
        if len(w) < 3: continue 
        lemma = get_lemma(w)
        rank = VOCAB_DICT.get(lemma, 99999)
        if rank > current_lvl and rank <= target_lvl:
            target_words.append((lemma, rank))
            
    target_words.sort(key=lambda x: x[1])
    return [x[0] for x in target_words], total_words

def parse_anki_data(raw_text):
    """
    æ™ºèƒ½è§£æå‡½æ•°ï¼šå¤„ç† Markdown è¡¨æ ¼ã€å¤šä½™ç®¡é“ç¬¦ã€è¡¨å¤´ç­‰é—®é¢˜
    """
    parsed_cards = []
    lines = raw_text.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        if not line: continue
        
        # 1. è¿‡æ»¤ Markdown åˆ†å‰²çº¿ (å¦‚ |---|---|)
        if set(line) == {'|', '-'} or '---' in line: continue
        
        # 2. è¿‡æ»¤è¡¨å¤´
        if 'Word' in line and 'Definition' in line: continue

        # 3. ç§»é™¤è¡Œé¦–è¡Œå°¾çš„ Markdown ç®¡é“ç¬¦ (é‡è¦!)
        # å¾ˆå¤šæ—¶å€™ AI ä¼šè¾“å‡º "| word | ipa |"ï¼Œå¦‚æœä¸ strip('|')ï¼Œsplitåç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œå¯¼è‡´é”™ä½
        clean_line = line.strip('|')
        
        # 4. åˆ†å‰²
        parts = [p.strip() for p in clean_line.split('|')]
        
        # 5. æœ‰æ•ˆæ€§æ£€æŸ¥ (è‡³å°‘è¦æœ‰å•è¯å’Œé‡Šä¹‰)
        if len(parts) >= 2:
            # è‡ªåŠ¨è¡¥å…¨ç¼ºå¤±å­—æ®µï¼Œé˜²æ­¢åˆ—è¡¨ç´¢å¼•è¶Šç•Œ
            while len(parts) < 5:
                parts.append("")
                
            parsed_cards.append({
                'word': parts[0],
                'ipa': parts[1],
                'meaning': parts[2],
                'examples': parts[3],
                'etymology': parts[4]
            })
            
    return parsed_cards

# ==========================================
# 3. Anki ç”Ÿæˆé€»è¾‘
# ==========================================
def generate_anki_package(cards_data, deck_name):
    # CSS æ ·å¼å¢å¼º
    CSS = """
    .card { font-family: arial; font-size: 20px; text-align: center; color: #333; background-color: white; padding: 20px; }
    .nightMode .card { background-color: #2f2f31; color: #f5f5f5; }
    .word { font-size: 40px; font-weight: bold; color: #007AFF; margin-bottom: 10px; }
    .nightMode .word { color: #5FA9FF; }
    .phonetic { color: #888; font-size: 18px; font-family: sans-serif; margin-bottom: 15px; }
    .def-container { text-align: left; margin-top: 20px; border-top: 1px solid #ddd; padding-top: 15px; }
    .definition { font-weight: bold; color: #222; margin-bottom: 15px; font-size: 22px; }
    .examples { background: #f4f4f4; padding: 15px; border-radius: 8px; color: #444; font-style: italic; font-size: 20px; line-height: 1.4; margin-bottom: 15px; }
    .etymology { display: block; font-size: 18px; color: #555; border: 1px dashed #bbb; padding: 8px 12px; border-radius: 6px; background-color: #fffaf0; margin-top: 10px; }
    """
    
    model_id = random.randrange(1 << 30, 1 << 31)
    model = genanki.Model(
        model_id, 
        f'VocabFlow Model {model_id}',
        fields=[{'name': 'Word'}, {'name': 'IPA'}, {'name': 'Meaning'}, {'name': 'Examples'}, {'name': 'Etymology'}],
        templates=[{
            'name': 'Card 1',
            'qfmt': '<div class="word">{{Word}}</div><div class="phonetic">{{IPA}}</div>',
            'afmt': '''{{FrontSide}}<div class="def-container"><div class="definition">{{Meaning}}</div><div class="examples">{{Examples}}</div><div class="etymology">ğŸŒ± <b>Etymology:</b> {{Etymology}}</div></div>''',
        }], css=CSS
    )
    
    deck = genanki.Deck(random.randrange(1 << 30, 1 << 31), deck_name)
    
    for c in cards_data:
        deck.add_note(genanki.Note(
            model=model, 
            fields=[
                str(c['word']), str(c['ipa']), str(c['meaning']), 
                str(c['examples']).replace('\n','<br>'), str(c['etymology'])
            ]
        ))
        
    with tempfile.NamedTemporaryFile(delete=False, suffix='.apkg') as tmp:
        genanki.Package(deck).write_to_file(tmp.name)
        return tmp.name

def get_ai_prompt(words):
    w_list = ", ".join(words)
    return f"""
Act as a Dictionary API. Output strictly formatted data for Anki cards.

**Words:** {w_list}

**Strict Output Rules:**
1. **Format:** `Word | IPA | Definition | Examples | Etymology`
2. **NO Markdown Tables:** Do NOT output headers. Just the data lines.
3. **Separator:** Use `|` ONLY to separate fields.
4. **Mandatory Fields:**
   - Definition: Concise (<12 words).
   - Examples: 2 sentences, separated by `<br>`.
   - **Etymology:** REQUIRED. (e.g., "root(meaning)+suffix").

**Example:**
benevolent | /bÉ™ËˆnevÉ™lÉ™nt/ | kind and helpful | He is **benevolent**.<br>A **benevolent** fund. | bene(good) + vol(wish)
"""

# ==========================================
# 4. ä¸»ç¨‹åº UI
# ==========================================
st.title("âš¡ï¸ Vocab Flow Ultra")

if not VOCAB_DICT:
    st.error("âš ï¸ ç¼ºå¤± `coca_cleaned.csv`")

tab_extract, tab_anki = st.tabs(["1ï¸âƒ£ å•è¯æå– & ç”Ÿæˆ", "2ï¸âƒ£ åˆ¶ä½œ Anki ç‰Œç»„"])

# --- TAB 1: æå– ---
with tab_extract:
    mode_context, mode_rank = st.tabs(["ğŸ“„ è¯­å¢ƒåˆ†æ (æ–‡æœ¬/æ–‡ä»¶)", "ğŸ”¢ è¯é¢‘åˆ—è¡¨ (Rank & Random)"])
    
    with mode_context:
        c1, c2 = st.columns(2)
        curr = c1.number_input("å¿½ç•¥å¤ªç®€å•çš„ (Current Level)", 1000, 20000, 4000, step=500)
        targ = c2.number_input("å¿½ç•¥å¤ªéš¾çš„ (Target Level)", 2000, 50000, 15000, step=500)
        uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£ (PDF/TXT/DOCX/EPUB)")
        pasted_text = st.text_area("ğŸ“„ ...æˆ–ç²˜è´´æ–‡æœ¬", height=100)
        
        if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
            raw_text = extract_text_from_file(uploaded_file) if uploaded_file else pasted_text
            if len(raw_text) > 10:
                final_words, total = analyze_logic(raw_text, curr, targ)
                st.session_state['gen_words'] = final_words
                st.session_state['total_count'] = total
            else: st.warning("å†…å®¹æ— æ•ˆ")
            
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®", type="secondary", on_click=clear_all_state): pass

    with mode_rank:
        gen_type = st.radio("ç”Ÿæˆæ¨¡å¼", ["ğŸ”¢ é¡ºåºæˆªå–", "ğŸ”€ èŒƒå›´éšæœº (Random)"], horizontal=True)
        if "é¡ºåº" in gen_type:
            c_a, c_b = st.columns(2)
            s_rank = c_a.number_input("èµ·å§‹æ’å", 1, 20000, 8000, step=100)
            count = c_b.number_input("æ•°é‡", 10, 500, 50, step=10)
            if st.button("ğŸš€ ç”Ÿæˆé¡ºåºåˆ—è¡¨"):
                if FULL_DF is not None:
                    r_col = next(c for c in FULL_DF.columns if 'rank' in c)
                    w_col = next(c for c in FULL_DF.columns if 'word' in c)
                    subset = FULL_DF[FULL_DF[r_col] >= s_rank].sort_values(r_col).head(count)
                    st.session_state['gen_words'] = subset[w_col].tolist()
                    st.session_state['total_count'] = count
        else:
            c_min, c_max, c_cnt = st.columns([1,1,1])
            min_r = c_min.number_input("Min Rank", 1, 20000, 6000, step=500)
            max_r = c_max.number_input("Max Rank", 1, 25000, 8000, step=500)
            r_count = c_cnt.number_input("Random Count", 10, 200, 50, step=10)
            if st.button("ğŸ² éšæœºæŠ½å–"):
                if FULL_DF is not None:
                    r_col = next(c for c in FULL_DF.columns if 'rank' in c)
                    w_col = next(c for c in FULL_DF.columns if 'word' in c)
                    mask = (FULL_DF[r_col] >= min_r) & (FULL_DF[r_col] <= max_r)
                    candidates = FULL_DF[mask]
                    if len(candidates) > 0:
                        subset = candidates.sample(n=min(r_count, len(candidates))).sort_values(r_col)
                        st.session_state['gen_words'] = subset[w_col].tolist()
                        st.session_state['total_count'] = len(subset)
                        st.success(f"æŠ½å–äº† {len(subset)} ä¸ªå•è¯")
                    else: st.error("è¯¥èŒƒå›´æ— å•è¯")

    if 'gen_words' in st.session_state and st.session_state['gen_words']:
        words = st.session_state['gen_words']
        st.divider()
        st.info(f"ğŸ¯ å‡†å¤‡ç”Ÿæˆ **{len(words)}** ä¸ªå•è¯")
        
        batch_size = st.number_input("AI åˆ†ç»„å¤§å°", 10, 200, 50)
        batches = [words[i:i + batch_size] for i in range(0, len(words), batch_size)]
        
        for idx, batch in enumerate(batches):
            with st.expander(f"ç¬¬ {idx+1} ç»„ (å•è¯ {idx*batch_size+1} - {idx*batch_size+len(batch)})", expanded=(idx==0)):
                st.code(get_ai_prompt(batch), language="markdown")

# --- TAB 2: Anki ---
with tab_anki:
    st.markdown("### ğŸ“¦ åˆ¶ä½œ Anki ç‰Œç»„")
    bj_time_str = get_beijing_time_str()
    
    if 'anki_input_text' not in st.session_state:
        st.session_state['anki_input_text'] = ""

    ai_resp = st.text_area(
        "åœ¨æ­¤ç²˜è´´ AI çš„å›å¤å†…å®¹ (ä¼šè‡ªåŠ¨ä¿®æ­£æ ¼å¼é”™è¯¯)", 
        height=200, 
        key="anki_input_text"
    )
    
    deck_name = st.text_input("ç‰Œç»„åç§°", f"Vocab_{bj_time_str}")
    
    # --- å®æ—¶é¢„è§ˆé€»è¾‘ ---
    if ai_resp.strip():
        # è°ƒç”¨æ™ºèƒ½è§£æ
        parsed_data = parse_anki_data(ai_resp)
        
        if parsed_data:
            st.markdown("#### ğŸ‘ï¸ é¢„è§ˆè§£æç»“æœ (Verify before Download)")
            st.caption("è¯·æ£€æŸ¥ä¸‹è¡¨ã€‚å¦‚æœè¡¨æ ¼åˆ—æ˜¯å¯¹é½çš„ï¼ŒAnki å¡ç‰‡å°±æ˜¯æ­£å¸¸çš„ã€‚")
            
            # å±•ç¤ºé¢„è§ˆè¡¨æ ¼ (åªå–å‰å‡ åˆ—ç”¨äºå±•ç¤º)
            df_preview = pd.DataFrame(parsed_data)
            st.dataframe(df_preview, use_container_width=True, hide_index=True)
            
            st.success(f"âœ… æˆåŠŸè§£æ {len(parsed_data)} æ¡æ•°æ®")
            
            # ä¸‹è½½æŒ‰é’®åªåœ¨æ•°æ®æœ‰æ•ˆæ—¶å‡ºç°
            f_path = generate_anki_package(parsed_data, deck_name)
            with open(f_path, "rb") as f:
                st.download_button(
                    label=f"ğŸ“¥ ä¸‹è½½ {deck_name}.apkg",
                    data=f,
                    file_name=f"{deck_name}.apkg",
                    mime="application/octet-stream",
                    type="primary"
                )
        else:
            st.warning("âš ï¸ å†…å®¹æ— æ³•è§£æï¼Œè¯·æ£€æŸ¥æ˜¯å¦åŒ…å« '|' åˆ†éš”ç¬¦")