import streamlit as st
import pandas as pd
import re
import os
import json
import time
import requests
import zipfile
import concurrent.futures
from pathlib import Path
from functools import lru_cache
from typing import Dict, List, Set, Tuple, Any, Optional

# ==========================================
# 0. ä¾èµ–æ£€æŸ¥ä¸å¯¼å…¥
# ==========================================
try:
    import lemminflect
    import nltk
except ImportError:
    st.error("âš ï¸ ç¼ºå°‘æ ¸å¿ƒ NLP ä¾èµ–ã€‚è¯·è¿è¡Œ: pip install lemminflect nltk")
    st.stop()

try:
    import PyPDF2
    import docx
except ImportError:
    st.warning("âš ï¸ ç¼ºå°‘æ–‡æ¡£å¤„ç†ä¾èµ– (PyPDF2, python-docx)ã€‚PDF å’Œ DOCX è§£æå°†ä¸å¯ç”¨ã€‚")

# ==========================================
# 1. åŸºç¡€é…ç½® & CSS
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode { font-family: 'Consolas', 'Courier New', monospace !important; font-size: 16px !important; }
    header {visibility: hidden;} 
    .block-container { padding-top: 1rem; }
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    [data-testid="stMetricValue"] { font-size: 28px !important; color: var(--primary-color) !important; }
    .param-box { background-color: var(--secondary-background-color); padding: 15px 20px 5px 20px; border-radius: 10px; border: 1px solid var(--border-color-light); margin-bottom: 20px; }
    .copy-hint { color: #888; font-size: 14px; margin-bottom: 5px; margin-top: 10px; padding-left: 5px; }
    /* è¿›åº¦æ¡æ ·å¼ä¼˜åŒ– */
    .stProgress > div > div > div > div { background-color: #00CC96; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. èµ„æºç®¡ç†ä¸ NLP å¼•æ“
# ==========================================
DATA_DIR = Path("data")
NLTK_DIR = Path(__file__).parent / "nltk_data"

@st.cache_resource
def setup_nltk():
    """åˆå§‹åŒ– NLTKï¼Œç¡®ä¿æ•°æ®å­˜åœ¨"""
    os.makedirs(NLTK_DIR, exist_ok=True)
    nltk.data.path.append(str(NLTK_DIR))
    for pkg in ['averaged_perceptron_tagger', 'punkt']:
        try:
            nltk.data.find(f'tokenizers/{pkg}')
        except LookupError:
            try:
                nltk.download(pkg, download_dir=str(NLTK_DIR), quiet=True)
            except Exception as e:
                st.warning(f"NLTK {pkg} ä¸‹è½½å¤±è´¥: {e}")

setup_nltk()

@st.cache_data
def load_knowledge_base() -> Tuple[Dict, Dict, Dict, Set]:
    """åŠ è½½æœ¬åœ° JSON çŸ¥è¯†åº“ï¼Œå…·å¤‡å®¹é”™èƒ½åŠ›"""
    def load_json_safe(filename: str, default_val: Any) -> Any:
        file_path = DATA_DIR / filename
        if not file_path.exists():
            return default_val
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return default_val

    terms = {k.lower(): v for k, v in load_json_safe('terms.json', {}).items()}
    proper = {k.lower(): v for k, v in load_json_safe('proper.json', {}).items()}
    patch = load_json_safe('patch.json', {})
    ambiguous = set(load_json_safe('ambiguous.json', []))
    
    return terms, proper, patch, ambiguous

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

@st.cache_data
def load_vocab() -> Dict[str, int]:
    """åŠ è½½è¯é¢‘è¡¨ (Rank)"""
    vocab = {}
    # ä¼˜å…ˆæŸ¥æ‰¾å­˜åœ¨çš„ CSV æ–‡ä»¶
    possible_files = ["coca_cleaned.csv", "data.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    
    if file_path:
        try:
            df = pd.read_csv(file_path)
            # æ›´åŠ é²æ£’çš„åˆ—ååŒ¹é…
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            
            # åŠ¨æ€å¯»æ‰¾ word å’Œ rank åˆ—
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), None)
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), None)
            
            if w_col and r_col:
                df[w_col] = df[w_col].astype(str).str.lower().str.strip()
                df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
                df = df.sort_values(r_col, ascending=True).drop_duplicates(subset=[w_col], keep='first')
                vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except Exception as e:
            st.error(f"è¯é¢‘è¡¨åŠ è½½å¼‚å¸¸: {e}")

    # åˆå¹¶è¡¥ä¸è¯åº“
    for word, rank in BUILTIN_PATCH_VOCAB.items():
        vocab[word] = rank
        
    # ç´§æ€¥ç¡¬ç¼–ç è¦†ç›– (Urgent Overrides)
    URGENT_OVERRIDES = {
        "china": 400, "turkey": 1500, "march": 500, "may": 100, "august": 1500, "polish": 2500,
        "monday": 300, "tuesday": 300, "wednesday": 300, "thursday": 300, "friday": 300, 
        "saturday": 300, "sunday": 300, "january": 400, "february": 400, "april": 400, 
        "june": 400, "july": 400, "september": 400, "october": 400, "november": 400, 
        "december": 400, "usa": 200, "uk": 200, "google": 1000, "apple": 1000, "microsoft": 1500
    }
    vocab.update(URGENT_OVERRIDES)
    return vocab

vocab_dict = load_vocab()

# âš¡ æ€§èƒ½ä¼˜åŒ–: LRU ç¼“å­˜é¿å…é‡å¤è®¡ç®—
@lru_cache(maxsize=10000)
def get_lemma(w: str) -> str:
    """è·å–å•è¯çš„è¯å…ƒ (Lemma)ï¼Œå¸¦ç¼“å­˜"""
    lemmas_dict = lemminflect.getAllLemmas(w)
    if not lemmas_dict:
        return w.lower()
    # ä¼˜å…ˆé¡ºåº: å½¢å®¹è¯ > å‰¯è¯ > åŠ¨è¯ > åè¯ (æ ¹æ®ç»éªŒè°ƒæ•´)
    for pos in ['ADJ', 'ADV', 'VERB', 'NOUN']:
        if pos in lemmas_dict:
            return lemmas_dict[pos][0]
    return list(lemmas_dict.values())[0][0]

# ==========================================
# 3. æ–‡æ¡£è§£æå¼•æ“
# ==========================================
def extract_text_from_file(uploaded_file) -> str:
    """ä»ä¸åŒæ ¼å¼æ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    if uploaded_file is None:
        return ""
        
    ext = uploaded_file.name.split('.')[-1].lower()
    uploaded_file.seek(0)
    
    try:
        if ext == 'txt':
            return uploaded_file.getvalue().decode("utf-8", errors="ignore")
        
        elif ext == 'pdf':
            if 'PyPDF2' not in globals(): return "âš ï¸ ç¼ºå°‘ PyPDF2 åº“"
            reader = PyPDF2.PdfReader(uploaded_file)
            return " ".join([page.extract_text() or "" for page in reader.pages])
            
        elif ext == 'docx':
            if 'docx' not in globals(): return "âš ï¸ ç¼ºå°‘ python-docx åº“"
            doc = docx.Document(uploaded_file)
            return " ".join([p.text for p in doc.paragraphs])
            
        elif ext == 'epub':
            text_blocks = []
            with zipfile.ZipFile(uploaded_file) as z:
                for filename in z.namelist():
                    if filename.endswith(('.html', '.xhtml', '.htm', '.xml')):
                        try:
                            content = z.read(filename).decode('utf-8', errors='ignore')
                            # ç®€å•çš„æ­£åˆ™å»é™¤ HTML æ ‡ç­¾
                            clean_text = re.sub(r'<[^>]+>', ' ', content)
                            text_blocks.append(clean_text)
                        except: pass
            return " ".join(text_blocks)
            
    except Exception as e:
        st.error(f"æ–‡ä»¶è§£æå¤±è´¥ ({ext}): {e}")
        return ""
    return ""

# ==========================================
# 4. API å¼•æ“ (å¥å£®å¹¶å‘ç‰ˆ)
# ==========================================
def get_base_prompt_template(export_format="TXT"):
    return f"""ã€è§’è‰²è®¾å®šã€‘ ä½ æ˜¯ä¸€ä½ç²¾é€šè¯æºå­¦ã€è®¤çŸ¥å¿ƒç†å­¦ä»¥åŠ Anki ç®—æ³•çš„â€œè‹±è¯­è¯æ±‡ä¸“å®¶ä¸é—ªå¡åˆ¶ä½œå¤§å¸ˆâ€ã€‚æ¥ä¸‹æ¥çš„å¯¹è¯ä¸­ï¼Œè¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹ 5 é¡¹åˆ¶å¡æ ‡å‡†ï¼Œå¤„ç†æˆ‘æä¾›çš„æ‰€æœ‰å•è¯åˆ—è¡¨ï¼š

1. æ ¸å¿ƒåŸåˆ™ï¼šåŸå­æ€§ (Atomicity)
å«ä¹‰æ‹†åˆ†ï¼šè‹¥ä¸€ä¸ªå•è¯æœ‰å¤šä¸ªå¸¸ç”¨å«ä¹‰ï¼ˆåè¯ vs åŠ¨è¯ï¼Œå­—é¢ä¹‰ vs å¼•ç”³ä¹‰ç­‰ï¼‰ï¼Œå¿…é¡»æ‹†åˆ†ä¸ºå¤šæ¡ç‹¬ç«‹æ•°æ®ã€‚
ä¸¥ç¦å †ç Œï¼šæ¯å¼ å¡ç‰‡åªæ‰¿è½½ä¸€ä¸ªç‰¹å®šè¯­å¢ƒä¸‹çš„å«ä¹‰ï¼Œä¸å‡†å°†å¤šä¸ªé‡Šä¹‰æŒ¤åœ¨ä¸€èµ·ã€‚
2. å¡ç‰‡æ­£é¢ (Column 1: Front)
å†…å®¹ï¼šæä¾›è‡ªç„¶çš„çŸ­è¯­æˆ–æ­é… (Phrase/Collocation)ï¼Œè€Œéå•ä¸ªå­¤ç«‹å•è¯ã€‚
3. å¡ç‰‡èƒŒé¢ (Column 2: Back - æ•´åˆé¡µ)
èƒŒé¢ä¿¡æ¯å¿…é¡»å…¨éƒ¨åˆå¹¶åœ¨ç¬¬äºŒåˆ—ï¼Œå¹¶ä½¿ç”¨ HTML æ ‡ç­¾æ’ç‰ˆã€‚
ç»“æ„ç¤ºä¾‹ï¼šè‹±æ–‡é‡Šä¹‰<br><br><em>æ–œä½“ä¾‹å¥</em><br><br>ã€è¯æ ¹ã€è¯æºã€è¯ç¼€ã€‘çš„ä¸­æ–‡è§£æ
4. è¾“å‡ºæ ¼å¼æ ‡å‡† ({export_format} æ ¼å¼)
æ–‡ä»¶è§„èŒƒï¼šçº¯æ–‡æœ¬ä»£ç å—ã€‚
åˆ†éš”ç¬¦ï¼šä½¿ç”¨é€—å· (Comma) åˆ†éš”å­—æ®µã€‚
å¼•å·åŒ…è£¹ï¼šæ¯ä¸ªå­—æ®µå¿…é¡»ç”¨åŒå¼•å· ("...") åŒ…è£¹ã€‚
5. æ•°æ®æ¸…æ´—ä¸ä¼˜åŒ–
æ‹¼å†™ä¿®æ­£ï¼šè‡ªåŠ¨ä¿®æ­£ç”¨æˆ·åˆ—è¡¨ä¸­çš„æ˜æ˜¾æ‹¼å†™é”™è¯¯ã€‚
ç¼©å†™å±•å¼€ï¼šå¯¹ç¼©å†™ï¼ˆå¦‚ WFH, akaï¼‰åœ¨èƒŒé¢æä¾›å…¨ç§°åŠè§£é‡Šã€‚
ğŸ’¡ æœ€ç»ˆè¾“å‡ºç¤ºä¾‹ï¼ˆ{export_format} å†…å®¹ï¼‰ï¼š
"run a business","to manage or operate a company<br><br><em>He quit his job to run a business selling handmade crafts.</em><br><br>ã€è¯æºã€‘æºè‡ªå¤è‹±è¯­ rinnanï¼ˆè·‘/æµåŠ¨ï¼‰ï¼Œå¼•ç”³ä¸ºâ€œä½¿æœºå™¨è¿è½¬â€æˆ–â€œä½¿ä¸šåŠ¡æµè½¬â€"
"""

def _fetch_deepseek_chunk_safe(batch_words: List[str], prompt_template: str, api_key: str) -> str:
    """Worker å‡½æ•°ï¼šå¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼Œä¸åŒ…å«ä»»ä½• UI æ“ä½œ"""
    url = "https://api.deepseek.com/chat/completions"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    system_enforcement = "\n\nã€ç³»ç»Ÿç»å¯¹å¼ºåˆ¶æŒ‡ä»¤ã€‘ç°åœ¨æˆ‘å·²ç»å‘é€äº†å•è¯åˆ—è¡¨ï¼Œè¯·ç«‹å³ä¸”ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„æ•°æ®ä»£ç ï¼Œç»å¯¹ä¸å‡†å›å¤â€œå¥½çš„â€ã€â€œæ²¡é—®é¢˜â€ç­‰ä»»ä½•å®¢å¥—è¯ï¼Œç»å¯¹ä¸å‡†ä½¿ç”¨ ```csv ç­‰ Markdown è¯­æ³•åŒ…è£¹ä»£ç ï¼"
    full_prompt = f"{prompt_template}{system_enforcement}\n\nå¾…å¤„ç†å•è¯åˆ—è¡¨ï¼š\n{', '.join(batch_words)}"
    
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": full_prompt}],
        "temperature": 0.3,
        "max_tokens": 4096
    }
    
    # æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥
    for attempt in range(3):
        try:
            resp = requests.post(url, json=payload, headers=headers, timeout=60)
            if resp.status_code == 429: # é™æµ
                time.sleep(2 * (attempt + 1))
                continue
            if resp.status_code == 402: return "ERROR: NO_BALANCE"
            if resp.status_code == 401: return "ERROR: INVALID_KEY"
            
            resp.raise_for_status()
            
            result = resp.json()['choices'][0]['message']['content'].strip()
            
            # æ¸…æ´— Markdown æ ‡è®°
            if result.startswith("```"):
                lines = result.split('\n')
                if lines[0].startswith("```"): lines = lines[1:]
                if lines and lines[-1].startswith("```"): lines = lines[:-1]
                result = '\n'.join(lines).strip()
            return result
            
        except requests.exceptions.RequestException as e:
            if attempt == 2: return f"ERROR: Request failed: {str(e)}"
            time.sleep(1)
            
    return "ERROR: Timeout"

def call_deepseek_api_main_thread_managed(prompt_template, words, progress_bar, status_container):
    """ä¸»çº¿ç¨‹ç®¡ç†çš„å¹¶å‘æ§åˆ¶å™¨"""
    try: 
        api_key = st.secrets["DEEPSEEK_API_KEY"]
    except (KeyError, FileNotFoundError):
        return "âš ï¸ æœªé…ç½® DEEPSEEK_API_KEYï¼Œè¯·åœ¨ .streamlit/secrets.toml ä¸­é…ç½®ã€‚"
    
    if not words: return "âš ï¸ æ²¡æœ‰éœ€è¦ç”Ÿæˆçš„å•è¯ã€‚"
    
    # é™åˆ¶å•æ¬¡è¯·æ±‚é‡
    MAX_WORDS = 300
    if len(words) > MAX_WORDS:
        st.toast(f"âš ï¸ å•è¯æ•°é‡è¿‡å¤šï¼Œå·²æˆªå–å‰ {MAX_WORDS} ä¸ªå•è¯è¿›è¡Œå¤„ç†ã€‚", icon="âœ‚ï¸")
        words = words[:MAX_WORDS]

    CHUNK_SIZE = 30  
    chunks = [words[i:i + CHUNK_SIZE] for i in range(0, len(words), CHUNK_SIZE)]
    total_chunks = len(chunks)
    
    results_map = {} # {index: result_text}
    
    # ä½¿ç”¨ st.status æä¾›æ›´å¥½çš„ UI åé¦ˆ
    with status_container.status("ğŸ¤– AI æ­£åœ¨å¹¶å‘æ€è€ƒä¸­...", expanded=True) as status:
        st.write("ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¹¶å‘çº¿ç¨‹æ± ...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤ä»»åŠ¡ï¼Œè®°å½• index ä»¥ä¾¿æœ€åæŒ‰é¡ºåºé‡ç»„
            future_to_idx = {
                executor.submit(_fetch_deepseek_chunk_safe, chunk, prompt_template, api_key): i 
                for i, chunk in enumerate(chunks)
            }
            
            completed_count = 0
            
            # as_completed åœ¨ä¸»çº¿ç¨‹è¿­ä»£ï¼Œå¯ä»¥å®‰å…¨æ›´æ–° UI
            for future in concurrent.futures.as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    res = future.result()
                    results_map[idx] = res
                    
                    if "ERROR:" in res:
                        st.write(f"âš ï¸ æ‰¹æ¬¡ {idx+1} å‡ºç°å¼‚å¸¸: {res}")
                    
                    completed_count += 1
                    progress = completed_count / total_chunks
                    progress_bar.progress(progress)
                    st.write(f"âœ… å®Œæˆæ‰¹æ¬¡ {idx+1}/{total_chunks} ({len(chunks[idx])} è¯)")
                    
                except Exception as exc:
                    st.error(f"âŒ æ‰¹æ¬¡ {idx+1} ä¸¥é‡å´©æºƒ: {exc}")
                    results_map[idx] = ""

        status.update(label="ğŸ‰ ç”Ÿæˆå®Œæˆï¼", state="complete", expanded=False)

    # æŒ‰åŸå§‹é¡ºåºé‡ç»„ç»“æœ
    final_output = []
    for i in range(total_chunks):
        if i in results_map and not results_map[i].startswith("ERROR"):
            final_output.append(results_map[i])
        elif i in results_map:
            final_output.append(f"<!-- Batch {i+1} Failed: {results_map[i]} -->")
            
    return "\n".join(final_output)

# ==========================================
# 5. åˆ†æå¼•æ“
# ==========================================
def analyze_words(unique_word_list):
    unique_items = [] 
    JUNK_WORDS = {'s', 't', 'd', 'm', 'll', 've', 're', 'don', 'isn', 'aren'} # æ‰©å±•åƒåœ¾è¯
    
    for item_lower in unique_word_list:
        # åŸºç¡€è¿‡æ»¤
        if len(item_lower) < 2 and item_lower not in ['a', 'i']: continue
        if item_lower in JUNK_WORDS: continue
        if item_lower.isdigit(): continue # è¿‡æ»¤çº¯æ•°å­—
        
        actual_rank = vocab_dict.get(item_lower, 99999)
        
        # 1. æŠ€æœ¯æœ¯è¯­æ£€æµ‹
        if item_lower in BUILTIN_TECHNICAL_TERMS:
            domain = BUILTIN_TECHNICAL_TERMS[item_lower]
            term_rank = actual_rank if actual_rank != 99999 else 15000
            unique_items.append({"word": f"{item_lower} ({domain})", "rank": term_rank, "raw": item_lower})
            continue
            
        # 2. ä¸“æœ‰åè¯æ£€æµ‹
        if item_lower in PROPER_NOUNS_DB or item_lower in AMBIGUOUS_WORDS:
            display = PROPER_NOUNS_DB.get(item_lower, item_lower.title())
            unique_items.append({"word": display, "rank": actual_rank, "raw": item_lower})
            continue
            
        # 3. å¸¸è§„è¯æ±‡
        if actual_rank != 99999:
            unique_items.append({"word": item_lower, "rank": actual_rank, "raw": item_lower})
            
    return pd.DataFrame(unique_items)

# ==========================================
# 6. UI ä¸ çŠ¶æ€ç®¡ç†
# ==========================================
# çŠ¶æ€åˆå§‹åŒ–
if "raw_input_text" not in st.session_state: st.session_state.raw_input_text = ""
if "uploader_key" not in st.session_state: st.session_state.uploader_key = 0 
if "analysis_result" not in st.session_state: st.session_state.analysis_result = None # å­˜å‚¨åˆ†æç»“æœ DataFrame
if "lemma_text" not in st.session_state: st.session_state.lemma_text = ""

def clear_all_inputs():
    st.session_state.raw_input_text = ""
    st.session_state.uploader_key += 1 
    st.session_state.analysis_result = None
    st.session_state.lemma_text = ""

# --- å‚æ•°é…ç½®åŒº ---
st.markdown("<div class='param-box'>", unsafe_allow_html=True)
c1, c2, c3, c4, c5 = st.columns(5)
with c1: current_level = st.number_input("ğŸ¯ å½“å‰è¯æ±‡é‡ (èµ·)", 0, 30000, 7500, 500, help="è¿‡æ»¤æ‰è¿‡äºç®€å•çš„è¯")
with c2: target_level = st.number_input("ğŸ¯ ç›®æ ‡è¯æ±‡é‡ (æ­¢)", 0, 30000, 15000, 500, help="è¿‡æ»¤æ‰è¿‡äºç”Ÿåƒ»çš„è¯")
with c3: top_n = st.number_input("ğŸ”¥ ç²¾é€‰ Top N", 10, 500, 50, 10)
with c4: min_rank_threshold = st.number_input("ğŸ“‰ å¿½ç•¥å‰ N è¯", 0, 20000, 3500, 500, help="å³ä¾¿æ˜¯é‡ç‚¹è¯ï¼Œå¦‚æœå¤ªå¸¸è§ä¹Ÿä¸æ˜¾ç¤º")
with c5: 
    st.write("") 
    st.write("") 
    show_rank = st.checkbox("ğŸ”¢ é™„åŠ æ˜¾ç¤º Rank", value=True)
st.markdown("</div>", unsafe_allow_html=True)

# --- è¾“å…¥åŒº ---
col_input1, col_input2 = st.columns([3, 2])
with col_input1:
    raw_text_input = st.text_area("ğŸ“¥ ç²˜è´´æ–‡æœ¬", height=150, key="raw_input_text", placeholder="åœ¨æ­¤ç²˜è´´è‹±æ–‡æ–‡ç« ...")
with col_input2:
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£ (TXT/PDF/DOCX/EPUB)", 
                                     type=["txt", "pdf", "docx", "epub"], 
                                     key=f"uploader_{st.session_state.uploader_key}")

col_btn1, col_btn2 = st.columns([5, 1])
with col_btn1: btn_process = st.button("ğŸš€ æé€Ÿæ™ºèƒ½è§£æ", type="primary", use_container_width=True)
with col_btn2: st.button("ğŸ—‘ï¸ æ¸…ç©º", on_click=clear_all_inputs, use_container_width=True)

st.divider()

# ==========================================
# 7. é€»è¾‘å¤„ç†æµ
# ==========================================
# è§¦å‘å¤„ç†é€»è¾‘
if btn_process:
    with st.status("ğŸ§  æ­£åœ¨è¿›è¡Œæ·±åº¦è§£æ...", expanded=True) as status:
        start_time = time.time()
        
        # 1. æ–‡æœ¬æå–
        combined_text = raw_text_input
        if uploaded_file is not None: 
            status.write("ğŸ“„ æ­£åœ¨è¯»å–æ–‡ä»¶å†…å®¹...")
            extracted = extract_text_from_file(uploaded_file)
            if extracted:
                combined_text += "\n" + extracted
            else:
                st.error("æ— æ³•ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬ã€‚")
        
        if not combined_text.strip():
            status.update(label="âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆæ–‡æœ¬", state="error")
        elif not vocab_dict:
             status.update(label="âš ï¸ è¯åº“æœªåŠ è½½", state="error")
        else:
            # 2. NLP å¤„ç†
            status.write("ğŸ” æ­£åœ¨åˆ†è¯ä¸è¯å½¢è¿˜åŸ (Lemmatization)...")
            # é¢„ç¼–è¯‘æ­£åˆ™æé«˜æ•ˆç‡
            word_pattern = re.compile(r"[a-zA-Z']+")
            raw_words = word_pattern.findall(combined_text)
            
            # ä½¿ç”¨å¸¦ç¼“å­˜çš„å‡½æ•°
            lemmatized_words = [get_lemma(w) for w in raw_words]
            unique_lemmas = list(set([w.lower() for w in lemmatized_words]))
            
            status.write(f"ğŸ“Š æ­£åœ¨æ¯”å¯¹ {len(unique_lemmas)} ä¸ªå»é‡è¯æ±‡...")
            analyzed_df = analyze_words(unique_lemmas)
            
            # å­˜å…¥ Session State
            st.session_state.analysis_result = analyzed_df
            st.session_state.lemma_text = " ".join(lemmatized_words)
            st.session_state.stats = {
                "raw_count": len(raw_words),
                "unique_count": len(unique_lemmas),
                "valid_count": len(analyzed_df),
                "time": time.time() - start_time
            }
            status.update(label="âœ… è§£æå®Œæˆï¼", state="complete", expanded=False)

# ==========================================
# 8. ç»“æœæ¸²æŸ“ (åŸºäº Session State)
# ==========================================
if st.session_state.analysis_result is not None:
    stats = st.session_state.stats
    df = st.session_state.analysis_result.copy()
    
    # é¡¶éƒ¨æŒ‡æ ‡
    col_m1, col_m2, col_m3, col_m4 = st.columns(4)
    col_m1.metric("ğŸ“ è§£ææ€»å­—æ•°", f"{stats['raw_count']:,}")
    col_m2.metric("âœ‚ï¸ å»é‡è¯æ ¹", f"{stats['unique_count']:,}")
    col_m3.metric("ğŸ¯ å‘½ä¸­è¯åº“", f"{stats['valid_count']:,}")
    col_m4.metric("âš¡ è€—æ—¶", f"{stats['time']:.2f} s")
    
    if not df.empty:
        # åŠ¨æ€åˆ†ç±»é€»è¾‘ (åœ¨è¿™é‡Œæ‰§è¡Œï¼Œè¿™æ ·ä¿®æ”¹ Slider ä¸éœ€è¦é‡æ–°è§£ææ–‡æœ¬)
        def categorize(row):
            r = row['rank']
            if r <= current_level: return "known"
            elif r <= target_level: return "target"
            else: return "beyond"
        
        df['final_cat'] = df.apply(categorize, axis=1)
        df = df.sort_values(by='rank')
        
        # ç­›é€‰é€»è¾‘
        top_df = df[df['rank'] >= min_rank_threshold].sort_values(by='rank', ascending=True).head(top_n)
        target_df = df[df['final_cat']=='target']
        beyond_df = df[df['final_cat']=='beyond']
        known_df = df[df['final_cat']=='known']
        
        t_top, t_target, t_beyond, t_known, t_raw = st.tabs([
            f"ğŸ”¥ Top {len(top_df)}", 
            f"ğŸŸ¡ é‡ç‚¹ ({len(target_df)})", 
            f"ğŸ”´ è¶…çº² ({len(beyond_df)})", 
            f"ğŸŸ¢ å·²æŒæ¡ ({len(known_df)})",
            "ğŸ“ åŸæ–‡ä¸‹è½½"
        ])
        
        def render_word_tab(tab_obj, data_df, label, key_prefix):
            with tab_obj:
                if data_df.empty:
                    st.info("æ­¤åŒºé—´æš‚æ— æ•°æ®")
                    return

                # å±•ç¤ºåˆ—è¡¨
                pure_words = data_df['word'].tolist()
                display_lines = [
                    f"{row['word']} [Rank: {int(row['rank'])}]" if show_rank else row['word']
                    for _, row in data_df.iterrows()
                ]
                
                with st.expander("ğŸ‘ï¸ æŸ¥çœ‹å•è¯åˆ—è¡¨", expanded=(label=="Top")):
                    st.code("\n".join(display_lines), language='text')

                st.divider()
                
                # AI ç”ŸæˆåŒº
                c_fmt, c_act = st.columns([1, 3])
                with c_fmt:
                    export_format = st.radio("è¾“å‡ºæ ¼å¼:", ["TXT", "CSV"], horizontal=True, key=f"fmt_{key_prefix}")
                
                ai_tab1, ai_tab2 = st.tabs(["ğŸ¤– å†…ç½® AI ç”Ÿæˆ", "ğŸ“‹ å¤åˆ¶ Prompt"])
                
                with ai_tab1:
                    custom_prompt = st.text_area("AI æŒ‡ä»¤æ¨¡æ¿", value=get_base_prompt_template(export_format), height=300, key=f"p_{key_prefix}")
                    if st.button(f"âš¡ ç”Ÿæˆ {label} å¡ç‰‡", key=f"btn_{key_prefix}", type="primary"):
                        progress = st.progress(0)
                        status_box = st.empty()
                        
                        result_text = call_deepseek_api_main_thread_managed(
                            custom_prompt, pure_words, progress, status_box
                        )
                        
                        if result_text and "ERROR" not in result_text[:20]:
                            mime = "text/csv" if export_format == "CSV" else "text/plain"
                            st.download_button(
                                "ğŸ“¥ ä¸‹è½½ç»“æœæ–‡ä»¶", 
                                data=result_text.encode('utf-8-sig'), 
                                file_name=f"anki_{label}_{int(time.time())}.{export_format.lower()}", 
                                mime=mime, 
                                type="primary"
                            )
                            with st.expander("é¢„è§ˆç»“æœ"):
                                st.text(result_text)
                        elif "ERROR" in result_text:
                            st.error(result_text)

                with ai_tab2:
                    prompt_txt = f"{get_base_prompt_template(export_format)}\n\nå•è¯åˆ—è¡¨:\n{', '.join(pure_words)}"
                    st.code(prompt_txt, language='markdown')

        render_word_tab(t_top, top_df, "Top", "top")
        render_word_tab(t_target, target_df, "Target", "target")
        render_word_tab(t_beyond, beyond_df, "Beyond", "beyond")
        render_word_tab(t_known, known_df, "Known", "known")
        
        with t_raw:
            st.info("å«è¯å½¢è¿˜åŸåçš„å…¨æ–‡ (ä¾‹å¦‚ 'running' -> 'run')")
            st.download_button("ğŸ’¾ ä¸‹è½½å…¨æ–‡", st.session_state.lemma_text, "full_text.txt")
            st.text_area("é¢„è§ˆ", st.session_state.lemma_text[:2000] + "...", height=300)