import streamlit as st
import pandas as pd
import re
import os
import io
import time
import lemminflect
import nltk
import genanki
import random
import tempfile
from bs4 import BeautifulSoup

# --- æ–‡ä»¶å¤„ç†åº“ ---
import pypdf
import docx
import ebooklib
from ebooklib import epub

# ==========================================
# 0. é¡µé¢åŸºç¡€é…ç½® & æ ·å¼
# ==========================================
st.set_page_config(
    page_title="Vocab Flow Ultra", 
    page_icon="âš¡ï¸", 
    layout="centered",
    initial_sidebar_state="collapsed"
)

st.markdown("""
<style>
    .stTextArea textarea { font-family: 'Consolas', monospace; font-size: 14px; }
    .stButton>button { border-radius: 8px; font-weight: 600; width: 100%; }
    .stat-box { padding: 10px; background-color: #f0f2f6; border-radius: 8px; margin-bottom: 10px; text-align: center; }
    .copy-hint { font-size: 0.8em; color: #888; margin-top: -10px; margin-bottom: 10px; text-align: right; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºåŠ è½½ & å·¥å…·å‡½æ•°
# ==========================================
@st.cache_resource
def setup_nltk():
    try:
        root_dir = os.path.dirname(os.path.abspath(__file__))
        nltk_data_dir = os.path.join(root_dir, 'nltk_data')
        os.makedirs(nltk_data_dir, exist_ok=True)
        nltk.data.path.append(nltk_data_dir)
        for pkg in ['averaged_perceptron_tagger', 'punkt', 'punkt_tab']:
            try: nltk.data.find(f'tokenizers/{pkg}')
            except LookupError: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
    except: pass
setup_nltk()

@st.cache_data
def load_vocab_data():
    """åŠ è½½è¯é¢‘è¡¨"""
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            df.columns = [c.strip().lower() for c in df.columns]
            w_col = next((c for c in df.columns if 'word' in c), df.columns[0])
            r_col = next((c for c in df.columns if 'rank' in c), df.columns[1])
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            df = df.sort_values(r_col).drop_duplicates(subset=[w_col], keep='first')
            return pd.Series(df[r_col].values, index=df[w_col]).to_dict(), df
        except: return {}, None
    return {}, None

VOCAB_DICT, FULL_DF = load_vocab_data()

def get_lemma(word):
    try: return lemminflect.getLemma(word, upos='VERB')[0]
    except: return word

# ==========================================
# 2. å¤šæ ¼å¼æ–‡ä»¶è§£æ (ç™¾ä¸‡å­—ä¼˜åŒ–ç‰ˆ)
# ==========================================
def extract_text_from_file(uploaded_file):
    """æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬"""
    text = ""
    file_type = uploaded_file.name.split('.')[-1].lower()
    
    try:
        if file_type == 'txt':
            text = uploaded_file.getvalue().decode("utf-8", errors='ignore')
            
        elif file_type == 'pdf':
            reader = pypdf.PdfReader(uploaded_file)
            text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
            
        elif file_type == 'docx':
            doc = docx.Document(uploaded_file)
            text = "\n".join([p.text for p in doc.paragraphs])
            
        elif file_type == 'epub':
            # éœ€è¦å…ˆä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶æ‰èƒ½ç”¨ ebooklib è¯»å–
            with tempfile.NamedTemporaryFile(delete=False, suffix='.epub') as tmp:
                tmp.write(uploaded_file.getvalue())
                tmp_path = tmp.name
            
            book = epub.read_epub(tmp_path)
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    text += soup.get_text() + " "
            os.remove(tmp_path)
            
    except Exception as e:
        return f"Error: {e}"
        
    return text

@st.cache_data
def fast_analyze_text(text, current_lvl, target_lvl):
    """
    æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒï¼š
    1. ä½¿ç”¨æ­£åˆ™ (re) æ›¿ä»£ NLTK åšåˆ†è¯ï¼Œé€Ÿåº¦å¿« 10 å€ã€‚
    2. ä½¿ç”¨ Set å»é‡åæ‰åš Lemmatizationï¼Œé¿å…å¯¹ 100 ä¸‡ä¸ªè¯é‡å¤è¿ç®—ã€‚
    """
    # 1. å¿«é€Ÿåˆ†è¯ (Regex)
    raw_tokens = re.findall(r"[a-z]+", text.lower())
    
    # 2. ç»Ÿè®¡æ€»è¯æ•°
    total_words = len(raw_tokens)
    
    # 3. æ ¸å¿ƒç®—æ³•ï¼šä»…å¯¹å»é‡åçš„è¯è¿›è¡Œè¯å½¢è¿˜åŸå’ŒæŸ¥è¡¨
    unique_tokens = set(raw_tokens)
    target_words = []
    
    for w in unique_tokens:
        if len(w) < 3: continue # å¿½ç•¥è¿‡çŸ­å•è¯
        lemma = get_lemma(w)
        rank = VOCAB_DICT.get(lemma, 99999)
        
        # ç­›é€‰é€»è¾‘
        if rank > current_lvl and rank <= target_lvl:
            target_words.append((lemma, rank))
            
    # 4. æ’åº
    target_words.sort(key=lambda x: x[1])
    final_list = [x[0] for x in target_words]
    
    return final_list, total_words

# ==========================================
# 3. Anki æ‰“åŒ… & CSS (ä¿æŒä¸å˜)
# ==========================================
def generate_anki_package(cards_data, deck_name="Vocab_Deck"):
    CSS = """
    .card { font-family: arial; font-size: 20px; text-align: center; color: #333; background-color: white; padding: 20px; }
    .nightMode .card { background-color: #2f2f31; color: #f5f5f5; }
    .word { font-size: 40px; font-weight: bold; color: #007AFF; margin-bottom: 10px; }
    .nightMode .word { color: #5FA9FF; }
    .phonetic { color: #888; font-size: 18px; font-family: sans-serif; }
    .def-container { text-align: left; margin-top: 20px; border-top: 1px solid #ddd; padding-top: 10px; }
    .definition { font-weight: bold; color: #444; margin-bottom: 10px; }
    .nightMode .definition { color: #ddd; }
    .examples { background: #f4f4f4; padding: 10px; border-radius: 5px; color: #555; font-style: italic; font-size: 16px; }
    .nightMode .examples { background: #333; color: #ccc; }
    .etymology { margin-top: 15px; font-size: 14px; color: #888; border: 1px dashed #ccc; padding: 5px; display: inline-block;}
    """
    
    model = genanki.Model(
        random.randrange(1 << 30, 1 << 31), 'VocabFlow Model',
        fields=[{'name': 'Word'}, {'name': 'IPA'}, {'name': 'Meaning'}, {'name': 'Examples'}, {'name': 'Etymology'}],
        templates=[{
            'name': 'Card 1',
            'qfmt': '<div class="word">{{Word}}</div><div class="phonetic">{{IPA}}</div>',
            'afmt': '{{FrontSide}}<div class="def-container"><div class="definition">{{Meaning}}</div><div class="examples">{{Examples}}</div><div class="etymology">ğŸŒ± {{Etymology}}</div></div>',
        }], css=CSS
    )
    
    deck = genanki.Deck(random.randrange(1 << 30, 1 << 31), deck_name)
    for c in cards_data:
        deck.add_note(genanki.Note(model=model, fields=[c['word'], c['ipa'], c['meaning'], c['examples'].replace('\n','<br>'), c['etymology']]))
        
    with tempfile.NamedTemporaryFile(delete=False, suffix='.apkg') as tmp:
        genanki.Package(deck).write_to_file(tmp.name)
        return tmp.name

def get_ai_prompt(words):
    w_list = ", ".join(words)
    return f"""
Act as a Lexicographer. Create Anki card data.
Words: {w_list}

**Strict Output Format (Pipe Separated `|`, NO Header):**
Word | IPA | Concise English Definition | 2 English Sentences | Etymology (Root+Suffix)

**Requirements:**
1. Definition: Simple English (B2 level).
2. Examples: 2 sentences separated by `<br>`.
3. Etymology: Format `root(meaning) + suffix`.
4. NO Header row.

**Example:**
benevolent | /bÉ™ËˆnevÉ™lÉ™nt/ | kind and meaningful | He is benevolent.<br>A benevolent fund. | bene(good) + vol(wish)
"""

# ==========================================
# 4. ä¸»ç•Œé¢
# ==========================================
st.title("âš¡ï¸ Vocab Flow Ultra")

if not VOCAB_DICT:
    st.error("âš ï¸ ç¼ºå¤± `coca_cleaned.csv`ï¼Œæ— æ³•è¿›è¡Œé¢‘ç‡ç­›é€‰ï¼")

tab1, tab2, tab3 = st.tabs(["ğŸ“‚ æ–‡ä»¶åˆ†æ", "ğŸ”¢ è¯é¢‘ç”Ÿæˆ", "ğŸ› ï¸ åˆ¶ä½œ Anki"])

# --- Tab 1: æ–‡ä»¶åˆ†æ (æ”¯æŒç™¾ä¸‡å­—) ---
with tab1:
    c1, c2 = st.columns(2)
    curr = c1.number_input("å¿½ç•¥ç®€å•è¯ (Rank <)", 1000, 20000, 4000, step=500)
    targ = c2.number_input("å¿½ç•¥ç”Ÿåƒ»è¯ (Rank >)", 2000, 50000, 15000, step=500)
    
    # æ”¯æŒå¤šç§æ ¼å¼ä¸Šä¼ 
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶ (æ”¯æŒ .txt, .pdf, .docx, .epub)", type=['txt', 'pdf', 'docx', 'epub'])
    
    if uploaded_file and st.button("ğŸš€ å¼€å§‹æé€Ÿåˆ†æ"):
        with st.spinner("æ­£åœ¨è§£ææ–‡ä»¶..."):
            # 1. è§£ææ–‡æœ¬
            raw_text = extract_text_from_file(uploaded_file)
            
            if len(raw_text) < 10:
                st.error("æ— æ³•è¯»å–æ–‡æœ¬ï¼Œå¯èƒ½æ˜¯æ‰«æç‰ˆPDFæˆ–åŠ å¯†æ–‡ä»¶ã€‚")
            else:
                # 2. æé€Ÿåˆ†æ
                t0 = time.time()
                final_words, total_count = fast_analyze_text(raw_text, curr, targ)
                t1 = time.time()
                
                st.markdown(f"""
                <div class="stat-box">
                    ğŸ“Š åŸæ–‡çº¦ <b>{total_count}</b> è¯ | è€—æ—¶ <b>{t1-t0:.2f}s</b><br>
                    ğŸ¯ ç­›é€‰å‡º <b>{len(final_words)}</b> ä¸ªé‡ç‚¹è¯ (Rank {curr}-{targ})
                </div>
                """, unsafe_allow_html=True)
                
                st.session_state['gen_words'] = final_words

# --- Tab 2: è¯é¢‘ç”Ÿæˆ (ä¿ç•™çš„åŠŸèƒ½) ---
with tab2:
    st.caption("ç›´æ¥æ ¹æ®è¯é¢‘æ’åç”Ÿæˆå•è¯è¡¨ï¼Œæ— éœ€ä¸Šä¼ æ–‡ä»¶ã€‚")
    c_a, c_b = st.columns(2)
    start_rank = c_a.number_input("èµ·å§‹æ’å (Start Rank)", 1, 20000, 8000, step=100)
    count_num = c_b.number_input("ç”Ÿæˆæ•°é‡ (Count)", 10, 500, 50, step=10)
    
    if st.button("ğŸ”¢ ç”Ÿæˆåˆ—è¡¨", type="primary"):
        if FULL_DF is not None:
            # è¿™é‡Œçš„ FULL_DF æ˜¯åœ¨ load_data é‡Œè¿”å›çš„åŸå§‹ DataFrame
            # æˆ‘ä»¬éœ€è¦ FULL_DF çš„ columns åˆ†åˆ«æ˜¯ word å’Œ rank
            # åœ¨ load_vocab_data ç¨å¾®è°ƒæ•´ä¸€ä¸‹è®©å®ƒè¿”å› DF
             
            # ç­›é€‰é€»è¾‘
            try:
                # æ‰¾åˆ° Rank åˆ—å
                r_col = next(c for c in FULL_DF.columns if 'rank' in c)
                w_col = next(c for c in FULL_DF.columns if 'word' in c)
                
                subset = FULL_DF[FULL_DF[r_col] >= start_rank].sort_values(r_col).head(count_num)
                gen_list = subset[w_col].tolist()
                st.session_state['gen_words'] = gen_list
                st.success(f"å·²ç”Ÿæˆ {len(gen_list)} ä¸ªå•è¯ (Rank {start_rank} èµ·)")
            except Exception as e:
                st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
        else:
            st.error("æ— æ•°æ®æº")

# --- ç»“æœå±•ç¤ºä¸ Prompt ç”Ÿæˆ (Tab 1 & 2 å…±ç”¨) ---
if 'gen_words' in st.session_state and st.session_state['gen_words']:
    st.divider()
    st.markdown("### ğŸ“‹ å•è¯åˆ—è¡¨ & Prompt")
    
    words = st.session_state['gen_words']
    words_str = ", ".join(words)
    
    # 1. æä¾›ä¸€é”®å¤åˆ¶çš„ Code Block
    st.markdown("<div class='copy-hint'>ğŸ‘‡ ç‚¹å‡»ä»£ç å—å³ä¸Šè§’å³å¯ä¸€é”®å¤åˆ¶å•è¯è¡¨</div>", unsafe_allow_html=True)
    st.code(words_str, language="text")
    
    # 2. ç”Ÿæˆ AI Prompt
    if st.button("ğŸ¤– ç”Ÿæˆ AI Prompt"):
        prompt = get_ai_prompt(words)
        st.code(prompt, language="markdown")
        st.info("å¤åˆ¶ä¸Šæ–¹ Prompt å‘é€ç»™ AIï¼Œç„¶åå°†ç»“æœç²˜è´´åˆ° 'åˆ¶ä½œ Anki' é¡µé¢ã€‚")

# --- Tab 3: åˆ¶ä½œ Anki ---
with tab3:
    st.markdown("### ğŸ› ï¸ åˆ¶ä½œ iOS é€‚é…åŒ… (.apkg)")
    ai_resp = st.text_area("ç²˜è´´ AI å›å¤ (Word | IPA | Def | Ex | Etym)", height=200)
    deck_name = st.text_input("ç‰Œç»„å", "My Deck")
    
    if st.button("ğŸ“¦ æ‰“åŒ…ä¸‹è½½"):
        if not ai_resp.strip(): st.error("å†…å®¹ä¸ºç©º")
        else:
            cards = []
            for line in ai_resp.strip().split('\n'):
                if "|" not in line or "Word |" in line: continue
                p = [x.strip() for x in line.split('|')]
                if len(p) >= 3:
                    cards.append({'word':p[0], 'ipa':p[1] if len(p)>1 else '', 'meaning':p[2] if len(p)>2 else '', 'examples':p[3] if len(p)>3 else '', 'etymology':p[4] if len(p)>4 else ''})
            
            if cards:
                f_path = generate_anki_package(cards, deck_name)
                with open(f_path, "rb") as f:
                    st.download_button(f"ğŸ“¥ ä¸‹è½½ {deck_name}.apkg", f, file_name=f"{deck_name}.apkg", mime="application/octet-stream", type="primary")
                st.success(f"æˆåŠŸæ‰“åŒ… {len(cards)} å¼ å¡ç‰‡ï¼")
            else:
                st.error("æ— æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥åˆ†éš”ç¬¦ |")