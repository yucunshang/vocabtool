import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode {
        font-family: 'Consolas', 'Courier New', monospace !important;
        font-size: 16px !important;
    }
    header {visibility: hidden;}
    footer {visibility: hidden;}
    .block-container { padding-top: 1rem; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. åˆå§‹åŒ– NLP å¼•æ“ (NLTK + Lemminflect)
# ==========================================
@st.cache_resource
def download_nltk_resources():
    """ä¸‹è½½å¿…è¦çš„ NLTK æ•°æ®åŒ… (åªè¿è¡Œä¸€æ¬¡)"""
    try:
        nltk.data.find('taggers/averaged_perceptron_tagger')
    except LookupError:
        nltk.download('averaged_perceptron_tagger')
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')

download_nltk_resources()

def get_display_case(word):
    """
    æ™ºèƒ½åˆ¤æ–­å¤§å°å†™ï¼š
    1. Sydney -> Sydney (ä¸“æœ‰åè¯ä¿ç•™å¤§å†™)
    2. ANTI -> anti (æ™®é€šè¯å¼ºåˆ¶å°å†™)
    3. Table -> table (æ™®é€šè¯å¼ºåˆ¶å°å†™)
    """
    # å…ˆæŠŠè¯å˜æˆ Title Case (é¦–å­—æ¯å¤§å†™) å»æµ‹è¯•ï¼Œè¿™æ · NLTK åˆ¤æ–­æœ€å‡†
    test_word = word.title()
    
    # è·å–è¯æ€§æ ‡æ³¨
    # NNP/NNPS ä»£è¡¨ä¸“æœ‰åè¯ (Proper Noun)
    tags = nltk.pos_tag([test_word])
    pos_tag = tags[0][1]
    
    if pos_tag.startswith('NNP'):
        return test_word # æ˜¯äººå/åœ°åï¼Œè¿”å› Sydney
    else:
        return word.lower() # æ˜¯æ™®é€šè¯ï¼Œè¿”å› anti

def smart_lemmatize(text):
    words = re.findall(r"[a-zA-Z']+", text)
    results = []
    for w in words:
        lemmas_dict = lemminflect.getAllLemmas(w)
        if not lemmas_dict:
            results.append(w.lower())
            continue
            
        if 'ADJ' in lemmas_dict: lemma = lemmas_dict['ADJ'][0]
        elif 'ADV' in lemmas_dict: lemma = lemmas_dict['ADV'][0]
        elif 'VERB' in lemmas_dict: lemma = lemmas_dict['VERB'][0]
        elif 'NOUN' in lemmas_dict: lemma = lemmas_dict['NOUN'][0]
        else: lemma = list(lemmas_dict.values())[0][0]
            
        results.append(lemma)
    return " ".join(results)

# ==========================================
# 3. è¯åº“åŠ è½½
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv"]

@st.cache_data
def load_vocab():
    file_path = next((f for f in POSSIBLE_FILES if os.path.exists(f)), None)
    if not file_path: return None
    try:
        df = pd.read_csv(file_path)
        cols = [str(c).strip().lower() for c in df.columns]
        df.columns = cols
        w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
        r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
        
        df[w_col] = df[w_col].astype(str).str.lower().str.strip()
        df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
        
        # æ’åºå¹¶å»é‡
        df = df.sort_values(r_col, ascending=True)
        df = df.drop_duplicates(subset=[w_col], keep='first')
        
        return pd.Series(df[r_col].values, index=df[w_col]).to_dict()
    except: return None

vocab_dict = load_vocab()

# ==========================================
# 4. ç•Œé¢å¸ƒå±€
# ==========================================
st.title("ğŸš€ Vocab Master Pro (Smart Case)")

tab_lemma, tab_grade = st.tabs(["ğŸ› ï¸ 1. æ™ºèƒ½è¿˜åŸ (Restore)", "ğŸ“Š 2. å•è¯åˆ†çº§ (Grade)"])

# ---------------------------------------------------------
# Tab 1: æ™ºèƒ½è¿˜åŸ
# ---------------------------------------------------------
with tab_lemma:
    c1, c2 = st.columns(2)
    with c1:
        raw_text = st.text_area("è¾“å…¥åŸå§‹æ–‡ç« ", height=400, placeholder="He was excited.\nShe went home.")
        btn_restore = st.button("å¼€å§‹è¿˜åŸ", type="primary")
    with c2:
        if btn_restore and raw_text:
            res = smart_lemmatize(raw_text)
            st.code(res, language='text')
            st.caption("ğŸ‘† ç‚¹å‡»å³ä¸Šè§’å›¾æ ‡ä¸€é”®å¤åˆ¶")
        elif not raw_text:
            st.info("ğŸ‘ˆ è¯·è¾“å…¥æ–‡æœ¬")

# ---------------------------------------------------------
# Tab 2: å•è¯åˆ†çº§ (æ™ºèƒ½å¤§å°å†™)
# ---------------------------------------------------------
with tab_grade:
    col_a, col_b, col_c = st.columns([1, 1, 2])
    with col_a: current_level = st.number_input("å½“å‰æ°´å¹³", 0, 20000, 9000, 500)
    with col_b: target_level = st.number_input("ç›®æ ‡æ°´å¹³", 0, 20000, 15000, 500)
    st.divider()
    
    g_col1, g_col2 = st.columns(2)
    with g_col1:
        input_mode = st.radio("è¯†åˆ«æ¨¡å¼:", ("è‡ªåŠ¨åˆ†è¯ (Word Mode)", "æŒ‰è¡Œå¤„ç† (Phrase Mode)"), horizontal=True)
        grade_input = st.text_area("input_box", height=400, placeholder="ANTI\nSydney\nTable", label_visibility="collapsed")
        btn_grade = st.button("å¼€å§‹åˆ†çº§", type="primary", use_container_width=True)

    with g_col2:
        if not vocab_dict:
            st.error("âŒ è¯åº“æœªåŠ è½½")
        elif btn_grade and grade_input:
            
            # 1. è·å–è¾“å…¥åˆ—è¡¨
            raw_items = []
            if "æŒ‰è¡Œå¤„ç†" in input_mode:
                lines = grade_input.split('\n')
                for line in lines:
                    if line.strip(): raw_items.append(line.strip())
            else:
                raw_items = grade_input.split()
            
            # 2. æ¸…æ´—ä¸å¤§å°å†™å¤„ç†
            seen = set()
            unique_items = []
            JUNK_WORDS = {'s', 't', 'd', 'm', 'll', 've', 're'}
            
            for item in raw_items:
                item_cleaned = item.strip()
                item_lower = item_cleaned.lower()
                
                # è¿‡æ»¤é‡å¤å’Œåƒåœ¾è¯
                if item_lower in seen: continue
                if len(item_lower) < 2 and item_lower not in ['a', 'i']: continue
                if item_lower in JUNK_WORDS: continue
                
                # === æ ¸å¿ƒä¿®æ”¹ï¼šæ™ºèƒ½åˆ¤æ–­æ˜¾ç¤ºçš„å¤§å°å†™ ===
                display_word = get_display_case(item_cleaned)
                
                seen.add(item_lower)
                unique_items.append(display_word)
            
            # 3. æŸ¥è¯ (ç»Ÿä¸€ç”¨å°å†™æŸ¥)
            data = []
            for item in unique_items:
                lookup_key = item.lower()
                rank = vocab_dict.get(lookup_key, 99999)
                
                cat = "beyond"
                if rank <= current_level: cat = "known"
                elif rank <= target_level: cat = "target"
                
                # data é‡Œå­˜çš„æ˜¯ display_word (Sydney / anti)
                data.append({"word": item, "rank": rank, "cat": cat})
            
            # 4. æ’åºä¸å±•ç¤º
            df = pd.DataFrame(data)
            if not df.empty:
                df = df.sort_values(by='rank', ascending=True)
                
                t1, t2, t3 = st.tabs([
                    f"ğŸŸ¡ é‡ç‚¹ ({len(df[df['cat']=='target'])})", 
                    f"ğŸ”´ è¶…çº² ({len(df[df['cat']=='beyond'])})", 
                    f"ğŸŸ¢ å·²æŒæ¡ ({len(df[df['cat']=='known'])})"
                ])
                
                def show(cat_name):
                    sub = df[df['cat'] == cat_name]
                    if sub.empty: 
                        st.info("æ— ")
                    else:
                        txt = "\n".join(sub['word'].tolist())
                        st.code(txt, language='text')
                        st.caption("ğŸ‘† ç‚¹å‡»å³ä¸Šè§’å›¾æ ‡ä¸€é”®å¤åˆ¶")

                with t1: show("target")
                with t2: show("beyond")
                with t3: show("known")
            else:
                st.warning("æ— æœ‰æ•ˆå•è¯")