import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import genanki
import random
import tempfile

# ==========================================
# 0. é¡µé¢é…ç½® & æ ·å¼ä¼˜åŒ–
# ==========================================
st.set_page_config(
    page_title="Vocab Flow (Server Ver.)", 
    page_icon="âš¡ï¸", 
    layout="centered",
    initial_sidebar_state="collapsed"
)

# æ³¨å…¥ CSS ç¾åŒ– Streamlit ç•Œé¢
st.markdown("""
<style>
    .stTextArea textarea { font-family: 'Consolas', monospace; font-size: 14px; }
    .stButton>button { border-radius: 8px; font-weight: 600; }
    .instruction { font-size: 0.9em; color: #666; margin-bottom: 10px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºåŠ è½½ (é€‚é…äº‘ç«¯ç¯å¢ƒ)
# ==========================================
@st.cache_resource
def setup_nltk():
    """åœ¨äº‘ç«¯ç¯å¢ƒä¸‹å®‰å…¨ä¸‹è½½ NLTK æ•°æ®"""
    try:
        root_dir = os.path.dirname(os.path.abspath(__file__))
        nltk_data_dir = os.path.join(root_dir, 'nltk_data')
        os.makedirs(nltk_data_dir, exist_ok=True)
        nltk.data.path.append(nltk_data_dir)
        
        # ä»…ä¸‹è½½å¿…è¦çš„åŒ…
        for pkg in ['averaged_perceptron_tagger', 'punkt', 'punkt_tab']:
            try:
                nltk.data.find(f'tokenizers/{pkg}')
            except LookupError:
                nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
    except Exception as e:
        st.warning(f"NLTK Setup Warning: {e}")

setup_nltk()

@st.cache_data
def load_vocab_data():
    """åŠ è½½è¯é¢‘æ•°æ®ï¼Œå¢åŠ å®¹é”™"""
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    
    if file_path:
        try:
            df = pd.read_csv(file_path)
            # ç®€å•çš„åˆ—åæ¸…æ´—
            df.columns = [c.strip().lower() for c in df.columns]
            # å°è¯•è‡ªåŠ¨å¯»æ‰¾ word å’Œ rank åˆ—
            w_col = next((c for c in df.columns if 'word' in c), df.columns[0])
            r_col = next((c for c in df.columns if 'rank' in c), df.columns[1])
            
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            
            # å»é‡ä¿ç•™ rank æœ€å°çš„
            df = df.sort_values(r_col).drop_duplicates(subset=[w_col], keep='first')
            return pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except:
            return {}
    return {}

VOCAB_DICT = load_vocab_data()

def get_lemma(word):
    """è·å–å•è¯åŸå½¢"""
    try:
        return lemminflect.getLemma(word, upos='VERB')[0]
    except:
        return word

# ==========================================
# 2. Anki é«˜è´¨é‡æ¨¡æ¿ä¸æ‰“åŒ…é€»è¾‘
# ==========================================
def generate_anki_package(cards_data, deck_name="Vocab_Deck"):
    """
    ç”Ÿæˆ .apkg æ–‡ä»¶å¹¶è¿”å›äºŒè¿›åˆ¶æ•°æ®
    cards_data: list of dicts
    """
    
    # --- CSS æ ·å¼ (é«˜è´¨é‡æ¨¡æ¿æ ¸å¿ƒ) ---
    # è¿™ä¸ªæ ·å¼ä¼šè‡ªåŠ¨é€‚é… iOS çš„å¤œé—´æ¨¡å¼
    CSS = """
    .card {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
        text-align: center;
        font-size: 20px;
        color: #333;
        background-color: #ffffff;
        padding: 20px 10px;
    }
    
    /* å¤œé—´æ¨¡å¼é€‚é… */
    .nightMode .card { background-color: #2f2f31; color: #f5f5f5; }
    
    /* æ­£é¢ */
    .word { font-size: 38px; font-weight: 700; color: #007AFF; margin-bottom: 8px; }
    .nightMode .word { color: #5FA9FF; }
    .phonetic { font-family: "Lucida Sans Unicode", sans-serif; color: #888; font-size: 18px; }
    
    /* èƒŒé¢ */
    .def-container { 
        text-align: left; margin-top: 20px; padding-top: 15px; 
        border-top: 1px solid #eee; 
    }
    .nightMode .def-container { border-top: 1px solid #444; }
    
    .definition { font-weight: 600; font-size: 18px; color: #444; margin-bottom: 15px; }
    .nightMode .definition { color: #ddd; }
    
    .example-box {
        background: #f2f7fa; border-left: 4px solid #007AFF;
        padding: 10px; margin: 10px 0; border-radius: 4px;
        font-size: 16px; color: #555; text-align: left;
    }
    .nightMode .example-box { background: #333333; border-left: 4px solid #5FA9FF; color: #ccc; }
    
    .etymology {
        margin-top: 20px; font-size: 14px; color: #999; font-style: italic;
        border: 1px dashed #ddd; padding: 5px; border-radius: 5px; display: inline-block;
    }
    .nightMode .etymology { border-color: #555; }
    """

    # --- Anki Model å®šä¹‰ ---
    # å­—æ®µï¼šWord, IPA, Meaning, Examples, Etymology
    model = genanki.Model(
        random.randrange(1 << 30, 1 << 31),
        'Streamlit High-End Model',
        fields=[
            {'name': 'Word'},
            {'name': 'IPA'},
            {'name': 'Meaning'},
            {'name': 'Examples'},
            {'name': 'Etymology'},
        ],
        templates=[
            {
                'name': 'Card 1',
                'qfmt': '<div class="word">{{Word}}</div><div class="phonetic">{{IPA}}</div>',
                'afmt': '''
                {{FrontSide}}
                <div class="def-container">
                    <div class="definition">{{Meaning}}</div>
                    <div class="example-box">{{Examples}}</div>
                    <div class="etymology">Origin: {{Etymology}}</div>
                </div>
                ''',
            },
        ],
        css=CSS
    )

    deck = genanki.Deck(random.randrange(1 << 30, 1 << 31), deck_name)

    for card in cards_data:
        deck.add_note(genanki.Note(
            model=model,
            fields=[
                card['word'],
                card['ipa'],
                card['meaning'],
                card['examples'].replace('\n', '<br>'), # å¤„ç†æ¢è¡Œ
                card['etymology']
            ]
        ))

    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç”Ÿæˆï¼Œé¿å…æƒé™é—®é¢˜
    with tempfile.NamedTemporaryFile(delete=False, suffix='.apkg') as tmp:
        genanki.Package(deck).write_to_file(tmp.name)
        return tmp.name

# ==========================================
# 3. æ ¸å¿ƒé€»è¾‘ï¼šæ–‡æœ¬åˆ†æ & Prompt
# ==========================================
def analyze_text(text, target_lvl):
    raw_words = re.findall(r"[a-z]+", text.lower())
    unique_words = set(raw_words)
    
    res = []
    for w in unique_words:
        if len(w) < 2: continue
        lemma = get_lemma(w)
        rank = VOCAB_DICT.get(lemma, 0)
        
        # ç®€å•ç­›é€‰é€»è¾‘ï¼šå¦‚æœ rank > 0 ä¸” rank <= target_lvl (æˆ–è€…æ²¡æœ‰è¯è¡¨æ—¶å…¨éƒ¨è¾“å‡º)
        if VOCAB_DICT:
            # è¿™é‡Œä½ å¯ä»¥è‡ªå®šä¹‰é€»è¾‘ï¼Œä¾‹å¦‚åªçœ‹ 4000-8000 è¯
            # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å‡è®¾åªæå–â€œéš¾è¯â€ (Rank > 3000)
            if rank > 3000 and rank <= target_lvl: 
                res.append((lemma, rank))
        else:
            res.append((lemma, 0))
            
    # æŒ‰è¯é¢‘æ’åº (è¶Šå¸¸è§è¶Šå‰ï¼Œæˆ–è€…åä¹‹)
    res.sort(key=lambda x: x[1])
    return [x[0] for x in res]

def get_ai_prompt(words):
    w_list = ", ".join(words)
    # ä½¿ç”¨ Markdown è¡¨æ ¼æˆ–ç®¡é“ç¬¦ï¼Œè®© AI ç”Ÿæˆç»“æ„åŒ–æ•°æ®
    # ç®¡é“ç¬¦ | æ¯” CSV é€—å·æ›´å®‰å…¨ï¼Œå› ä¸ºä¾‹å¥é‡Œå¸¸æœ‰é€—å·
    return f"""
Act as a Dictionary API. I need Anki card data for these words.
Words: {w_list}

**Strict Output Format (Pipe Separated, NO Header):**
Word | IPA | Chinese Definition | 2 English Sentences (Cn translation included) | Etymology/Root

**Requirements:**
1. Use `|` as separator.
2. Example Sentences: Use `<br>` to separate the two sentences.
3. Definition: Concise Chinese.
4. Etymology: Very short root explanation (Chinese).

**Example Line:**
benevolent | /bÉ™ËˆnevÉ™lÉ™nt/ | ä»æ…ˆçš„ | He is benevolent.<br>She smiled benevolently. | bene(å¥½) + vol(æ„æ„¿)
"""

# ==========================================
# 4. Streamlit UI ä¸»ç¨‹åº
# ==========================================
st.title("âš¡ï¸ Vocab Flow (Cloud)")
st.caption("Step 1: æå–å•è¯ -> Step 2: AI ç”Ÿæˆ -> Step 3: ä¸€é”®æ‰“åŒ… iOS")

# ä½¿ç”¨ Tab åˆ†éš”æ­¥éª¤ï¼Œé€»è¾‘æ›´æ¸…æ™°
t1, t2 = st.tabs(["1. åˆ†æ & æè¯", "2. ç”Ÿæˆ & ä¸‹è½½"])

with t1:
    c1, c2 = st.columns(2)
    max_rank = c1.number_input("ç­›é€‰è¯é¢‘ä¸Šé™ (Rank)", 5000, 20000, 10000, step=1000)
    
    txt = st.text_area("ç²˜è´´è‹±æ–‡æ–‡æœ¬", height=150)
    
    if st.button("ğŸ” åˆ†ææ–‡æœ¬"):
        if not txt.strip():
            st.warning("è¯·å…ˆç²˜è´´æ–‡æœ¬")
        else:
            final_words = analyze_text(txt, max_rank)
            st.session_state['words'] = final_words
            st.success(f"ç­›é€‰å‡º {len(final_words)} ä¸ªå•è¯")

    if 'words' in st.session_state:
        # å…è®¸ç”¨æˆ·äºŒæ¬¡ç¼–è¾‘
        words_str = st.text_area("ç¡®è®¤å•è¯åˆ—è¡¨ (å¯æ‰‹åŠ¨å¢åˆ )", ", ".join(st.session_state['words']))
        
        if st.button("ğŸ“‹ ç”Ÿæˆ AI Prompt"):
            final_list = [w.strip() for w in words_str.split(',') if w.strip()]
            prompt = get_ai_prompt(final_list)
            st.code(prompt, language="markdown")
            st.info("ğŸ‘† å¤åˆ¶ä¸Šé¢ä»£ç å—å‘ç»™ ChatGPT/Claude/DeepSeekã€‚ç„¶åæŠŠå®ƒçš„å›å¤å¤åˆ¶ä¸‹æ¥ã€‚")

with t2:
    st.markdown("##### ğŸ› ï¸ åˆ¶ä½œ Anki åŒ…")
    st.markdown("<div class='instruction'>å°† AI å›å¤çš„ç®¡é“ç¬¦æ ¼å¼å†…å®¹ (ä¸å« ```) ç²˜è´´åˆ°ä¸‹æ–¹ï¼š</div>", unsafe_allow_html=True)
    
    ai_response = st.text_area("ç²˜è´´ AI å›å¤æ•°æ®", height=200, placeholder="word | ipa | def | ex | etym")
    deck_title = st.text_input("ç‰Œç»„åç§°", "My Vocab Deck")
    
    if st.button("ğŸ“¦ ç”Ÿæˆ .apkg (iOS ä¸“ç”¨)"):
        if not ai_response.strip():
            st.error("å†…å®¹ä¸ºç©º")
        else:
            # è§£ææ•°æ®
            lines = ai_response.strip().split('\n')
            cards = []
            err_cnt = 0
            
            for line in lines:
                if "|" not in line: continue
                parts = [p.strip() for p in line.split('|')]
                if len(parts) >= 3: # è‡³å°‘è¦æœ‰å•è¯ã€éŸ³æ ‡ã€é‡Šä¹‰
                    cards.append({
                        'word': parts[0],
                        'ipa': parts[1] if len(parts) > 1 else '',
                        'meaning': parts[2] if len(parts) > 2 else '',
                        'examples': parts[3] if len(parts) > 3 else '',
                        'etymology': parts[4] if len(parts) > 4 else ''
                    })
                else:
                    err_cnt += 1
            
            if cards:
                # ç”Ÿæˆæ–‡ä»¶
                tmp_file_path = generate_anki_package(cards, deck_title)
                
                # è¯»å–äºŒè¿›åˆ¶æ•°æ®ç”¨äºä¸‹è½½
                with open(tmp_file_path, "rb") as f:
                    file_data = f.read()
                
                st.download_button(
                    label=f"ğŸ“¥ ä¸‹è½½ {deck_title}.apkg",
                    data=file_data,
                    file_name=f"{deck_title}.apkg",
                    mime="application/octet-stream",
                    type="primary"
                )
                
                st.success(f"æˆåŠŸç”Ÿæˆ {len(cards)} å¼ å¡ç‰‡ï¼(iOSä¸Šä¸‹è½½åé€‰æ‹©ç”¨Ankiæ‰“å¼€å³å¯)")
                if err_cnt > 0:
                    st.warning(f"è·³è¿‡äº† {err_cnt} è¡Œæ ¼å¼é”™è¯¯çš„è¡Œ")
            else:
                st.error("æœªèƒ½è¯†åˆ«æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥åˆ†éš”ç¬¦æ˜¯å¦ä¸º |")