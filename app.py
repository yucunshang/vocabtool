import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import json
import time
import requests
import zipfile
import concurrent.futures

# å°è¯•å¯¼å…¥å¤šæ ¼å¼æ–‡æ¡£å¤„ç†åº“
try:
    import PyPDF2
    import docx
except ImportError:
    st.error("âš ï¸ ç¼ºå°‘æ–‡ä»¶å¤„ç†ä¾èµ–ã€‚è¯·åœ¨ç»ˆç«¯è¿è¡Œ: pip install PyPDF2 python-docx")

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode { font-family: 'Consolas', 'Courier New', monospace !important; font-size: 16px !important; }
    header {visibility: hidden;} footer {visibility: hidden;}
    .block-container { padding-top: 1rem; }
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    [data-testid="stMetricValue"] { font-size: 28px !important; color: var(--primary-color) !important; }
    .param-box { background-color: var(--secondary-background-color); padding: 15px 20px 5px 20px; border-radius: 10px; border: 1px solid var(--border-color-light); margin-bottom: 20px; }
    .copy-hint { color: #888; font-size: 14px; margin-bottom: 5px; margin-top: 10px; padding-left: 5px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ•°æ®ä¸ NLP åˆå§‹åŒ–
# ==========================================
@st.cache_data
def load_knowledge_base():
    try:
        if not os.path.exists('data'):
            return {}, {}, {}, set()
            
        with open('data/terms.json', 'r', encoding='utf-8') as f: terms = {k.lower(): v for k, v in json.load(f).items()}
        with open('data/proper.json', 'r', encoding='utf-8') as f: proper = {k.lower(): v for k, v in json.load(f).items()}
        with open('data/patch.json', 'r', encoding='utf-8') as f: patch = json.load(f)
        with open('data/ambiguous.json', 'r', encoding='utf-8') as f: ambiguous = set(json.load(f))
        return terms, proper, patch, ambiguous
    except Exception as e:
        print(f"Knowledge base load error: {e}")
        return {}, {}, {}, set()

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    for pkg in ['averaged_perceptron_tagger', 'punkt']:
        try: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
        except: pass
setup_nltk()

def get_lemma(w):
    lemmas_dict = lemminflect.getAllLemmas(w)
    if not lemmas_dict: return w.lower()
    for pos in ['ADJ', 'ADV', 'VERB', 'NOUN']:
        if pos in lemmas_dict: return lemmas_dict[pos][0]
    return list(lemmas_dict.values())[0][0]

@st.cache_data
def load_vocab():
    vocab = {}
    file_path = next((f for f in ["coca_cleaned.csv", "data.csv"] if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
            df = df.sort_values(r_col, ascending=True).drop_duplicates(subset=[w_col], keep='first')
            vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except: pass
    
    for word, rank in BUILTIN_PATCH_VOCAB.items(): vocab[word] = rank
    
    # æ‰‹åŠ¨é™çº§/è¦†ç›–ç‰¹å®šå•è¯çš„æƒé‡ (å°†å¸¸ç”¨æ•°å­—è¯/åºæ•°è¯å¼ºè¡Œå‹åˆ° 1000 ä»¥å†…)
    URGENT_OVERRIDES = {
        "china": 400, "turkey": 1500, "march": 500, "may": 100, "august": 1500, "polish": 2500,
        "monday": 300, "tuesday": 300, "wednesday": 300, "thursday": 300, "friday": 300, "saturday": 300, "sunday": 300,
        "january": 400, "february": 400, "april": 400, "june": 400, "july": 400, "september": 400, "october": 400, "november": 400, "december": 400,
        "usa": 200, "uk": 200, "google": 1000, "apple": 1000, "microsoft": 1500
    }
    
    # è¿½åŠ ï¼šå¸¸è§åŸºæ•°è¯ä¸åºæ•°è¯é™çº§åå•
    number_words = [
        "zero", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten",
        "eleven", "twelve", "thirteen", "fourteen", "fifteen", "sixteen", "seventeen", "eighteen", "nineteen", "twenty",
        "thirty", "forty", "fifty", "sixty", "seventy", "eighty", "ninety", "hundred", "thousand", "million", "billion",
        "first", "second", "third", "fourth", "fifth", "sixth", "seventh", "eighth", "ninth", "tenth",
        "eleventh", "twelfth", "thirteenth", "fourteenth", "fifteenth", "sixteenth", "seventeenth", "eighteenth", "nineteenth", "twentieth",
        "thirtieth", "fortieth", "fiftieth", "sixtieth", "seventieth", "eightieth", "ninetieth", "hundredth", "thousandth"
    ]
    for nw in number_words:
        URGENT_OVERRIDES[nw] = 1000

    for word, rank in URGENT_OVERRIDES.items(): vocab[word] = rank
    return vocab

vocab_dict = load_vocab()

# ==========================================
# 3. æ–‡æ¡£è§£æ & åŠ¨æ€ AI æç¤ºè¯å¼•æ“
# ==========================================
def extract_text_from_file(uploaded_file):
    ext = uploaded_file.name.split('.')[-1].lower()
    uploaded_file.seek(0)
    try:
        if ext == 'txt':
            return uploaded_file.getvalue().decode("utf-8", errors="ignore")
        elif ext == 'pdf':
            reader = PyPDF2.PdfReader(uploaded_file)
            return " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif ext == 'docx':
            doc = docx.Document(uploaded_file)
            return " ".join([p.text for p in doc.paragraphs])
        elif ext == 'epub':
            text_blocks = []
            with zipfile.ZipFile(uploaded_file) as z:
                for filename in z.namelist():
                    if filename.endswith(('.html', '.xhtml', '.htm', '.xml')):
                        try:
                            content = z.read(filename).decode('utf-8', errors='ignore')
                            clean_text = re.sub(r'<[^>]+>', ' ', content)
                            text_blocks.append(clean_text)
                        except: pass
            return " ".join(text_blocks)
    except Exception as e:
        st.error(f"æ–‡ä»¶è§£æå¤±è´¥: {e}")
        return ""
    return ""

def get_dynamic_prompt_template(export_format, front_style, add_pos, def_lang, ex_count, add_ety, split_polysemy):
    """
    åŠ¨æ€ç”Ÿæˆ Anki æé€Ÿåˆ¶å¡ Prompt (å¸¦ä¸¥æ ¼è¯­ä¹‰çº¦æŸæ¨¡å¼)
    """
    front_desc = "A natural phrase or collocation using the specific meaning." if front_style == "phrase" else "The target word itself."
    if add_pos:
        front_desc += " MUST append the precise part of speech tag at the end, e.g., ' (v)', ' (n)', ' (adj)'."
    else:
        front_desc += " Do NOT add part of speech tags."

    def_map = {
        "en": "English definition of the specific meaning",
        "zh": "Chinese definition of the specific meaning",
        "en_zh": "English definition followed by Chinese definition separated by a slash (/)"
    }
    def_desc = def_map.get(def_lang, "English definition")

    if ex_count == 0:
        ex_desc = ""
    elif ex_count == 1:
        ex_desc = "<br><br><em>Italicized example sentence</em>"
    else:
        examples = [f"{i+1}. <em>Italicized example sentence {i+1}</em>" for i in range(ex_count)]
        ex_desc = "<br><br>" + " <br><br> ".join(examples)

    ety_desc = "<br><br>ã€è¯æ ¹è¯ç¼€/è¯æºã€‘Chinese etymology or affix explanation." if add_ety else ""
    
    if split_polysemy:
        poly_rule = "Atomicity: ONE meaning per row. Polysemous words MUST be split into multiple separate rows. NEVER stack multiple definitions in one card."
    else:
        poly_rule = "One Card Per Word: Generate EXACTLY ONE row per input word. Extract ONLY the single most common/primary meaning. NEVER split a word into multiple cards."

    prompt = f"""# Role
You are an expert English linguist and a highly precise Anki flashcard generator.

# Task
Process the user's input words, auto-correct any spelling errors/abbreviations, and generate Anki flashcards strictly following the rules below.

# Strict Rules
1. Format: Pure {export_format} format in a single code block. NO conversational filler, NO markdown formatting outside the code block.
2. Structure: STRICTLY TWO COLUMNS per row. Format: "Column 1","Column 2"
3. Quotes: Both columns MUST be wrapped in double quotes. Use single quotes (' ') inside the text if needed.
4. {poly_rule}
5. Strict Alignment (CRITICAL): The generated phrase, part of speech (if requested), definition, example sentence(s), and etymology MUST strictly logically align with the EXACT SAME specific meaning of the target word. Do not mix definitions or examples of different meanings in a single card.

# Content Formatting
- Column 1 (Front): {front_desc} Do NOT bold or highlight the target word.
- Column 2 (Back): Must be exactly formatted as follows (using HTML tags):
  {def_desc}{ex_desc}{ety_desc}

# Action
Process the following list of words immediately and output ONLY the final code block:"""

    return prompt

# ==========================================
# 4. å¤šæ ¸å¹¶å‘ API å¼•æ“ (æ ¸å¿ƒæé€ŸåŒº)
# ==========================================
def _fetch_deepseek_chunk(batch_words, prompt_template, api_key):
    url = "https://api.deepseek.com/chat/completions".strip()
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    system_enforcement = "\n\nã€ç³»ç»Ÿç»å¯¹å¼ºåˆ¶æŒ‡ä»¤ã€‘ç°åœ¨æˆ‘å·²ç»å‘é€äº†å•è¯åˆ—è¡¨ï¼Œè¯·ç«‹å³ä¸”ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„æ•°æ®ä»£ç ï¼Œç»å¯¹ä¸å‡†å›å¤â€œå¥½çš„â€ã€â€œæ²¡é—®é¢˜â€ç­‰ä»»ä½•å®¢å¥—è¯ï¼Œç»å¯¹ä¸å‡†ä½¿ç”¨ ```csv ç­‰ Markdown è¯­æ³•åŒ…è£¹ä»£ç ï¼"
    full_prompt = f"{prompt_template}{system_enforcement}\n\nå¾…å¤„ç†å•è¯åˆ—è¡¨ï¼š\n{', '.join(batch_words)}"
    
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": full_prompt}],
        "temperature": 0.3,
        "max_tokens": 4096
    }
    
    try:
        for attempt in range(3):
            resp = requests.post(url, json=payload, headers=headers, timeout=90)
            if resp.status_code == 429: 
                time.sleep(2 * (attempt + 1))
                continue
            if resp.status_code == 402: return "âŒ ERROR_402_NO_BALANCE"
            elif resp.status_code == 401: return "âŒ ERROR_401_INVALID_KEY"
            resp.raise_for_status()
            
            result = resp.json()['choices'][0]['message']['content'].strip()
            
            if result.startswith("```"):
                lines = result.split('\n')
                if lines[0].startswith("```"): lines = lines[1:]
                if lines and lines[-1].startswith("```"): lines = lines[:-1]
                result = '\n'.join(lines).strip()
            return result
            
        return f"\nğŸš¨ æ‰¹æ¬¡è¶…æ—¶æˆ–è¢«é™æµï¼Œæ­¤æ‰¹æ¬¡ ({len(batch_words)}è¯) ç”Ÿæˆå¤±è´¥ã€‚"
    except Exception as e:
        return f"\nğŸš¨ æ‰¹æ¬¡è¯·æ±‚å‘ç”Ÿå¼‚å¸¸: {str(e)}"

def call_deepseek_api_chunked(prompt_template, words, progress_bar, status_text):
    try: api_key = st.secrets["DEEPSEEK_API_KEY"]
    except KeyError: return "âš ï¸ ç«™é•¿é…ç½®é”™è¯¯ï¼šæœªåœ¨ Streamlit åå° Secrets ä¸­é…ç½® DEEPSEEK_API_KEYã€‚"
    
    if not words: return "âš ï¸ é”™è¯¯ï¼šæ²¡æœ‰éœ€è¦ç”Ÿæˆçš„å•è¯ã€‚"
    
    MAX_WORDS = 250
    if len(words) > MAX_WORDS:
        st.warning(f"âš ï¸ ä¸ºä¿è¯å¹¶å‘ç¨³å®šï¼Œæœ¬æ¬¡ä»…æˆªå–å‰ **{MAX_WORDS}** ä¸ªå•è¯ã€‚")
        words = words[:MAX_WORDS]

    CHUNK_SIZE = 30  
    chunks = [words[i:i + CHUNK_SIZE] for i in range(0, len(words), CHUNK_SIZE)]
    total_words = len(words)
    processed_count = 0
    
    results_ordered = [None] * len(chunks)
    
    status_text.markdown("ğŸš€ **å¹¶å‘ä»»åŠ¡å·²å‘å°„ï¼** æ­£åœ¨å…¨é€Ÿç”Ÿæˆé¦–æ‰¹å¡ç‰‡ï¼ˆé¦–æ¬¡è¿”å›çº¦éœ€ 8~12 ç§’ï¼Œè¯·ç¨å€™ï¼‰...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_index = {
            executor.submit(_fetch_deepseek_chunk, chunk, prompt_template, api_key): i 
            for i, chunk in enumerate(chunks)
        }
        
        for future in concurrent.futures.as_completed(future_to_index):
            idx = future_to_index[future]
            chunk_size = len(chunks[idx])
            res = future.result()
            
            if "ERROR_402_NO_BALANCE" in res: return "âŒ é”™è¯¯ï¼šDeepSeek è´¦æˆ·ä½™é¢ä¸è¶³ï¼Œè¯·å……å€¼ã€‚"
            if "ERROR_401_INVALID_KEY" in res: return "âŒ é”™è¯¯ï¼šAPI Key æ— æ•ˆã€‚"
            
            results_ordered[idx] = res 
            
            processed_count += chunk_size
            current_progress = min(processed_count / total_words, 1.0)
            progress_bar.progress(current_progress)
            status_text.markdown(f"**âš¡ AI å¤šæ ¸å¹¶å‘å…¨é€Ÿç¼–çº‚ä¸­ï¼š** `{processed_count} / {total_words}` è¯")

    return "\n".join(filter(None, results_ordered))

# ==========================================
# 5. åˆ†æå¼•æ“
# ==========================================
def analyze_words(unique_word_list):
    unique_items = [] 
    JUNK_WORDS = {'s', 't', 'd', 'm', 'll', 've', 're'}
    for item_lower in unique_word_list:
        if len(item_lower) < 2 and item_lower not in ['a', 'i']: continue
        if item_lower in JUNK_WORDS: continue
        actual_rank = vocab_dict.get(item_lower, 99999)
        
        if item_lower in BUILTIN_TECHNICAL_TERMS:
            domain = BUILTIN_TECHNICAL_TERMS[item_lower]
            term_rank = actual_rank if actual_rank != 99999 else 15000
            unique_items.append({"word": f"{item_lower} ({domain})", "rank": term_rank, "raw": item_lower})
            continue
        if item_lower in PROPER_NOUNS_DB or item_lower in AMBIGUOUS_WORDS:
            display = PROPER_NOUNS_DB.get(item_lower, item_lower.title())
            unique_items.append({"word": display, "rank": actual_rank, "raw": item_lower})
            continue
        if actual_rank != 99999:
            unique_items.append({"word": item_lower, "rank": actual_rank, "raw": item_lower})
            
    return pd.DataFrame(unique_items)

# ==========================================
# 6. UI ä¸æµæ°´çº¿çŠ¶æ€ç®¡ç†
# ==========================================
st.title("ğŸš€ Vocab Master Pro - Stable")
st.markdown("ğŸ’¡ æ”¯æŒç²˜è´´é•¿æ–‡æˆ–ç›´æ¥ä¸Šä¼  `TXT / PDF / DOCX / EPUB` æ–‡ä»¶ï¼Œå¹¶**å†…ç½®å…è´¹ AI** ä¸€é”®ç”Ÿæˆ Anki è®°å¿†å¡ç‰‡ã€‚")

if "raw_input_text" not in st.session_state: st.session_state.raw_input_text = ""
if "uploader_key" not in st.session_state: st.session_state.uploader_key = 0 
if "is_processed" not in st.session_state: st.session_state.is_processed = False

def clear_all_inputs():
    st.session_state.raw_input_text = ""
    st.session_state.uploader_key += 1 
    st.session_state.is_processed = False
    if 'base_df' in st.session_state: del st.session_state.base_df

# === æ›´æ–°ï¼šæŒ‰ç…§æˆªå›¾è®¾ç½®å‚æ•°é»˜è®¤å€¼ ===
st.markdown("<div class='param-box'>", unsafe_allow_html=True)
c1, c2, c3, c4, c5 = st.columns(5)
with c1: current_level = st.number_input("ğŸ¯ å½“å‰è¯æ±‡é‡ (èµ·)", 0, 30000, 9000, 500)     # ä¿®æ”¹é»˜è®¤å€¼ä¸º 9000
with c2: target_level = st.number_input("ğŸ¯ ç›®æ ‡è¯æ±‡é‡ (æ­¢)", 0, 30000, 15000, 500)    # ä¿®æ”¹é»˜è®¤å€¼ä¸º 15000
with c3: top_n = st.number_input("ğŸ”¥ ç²¾é€‰ Top N", 10, 500, 100, 10)                 # ä¿®æ”¹é»˜è®¤å€¼ä¸º 100
with c4: min_rank_threshold = st.number_input("ğŸ“‰ å¿½ç•¥å‰ N è¯", 0, 20000, 10000, 500) # ä¿®æ”¹é»˜è®¤å€¼ä¸º 10000
with c5: 
    st.write("") 
    st.write("") 
    show_rank = st.checkbox("ğŸ”¢ é™„åŠ æ˜¾ç¤º Rank", value=True)
st.markdown("</div>", unsafe_allow_html=True)

col_input1, col_input2 = st.columns([3, 2])
with col_input1:
    raw_text = st.text_area("ğŸ“¥ ç²˜è´´æ–‡æœ¬ (æ”¯æŒ10ä¸‡å­—ä»¥å†…)", height=150, key="raw_input_text")
with col_input2:
    st.info("ğŸ’¡ **å¤šæ ¼å¼è§£æ**ï¼šç›´æ¥æ‹–å…¥ç”µå­ä¹¦/è®ºæ–‡åŸè‘— ğŸ‘‡")
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£", type=["txt", "pdf", "docx", "epub"], key=f"uploader_{st.session_state.uploader_key}")

col_btn1, col_btn2 = st.columns([5, 1])
with col_btn1: btn_process = st.button("ğŸš€ æé€Ÿæ™ºèƒ½è§£æ", type="primary", use_container_width=True)
with col_btn2: st.button("ğŸ—‘ï¸ ä¸€é”®æ¸…ç©º", on_click=clear_all_inputs, use_container_width=True)

st.divider()

# ==========================================
# 7. åå°ç¡¬æ ¸è®¡ç®—
# ==========================================
if btn_process:
    with st.spinner("ğŸ§  æ­£åœ¨æ€¥é€Ÿè¯»å–æ–‡ä»¶å¹¶è¿›è¡Œæ™ºèƒ½è§£æï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰..."):
        start_time = time.time()
        combined_text = raw_text
        if uploaded_file is not None: combined_text += "\n" + extract_text_from_file(uploaded_file)
            
        if not combined_text.strip():
            st.warning("âš ï¸ æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æœ¬ï¼")
            st.session_state.is_processed = False
        elif vocab_dict:
            raw_words = re.findall(r"[a-zA-Z']+", combined_text)
            unique_raw_words = list(set(raw_words)) 
            lemmatized_unique = [get_lemma(w).lower() for w in unique_raw_words]
            unique_lemmas = list(set(lemmatized_unique)) 
            
            st.session_state.base_df = analyze_words(unique_lemmas)
            
            st.session_state.stats = {
                "raw_count": len(raw_words),
                "unique_count": len(unique_lemmas),
                "valid_count": len(st.session_state.base_df),
                "time": time.time() - start_time
            }
            st.session_state.is_processed = True

# ==========================================
# 8. åŠ¨æ€ç•Œé¢æ¸²æŸ“
# ==========================================
if st.session_state.get("is_processed", False):
    
    stats = st.session_state.stats
    col_m1, col_m2, col_m3, col_m4 = st.columns(4)
    col_m1.metric(label="ğŸ“ è§£ææ€»å­—æ•°", value=f"{stats['raw_count']:,}")
    col_m2.metric(label="âœ‚ï¸ å»é‡è¯æ ¹æ•°", value=f"{stats['unique_count']:,}")
    col_m3.metric(label="ğŸ¯ çº³å…¥åˆ†çº§è¯æ±‡", value=f"{stats['valid_count']:,}")
    col_m4.metric(label="âš¡ æé€Ÿè§£æè€—æ—¶", value=f"{stats['time']:.2f} ç§’")
    
    df = st.session_state.base_df.copy()
    
    if not df.empty:
        def categorize(row):
            r = row['rank']
            if r <= current_level: return "known"
            elif r <= target_level: return "target"
            else: return "beyond"
        
        df['final_cat'] = df.apply(categorize, axis=1)
        df = df.sort_values(by='rank')
        top_df = df[df['rank'] >= min_rank_threshold].sort_values(by='rank', ascending=True).head(top_n)
        
        t_top, t_target, t_beyond, t_known = st.tabs([
            f"ğŸ”¥ Top {len(top_df)}", 
            f"ğŸŸ¡ é‡ç‚¹ ({len(df[df['final_cat']=='target'])})", 
            f"ğŸ”´ è¶…çº² ({len(df[df['final_cat']=='beyond'])})", 
            f"ğŸŸ¢ å·²æŒæ¡ ({len(df[df['final_cat']=='known'])})"
        ])
        
        def render_tab(tab_obj, data_df, label, expand_default=False, df_key=""):
            with tab_obj:
                if not data_df.empty:
                    display_lines = []
                    for _, row in data_df.iterrows():
                        if show_rank:
                            rank_str = str(int(row['rank'])) if row['rank'] != 99999 else "æœªæ”¶å½•"
                            display_lines.append(f"{row['word']} [Rank: {rank_str}]")
                        else:
                            display_lines.append(row['word'])
                    
                    with st.expander("ğŸ‘ï¸ æŸ¥çœ‹å•è¯åˆ—è¡¨", expanded=expand_default):
                        st.markdown("<p class='copy-hint'>ğŸ‘† é¼ æ ‡æ‚¬åœåœ¨ä¸‹æ–¹æ¡†å†…ï¼Œç‚¹å‡»å³ä¸Šè§’ ğŸ“‹ å›¾æ ‡ä¸€é”®å¤åˆ¶å•è¯</p>", unsafe_allow_html=True)
                        st.code("\n".join(display_lines), language='text')
                    
                    st.divider()
                    
                    st.markdown("#### âš™ï¸ å®šåˆ¶å¡ç‰‡å†…å®¹")
                    ui_col1, ui_col2 = st.columns(2)
                    
                    # === æ›´æ–°ï¼šæŒ‰ç…§æˆªå›¾è®¾ç½®åˆ¶å¡ç•Œé¢çš„é»˜è®¤å•é€‰æ¡†å’Œå¤é€‰æ¡† ===
                    with ui_col1:
                        st.markdown("**æ­£é¢é…ç½® (Front)**")
                        export_format = st.radio("è¾“å‡ºæ ¼å¼:", ["TXT", "CSV"], horizontal=True, key=f"fmt_{df_key}", index=0)
                        ui_front = st.radio("å‘ˆç°å½¢å¼:", ["çŸ­è¯­/æ­é… (Phrase)", "ä»…å•è¯ (Word Only)"], horizontal=True, key=f"front_{df_key}", index=0)
                        ui_pos = st.checkbox("é™„åŠ è¯æ€§æ ‡ç¤º (å¦‚ v, n)", value=True, key=f"pos_{df_key}")
                        ui_poly = st.radio("å¤šä¹‰è¯å¤„ç†:", ["æ‹†åˆ†ä¸ºå¤šå¼ å¡ç‰‡ (åŸç‰ˆé»˜è®¤)", "ä»…ç”Ÿæˆæ ¸å¿ƒé‡Šä¹‰ (1è¯1å¡)"], index=1, horizontal=True, key=f"poly_{df_key}") # é»˜è®¤é€‰ä¸­ç¬¬äºŒé¡¹

                    with ui_col2:
                        st.markdown("**èƒŒé¢é…ç½® (Back)**")
                        ui_def = st.radio("é‡Šä¹‰è¯­è¨€:", ["çº¯è‹±æ–‡ (EN)", "çº¯ä¸­æ–‡ (ZH)", "ä¸­è‹±åŒè¯­ (EN+ZH)"], index=0, horizontal=True, key=f"def_{df_key}") # é»˜è®¤é€‰ä¸­çº¯è‹±æ–‡
                        ui_ex = st.slider("ä¾‹å¥æ•°é‡:", 0, 5, 1, key=f"ex_{df_key}")
                        ui_ety = st.checkbox("åŒ…å«ã€è¯æ ¹è¯ç¼€/è¯æºã€‘", value=True, key=f"ety_{df_key}")

                    front_style_val = "phrase" if "çŸ­è¯­" in ui_front else "word"
                    def_lang_val = "en" if "çº¯è‹±æ–‡" in ui_def else "zh" if "çº¯ä¸­æ–‡" in ui_def else "en_zh"
                    split_poly_val = True if "æ‹†åˆ†" in ui_poly else False
                    
                    custom_prompt_text = get_dynamic_prompt_template(
                        export_format=export_format,
                        front_style=front_style_val,
                        add_pos=ui_pos,
                        def_lang=def_lang_val,
                        ex_count=ui_ex,
                        add_ety=ui_ety,
                        split_polysemy=split_poly_val
                    )
                    
                    words_to_process = data_df['raw'].tolist()

                    ai_tab1, ai_tab2 = st.tabs(["ğŸ¤– æ¨¡å¼ 1ï¼šå†…ç½® AI å¹¶å‘æé€Ÿç›´å‡º", "ğŸ“‹ æ¨¡å¼ 2ï¼šå¤åˆ¶ Prompt ç»™ç¬¬ä¸‰æ–¹ AI"])
                    
                    with ai_tab1:
                        st.info("ğŸ’¡ ç«™é•¿å·²ä¸ºæ‚¨å†…ç½®ä¸“å± AI ç®—åŠ›ã€‚é‡‡ç”¨ **å¤šæ ¸å¹¶å‘æŠ€æœ¯**ï¼Œæé€Ÿå“åº”ï¼Œå‘Šåˆ«å¡æ­»ï¼")
                        
                        custom_prompt = st.text_area(
                            "ğŸ“ æœ€ç»ˆ AI Prompt (ç³»ç»Ÿå·²æ ¹æ®æ‚¨çš„è®¾ç½®åŠ¨æ€ç”Ÿæˆï¼Œæ”¯æŒæ‰‹åŠ¨å¾®è°ƒ)", 
                            value=custom_prompt_text, 
                            height=380, 
                            key=f"prompt_{df_key}_{export_format}"
                        )
                        
                        if st.button("âš¡ å¬å”¤ DeepSeek æé€Ÿç”Ÿæˆå¡ç‰‡", key=f"btn_{df_key}", type="primary"):
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            status_text.markdown("**âš¡ æ­£åœ¨è¿æ¥ DeepSeek äº‘ç«¯ç®—åŠ›é›†ç¾¤...**") 
                            
                            ai_start_time = time.time()
                            ai_result = call_deepseek_api_chunked(custom_prompt, words_to_process, progress_bar, status_text)
                            ai_duration = time.time() - ai_start_time
                            
                            if "âŒ" in ai_result and len(ai_result) < 100:
                                st.error(ai_result)
                            else:
                                status_text.markdown(f"### ğŸ‰ ç¼–çº‚å…¨éƒ¨å®Œæˆï¼(æ€»è€—æ—¶: **{ai_duration:.2f}** ç§’)")
                                
                                mime_type = "text/csv" if export_format == "CSV" else "text/plain"
                                st.download_button(
                                    label=f"ğŸ“¥ ä¸€é”®ä¸‹è½½æ ‡å‡† Anki å¯¼å…¥æ–‡ä»¶ (.{export_format.lower()})", 
                                    data=ai_result.encode('utf-8-sig'), 
                                    file_name=f"anki_cards_{label}.{export_format.lower()}", 
                                    mime=mime_type,
                                    type="primary",
                                    use_container_width=True
                                )
                                
                                st.markdown("##### ğŸ“ é¢„è§ˆæ¡†")
                                st.code(ai_result, language="text")
                    
                    with ai_tab2:
                        st.info("ğŸ’¡ å¦‚æœæ‚¨æƒ³ä½¿ç”¨ ChatGPT/Claude ç­‰è‡ªå·±çš„ AI å·¥å…·ï¼Œè¯·ç‚¹å‡»å³ä¸Šè§’ä¸€é”®å¤åˆ¶ä¸‹æ–¹å®Œæ•´æŒ‡ä»¤ï¼š")
                        full_prompt_to_copy = f"{custom_prompt_text}\n\nå¾…å¤„ç†å•è¯ï¼š\n{', '.join(words_to_process)}"
                        st.markdown("<p class='copy-hint'>ğŸ‘† é¼ æ ‡æ‚¬åœåœ¨ä¸‹æ–¹æ¡†å†…ï¼Œç‚¹å‡»å³ä¸Šè§’ ğŸ“‹ å›¾æ ‡ä¸€é”®å¤åˆ¶</p>", unsafe_allow_html=True)
                        st.code(full_prompt_to_copy, language='markdown')
                else: st.info("è¯¥åŒºé—´æš‚æ— å•è¯")

        render_tab(t_top, top_df, "Topç²¾é€‰", expand_default=True, df_key="top") 
        render_tab(t_target, df[df['final_cat']=='target'], "é‡ç‚¹", expand_default=False, df_key="target")
        render_tab(t_beyond, df[df['final_cat']=='beyond'], "è¶…çº²", expand_default=False, df_key="beyond")
        render_tab(t_known, df[df['final_cat']=='known'], "ç†Ÿè¯", expand_default=False, df_key="known")