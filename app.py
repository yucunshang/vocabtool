import streamlit as st
import pandas as pd
import re
import lemminflect
import nltk
# æ–°å¢åº“
import PyPDF2
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import io

# ç¡®ä¿ NLTK æ•°æ®å·²ä¸‹è½½
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
    nltk.download('punkt')
    nltk.download('averaged_perceptron_tagger')
    nltk.download('wordnet')

# ================= é…ç½®åŒº =================
COCA_FILE_PATH = 'coca20000.csv'  # å‡è®¾ä½ å·²æœ‰è¿™ä¸ªæ–‡ä»¶
DEFAULT_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­è¨€å­¦ä¹ åŠ©æ‰‹ã€‚è¯·åˆ†æä»¥ä¸‹è‹±è¯­å•è¯åˆ—è¡¨ã€‚
è¯·ä¸ºæ¯ä¸ªå•è¯æä¾›ä»¥ä¸‹å†…å®¹ï¼Œå¹¶ä¸¥æ ¼ä»¥ CSV æ ¼å¼è¾“å‡º (åˆ†éš”ç¬¦ä¸º | )ï¼š
å•è¯ | éŸ³æ ‡ | è¯æ€§ | ä¸­æ–‡ç®€æ˜é‡Šä¹‰ | å¸¸è§æ­é…(è‹±æ–‡çŸ­è¯­) | è®°å¿†æ³•(è¯æ ¹/è”æƒ³)

å•è¯åˆ—è¡¨ï¼š
{words}

æ³¨æ„ï¼š
1. ä¸è¦åŒ…å«è¡¨å¤´ã€‚
2. é‡Šä¹‰è¦ç²¾ç®€ï¼Œé€‚åˆèƒŒè¯µã€‚
3. å¦‚æœå•è¯æ˜¯å¤šä¹‰è¯ï¼Œä¼˜å…ˆæä¾›æœ€å¸¸ç”¨çš„å«ä¹‰ã€‚
"""

# ================= æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================

# 1. æ˜ å°„é€»è¾‘ (Q1)
def get_exam_tag(rank):
    if pd.isna(rank): return "æœªçŸ¥"
    rank = int(rank)
    if rank <= 2000: return "åˆä¸­/åŸºç¡€"
    if rank <= 4000: return "é«˜ä¸­/å››çº§"
    if rank <= 6000: return "å…­çº§/è€ƒç ”"
    if rank <= 9000: return "é›…æ€/æ‰˜ç¦"
    if rank <= 15000: return "GRE/ä¸“å…«"
    return "åŸè‘—/è¶…çº²"

# 2. å¤šæ ¼å¼è§£æ (Q4)
def extract_text_from_file(uploaded_file):
    text = ""
    file_type = uploaded_file.name.split('.')[-1].lower()
    
    try:
        if file_type == 'txt':
            text = uploaded_file.getvalue().decode("utf-8")
            
        elif file_type == 'pdf':
            reader = PyPDF2.PdfReader(uploaded_file)
            for page in reader.pages:
                text += page.extract_text() + "\n"
                
        elif file_type == 'epub':
            # éœ€è¦ä¿å­˜ä¸´æ—¶æ–‡ä»¶å› ä¸º ebooklib ä¸æ”¯æŒç›´æ¥è¯» stream
            with open("temp.epub", "wb") as f:
                f.write(uploaded_file.getbuffer())
            book = epub.read_epub("temp.epub")
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_body_content(), 'html.parser')
                    text += soup.get_text() + "\n"
                    
        elif file_type in ['srt', 'vtt']:
            # ç®€å•ç²—æš´å»æ—¶é—´è½´
            content = uploaded_file.getvalue().decode("utf-8")
            # ç§»é™¤æ—¶é—´è½´å’Œåºå· (ç®€å•æ­£åˆ™)
            lines = [l for l in content.splitlines() if not re.match(r'(\d{2}:\d{2})|(\d+$)', l)]
            text = "\n".join(lines)
            
    except Exception as e:
        st.error(f"è§£ææ–‡ä»¶å¤±è´¥: {str(e)}")
        
    return text

# 3. ä¼˜åŒ–ç‰ˆè¯å½¢è¿˜åŸ (Q3) - å¢åŠ ç®€å•çš„è¯æ€§åˆ¤æ–­
def smart_lemmatize(text):
    words = nltk.word_tokenize(text)
    # è·å–ä¸Šä¸‹æ–‡è¯æ€§ï¼Œå¸®åŠ©è¿˜åŸ (better -> good)
    pos_tags = nltk.pos_tag(words) 
    
    lemmatized_words = []
    for word, tag in pos_tags:
        if not word[0].isalpha(): continue # è¿‡æ»¤æ ‡ç‚¹
        
        # å°† NLTK tag è½¬æ¢ä¸º lemminflect tag
        if tag.startswith('J'): tag_type = 'ADJ'
        elif tag.startswith('V'): tag_type = 'VERB'
        elif tag.startswith('R'): tag_type = 'ADV'
        else: tag_type = 'NOUN' # é»˜è®¤
        
        lemma = lemminflect.getLemma(word, upos=tag_type)
        if not lemma: lemma = word.lower() # å…œåº•
        else: lemma = lemma[0] # getLemmaè¿”å›åˆ—è¡¨
        
        lemmatized_words.append(lemma.lower())
        
    return lemmatized_words

# ================= UI ä¸»ç¨‹åº =================
st.title("ğŸš€ Vocab Master Pro v2.0")

# ä¾§è¾¹æ ï¼šAPI è®¾ç½® (Q7)
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    api_key = st.text_input("DeepSeek API Key (å¯é€‰)", type="password", help="å¡«å…¥åå¯ç›´æ¥ä¸€é”®ç”Ÿæˆè§£é‡Š")
    show_charts = st.checkbox("æ˜¾ç¤ºç»Ÿè®¡å›¾è¡¨", value=True) # (Q5)

# ä¸»åŒºåŸŸ
uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶ (æ”¯æŒ TXT, PDF, EPUB, SRT)", type=['txt', 'pdf', 'epub', 'srt'])
user_text = st.text_area("æˆ–ç›´æ¥ç²˜è´´æ–‡æœ¬", height=150)

if uploaded_file or user_text:
    # 1. è·å–æ–‡æœ¬
    if uploaded_file:
        raw_text = extract_text_from_file(uploaded_file)
    else:
        raw_text = user_text
        
    if raw_text:
        # 2. å¤„ç†æµç¨‹
        with st.spinner("æ­£åœ¨æ™ºèƒ½è§£æ & è¿˜åŸè¯å½¢..."):
            # A. è¿˜åŸ
            words = smart_lemmatize(raw_text)
            
            # B. é¢‘æ¬¡ç»Ÿè®¡
            word_counts = pd.Series(words).value_counts().reset_index()
            word_counts.columns = ['word', 'count']
            
            # C. è¯»å– COCA æ•°æ®åº“ (æ¨¡æ‹Ÿ)
            # å®é™…ä½¿ç”¨æ—¶è¯·è¯»å–ä½ çš„ coca20000.csv
            # df_coca = pd.read_csv(COCA_FILE_PATH) 
            # è¿™é‡Œåšä¸€ä¸ª Mock æ•°æ®æ–¹ä¾¿ä½ è¿è¡Œ demo
            mock_coca = pd.DataFrame({
                'word': ['the', 'apple', 'ephemeral', 'serendipity', 'abandon'],
                'rank': [1, 1000, 14000, 16000, 3000]
            })
            
            # D. åˆå¹¶æ•°æ®
            #df_merged = pd.merge(word_counts, df_coca, on='word', how='left')
            # ä¸´æ—¶ç”¨ mock æ¼”ç¤º
            df_merged = pd.merge(word_counts, mock_coca, on='word', how='left')
            
            # E. å¢åŠ è€ƒè¯•æ ‡ç­¾ (Q1)
            df_merged['Exam_Tag'] = df_merged['rank'].apply(get_exam_tag)
            
            # F. ç­›é€‰é€»è¾‘ (è¿™é‡Œå‡è®¾ç­›é€‰ç”Ÿè¯)
            # ç”¨æˆ·å¯ä»¥äº¤äº’å¼ç­›é€‰ Rank åŒºé—´
            min_rank, max_rank = st.slider("é€‰æ‹©è¯é¢‘èŒƒå›´ (Rank)", 0, 20000, (4000, 15000))
            filtered_df = df_merged[
                (df_merged['rank'] >= min_rank) & 
                (df_merged['rank'] <= max_rank)
            ].sort_values('rank')

        # 3. ç»“æœå±•ç¤ºåŒº
        st.success(f"è§£æå®Œæˆï¼åŸæ–‡å…± {len(words)} è¯ï¼Œç­›é€‰å‡º {len(filtered_df)} ä¸ªç›®æ ‡ç”Ÿè¯ã€‚")
        
        # (Q5) å¯è§†åŒ–åé¦ˆ
        if show_charts and not filtered_df.empty:
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("### éš¾åº¦åˆ†å¸ƒ (è€ƒè¯•æ ‡å‡†)")
                exam_dist = filtered_df['Exam_Tag'].value_counts()
                st.bar_chart(exam_dist)
            with col2:
                st.markdown("### è¯é¢‘åˆ†å¸ƒ")
                st.line_chart(filtered_df['rank'].reset_index(drop=True))

        # 4. å•è¯é¢„è§ˆä¸ Prompt ç”Ÿæˆ
        st.subheader("ğŸ“ ç”Ÿè¯åˆ—è¡¨ & AI ç”Ÿæˆ")
        
        # æ˜¾ç¤ºæ•°æ®è¡¨
        st.dataframe(filtered_df[['word', 'count', 'rank', 'Exam_Tag']], use_container_width=True)
        
        # (Q6) åŠ¨æ€ Prompt
        target_words = filtered_df['word'].tolist()
        words_str = ", ".join(target_words[:50]) # é™åˆ¶æ•°é‡é˜²æ­¢ Token çˆ†ç‚¸ï¼Œå®é™…å¯åˆ†æ‰¹
        
        default_prompt = DEFAULT_PROMPT_TEMPLATE.format(words=words_str)
        
        st.markdown("### ğŸ¤– å‘é€ç»™ AI")
        user_prompt = st.text_area("ç¼–è¾‘ Prompt (å¯ä¿®æ”¹è¦æ±‚)", value=default_prompt, height=200)
        
        col_copy, col_run = st.columns([1, 1])
        with col_copy:
            st.code(user_prompt, language="text")
            st.caption("ğŸ‘† ç‚¹å‡»å³ä¸Šè§’å¤åˆ¶ï¼Œå» ChatGPT/Claude ç²˜è´´")
            
        with col_run:
            if st.button("ğŸš€ ä½¿ç”¨ DeepSeek ç›´æ¥ç”Ÿæˆ (éœ€é…ç½® Key)", type="primary"):
                if not api_key:
                    st.warning("è¯·å…ˆåœ¨å·¦ä¾§è¾¹æ å¡«å…¥ API Key")
                else:
                    st.info("æ­£åœ¨è°ƒç”¨ DeepSeek V3 API (æ¨¡æ‹Ÿ)...")
                    # è¿™é‡Œæ¥å…¥ requests è°ƒç”¨ DeepSeek
                    # response = requests.post(...)
                    # st.markdown(response.choices[0].message.content)
                    st.success("API è°ƒç”¨åŠŸèƒ½éœ€è‡ªè¡Œå¯¹æ¥ requests åº“å®ç°ï¼Œé€»è¾‘å·²é€šï¼")