import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import time

# ==========================================
# 0. åŸºç¡€é…ç½®ä¸ç§»åŠ¨ç«¯é€‚é…
# ==========================================
st.set_page_config(
    page_title="Prompt Gen", 
    page_icon="ğŸ“±", 
    layout="centered", 
    initial_sidebar_state="collapsed"
)

# ç§»åŠ¨ç«¯ CSS æ·±åº¦ä¼˜åŒ–
st.markdown("""
<style>
    /* 1. å…¨å±€å®¹å™¨ï¼šå‡å°‘ç•™ç™½ï¼Œé€‚åº”æ‰‹æœºå± */
    .block-container { 
        padding-top: 1rem; 
        padding-bottom: 3rem; 
        padding-left: 1rem; 
        padding-right: 1rem;
    }
    
    /* 2. éšè—æ— å…³å…ƒç´  (é¡¶éƒ¨æ¡ã€é¡µè„šã€ä¾§è¾¹æ æŒ‰é’®) */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    
    /* 3. æŒ‰é’®ï¼šå¤§å°ºå¯¸ï¼Œåœ†è§’ï¼Œé€‚åˆæ‰‹æŒ‡ç‚¹å‡» */
    .stButton>button {
        width: 100%;
        border-radius: 12px;
        height: 3.5em;
        font-weight: bold;
        font-size: 18px !important;
        margin-top: 10px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    
    /* 4. è¾“å…¥æ¡†ï¼šé˜²æ­¢ iOS è‡ªåŠ¨ç¼©æ”¾ (å­—ä½“éœ€>=16px) */
    .stTextArea>div>div>textarea {
        font-size: 16px !important; 
        border-radius: 10px;
    }
    .stNumberInput input {
        font-size: 18px !important;
    }
    
    /* 5. æç¤ºæ¡†ç¾åŒ– */
    .stAlert {
        border-radius: 10px;
    }
    
    /* 6. ä»£ç å—ï¼šç´§å‡‘æ¨¡å¼ */
    .stCode {
        font-size: 14px !important;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºåŠ è½½ä¸ NLTK åˆå§‹åŒ–
# ==========================================
@st.cache_resource
def setup_nltk():
    """ä¸‹è½½å¿…è¦çš„ NLTK æ•°æ®åŒ…"""
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    try: 
        nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_dir, quiet=True)
        nltk.download('punkt', download_dir=nltk_data_dir, quiet=True)
    except: pass

setup_nltk()

@st.cache_data
def load_data():
    """åŠ è½½è¯é¢‘è¡¨å’Œè¿‡æ»¤è¡¨"""
    vocab_dict = {}     # Word -> Rank
    rank_map = {}       # Rank -> List of Words
    
    # 1. å°è¯•åŠ è½½è¯é¢‘è¡¨ (æ”¯æŒå¤šç§æ–‡ä»¶å)
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    
    if file_path:
        try:
            df = pd.read_csv(file_path)
            # è‡ªåŠ¨è¯†åˆ«åˆ—å
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            
            # å¯»æ‰¾ word å’Œ rank åˆ—
            w_col = next((c for c in cols if 'word' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c), cols[1])
            
            # æ¸…æ´—
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            # ç¡®ä¿ Rank æ˜¯æ•°å­—
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            df = df.dropna(subset=[r_col])
            df = df.sort_values(r_col).drop_duplicates(subset=[w_col])
            
            # æ„å»ºå­—å…¸
            vocab_dict = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
            
            # æ„å»ºåå‘æ˜ å°„ (Rank -> Word List)
            for w, r in vocab_dict.items():
                r = int(r)
                if r not in rank_map: rank_map[r] = []
                rank_map[r].append(w)
                
        except Exception as e:
            st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    return vocab_dict, rank_map

VOCAB_DICT, RANK_MAP = load_data()

def get_lemma(word):
    """è·å–å•è¯åŸå½¢"""
    return lemminflect.getLemma(word, upos='VERB')[0] 

# ==========================================
# 2. æ ¸å¿ƒåŠŸèƒ½ï¼šPrompt ç”Ÿæˆå™¨ (æœ€ç»ˆä¼˜åŒ–ç‰ˆ)
# ==========================================
def generate_strict_prompt(words):
    word_list_str = ", ".join(words)
    count = len(words)
    
    # è¿™æ˜¯æ‚¨è¦æ±‚çš„ã€ä¸è€ƒè™‘ Token é™åˆ¶ã€è¿½æ±‚æœ€é«˜è´¨é‡çš„ Prompt
    prompt = f"""Role: High-Efficiency Anki Card Creator
Task: Convert the provided word list into a strict CSV code block for Anki import.

--- OUTPUT FORMAT RULES ---

1. Structure: 2 Columns only. Comma-separated. All fields double-quoted.
   Format: "Front","Back"

2. Column 1 (Front):
   - Content: A **natural, high-frequency English collocation or short phrase** containing the target word.
   - Goal: Maximize context retention.
   - Style: Plain text (NO bolding, NO extra symbols).

3. Column 2 (Back):
   - Content: Definition + Example + Etymology.
   - HTML Layout: Definition<br><em>Example Sentence</em><br>ã€æºã€‘Etymology
   - Constraints:
     - Use <br> tags for clear visual separation.
     - Example sentence must be wrapped in <em> tags (Italics) and be **natural/native-sounding**.
     - Definition: Precise and clear English definition matching the context of the phrase.

4. Etymology Style (Strict):
   - Language: CHINESE (ä¸­æ–‡).
   - Style: Logic-based, concise. Use arrows (â†’) to show the evolution of meaning.
   - Format: ã€æºã€‘Root/Origin (Meaning) â†’ Result/Logic.
   - Example: ã€æºã€‘Lat. 'vigere' (æ´»è·ƒ) â†’ ç²¾åŠ›/æ´»åŠ›

5. Atomicity Principle (Crucial):
   - If a word has distinct meanings (e.g., Noun vs. Verb, or Literal vs. Metaphorical), **generate SEPARATE rows** for each distinct meaning. Do not combine them into one card.

6. Output Requirement:
   - Output the Code Block ONLY. No conversational text before or after.
   - Ensure specific CSV escaping if the content itself contains double quotes.

--- EXAMPLE OUTPUT ---
"limestone quarry","deep pit for extracting stone<br><em>The company owns a granite quarry.</em><br>ã€æºã€‘å¤æ³•è¯­ quarriere (æ–¹çŸ³) â†’ åˆ‡çŸ³åœº"
"hunter's quarry","animal pursued by a hunter<br><em>The eagle spotted its quarry.</em><br>ã€æºã€‘å¤æ³•è¯­ cuir (çš®é©) â†’ æ”¾çš®ä¸Šçš„å†…è„èµèµ â†’ çŒç‰©"
"stiffen with cold","make or become rigid<br><em>His muscles began to stiffen.</em><br>ã€æºã€‘stiff (åƒµç¡¬) + -en (ä½¿åŠ¨)"

--- MY WORD LIST ({count} words) ---
{word_list_str}
"""
    return prompt

# ==========================================
# 3. è¾…åŠ©åŠŸèƒ½ï¼šæ–‡æœ¬æå–
# ==========================================
def extract_text_from_file(uploaded_file):
    try:
        ext = uploaded_file.name.split('.')[-1].lower()
        if ext == 'txt':
            return uploaded_file.getvalue().decode("utf-8", errors="ignore")
        elif ext == 'pdf':
            import PyPDF2
            reader = PyPDF2.PdfReader(uploaded_file)
            return " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif ext == 'docx':
            import docx
            doc = docx.Document(uploaded_file)
            return " ".join([p.text for p in doc.paragraphs])
    except Exception as e:
        st.error(f"æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}")
        return ""
    return ""

def process_text_input(text, min_rank, max_rank):
    # 1. æ¸…æ´—
    words = re.findall(r"[a-zA-Z']+", text)
    # 2. è¿˜åŸ
    lemmas = set()
    for w in words:
        if len(w) < 2: continue
        lemma = get_lemma(w).lower()
        lemmas.add(lemma)
    
    # 3. è¿‡æ»¤ (æ ¹æ® Rank)
    filtered = []
    for w in lemmas:
        rank = VOCAB_DICT.get(w, 99999) # æ²¡æ‰¾åˆ°çš„è¯è§†ä¸ºç”Ÿåƒ»è¯(99999)
        if min_rank <= rank <= max_rank:
            filtered.append((w, rank))
            
    # 4. æ’åº (æŒ‰ Rank æ’åºï¼Œç¡®ä¿ç®€å•çš„åœ¨å‰æˆ–éš¾çš„åœ¨å‰ï¼Œè¿™é‡Œé»˜è®¤æŒ‰ Rank å‡åº)
    filtered.sort(key=lambda x: x[1])
    return [x[0] for x in filtered]

# ==========================================
# 4. ä¸»ç•Œé¢é€»è¾‘ (APP UI)
# ==========================================

st.title("âš¡ï¸ Anki Prompt Gen")

# æ£€æŸ¥æ•°æ®æ˜¯å¦åŠ è½½
if not VOCAB_DICT:
    st.error("âš ï¸ æœªæ‰¾åˆ°è¯é¢‘æ•°æ® (coca_cleaned.csv)ã€‚è¯·ç¡®ä¿æ–‡ä»¶åœ¨æ ¹ç›®å½•ä¸‹ã€‚")
else:
    # é¡¶éƒ¨å¯¼èˆª (ç±»ä¼¼ App Tab)
    mode = st.radio("æ¨¡å¼", ["ğŸ”¢ è¯é¢‘åˆ·è¯", "ğŸ“– æ–‡æœ¬æå–"], horizontal=True, label_visibility="collapsed")
    
    target_words = []
    
    # ----------------------
    # æ¨¡å¼ A: è¯é¢‘èŒƒå›´ç”Ÿæˆ
    # ----------------------
    if mode == "ğŸ”¢ è¯é¢‘åˆ·è¯":
        st.caption("é€‚åˆæ¯æ—¥å®šé‡åˆ·è¯")
        col1, col2 = st.columns(2)
        with col1:
            start_r = st.number_input("Start Rank", value=8000, step=50)
        with col2:
            end_r = st.number_input("End Rank", value=8050, step=50)
            
        if start_r >= end_r:
            st.warning("å¼€å§‹æ’åå¿…é¡»å°äºç»“æŸæ’å")
        else:
            # æå–åŒºé—´å•è¯
            found_words = []
            for r in range(start_r, end_r + 1):
                if r in RANK_MAP:
                    found_words.extend(RANK_MAP[r])
            
            # å»é‡å¹¶ä¿æŒé¡ºåº
            found_words = list(dict.fromkeys(found_words))
            
            if len(found_words) > 0:
                st.info(f"ğŸ¯ åŒºé—´ {start_r}-{end_r} å‘½ä¸­ **{len(found_words)}** ä¸ªå•è¯")
                
                # é¢„è§ˆ
                with st.expander(f"é¢„è§ˆåˆ—è¡¨ ({found_words[0]}...)", expanded=False):
                    st.text(", ".join(found_words))
                    
                target_words = found_words
            else:
                st.warning("è¯¥åŒºé—´æ²¡æœ‰æ‰¾åˆ°å•è¯ã€‚")

    # ----------------------
    # æ¨¡å¼ B: æ–‡æœ¬æå–ç”Ÿæˆ
    # ----------------------
    else:
        st.caption("ä»æ–‡ç« /å­—å¹•æå–ç”Ÿè¯")
        
        input_method = st.radio("Input", ["ç²˜è´´", "ä¸Šä¼ "], horizontal=True, label_visibility="collapsed")
        
        raw_text = ""
        if input_method == "ç²˜è´´":
            raw_text = st.text_area("åœ¨æ­¤ç²˜è´´", height=150, placeholder="ç²˜è´´è‹±æ–‡æ–‡ç« ...")
        else:
            uploaded = st.file_uploader("æ–‡ä»¶ (TXT/PDF/DOCX)", type=["txt", "pdf", "docx"])
            if uploaded:
                raw_text = extract_text_from_file(uploaded)
                if raw_text: st.success("âœ… æ–‡ä»¶å·²è¯»å–")

        # è¿‡æ»¤è®¾ç½®
        with st.expander("âš™ï¸ éš¾åº¦è¿‡æ»¤ (Rank)"):
            f_col1, f_col2 = st.columns(2)
            with f_col1:
                min_filter = st.number_input("å¿½ç•¥ç®€å•è¯ (Top X)", value=3000, step=500)
            with f_col2:
                max_filter = st.number_input("å¿½ç•¥ç”Ÿåƒ»è¯ (Bottom X)", value=20000, step=1000)
        
        if raw_text:
            if st.button("ğŸ” åˆ†æå¹¶æå–", type="primary"):
                target_words = process_text_input(raw_text, min_filter, max_filter)
                if not target_words:
                    st.warning("æœªæå–åˆ°ç¬¦åˆæ¡ä»¶çš„ç”Ÿè¯ã€‚")
                else:
                    st.success(f"ç­›é€‰å‡º {len(target_words)} ä¸ªç”Ÿè¯")
                    with st.expander("æŸ¥çœ‹ç»“æœ"):
                        st.text(", ".join(target_words))
                        # ä¸´æ—¶ä¿å­˜åˆ° session_state è¿™æ ·ä¸ä¼šåˆ·æ–°æ¶ˆå¤±
                        st.session_state['temp_words'] = target_words

        if 'temp_words' in st.session_state and mode == "ğŸ“– æ–‡æœ¬æå–":
             target_words = st.session_state['temp_words']

    # ----------------------
    # ç»“æœç”ŸæˆåŒº (é€šç”¨)
    # ----------------------
    if target_words:
        st.divider()
        
        # æ‰¹é‡å¤„ç†å»ºè®®
        MAX_BATCH = 100
        if len(target_words) > MAX_BATCH:
            st.warning(f"âš ï¸ å•è¯è¾ƒå¤š ({len(target_words)}ä¸ª)ï¼Œå»ºè®®åˆ†æ‰¹ã€‚å·²è‡ªåŠ¨æˆªå–å‰ {MAX_BATCH} ä¸ªã€‚")
            target_words = target_words[:MAX_BATCH]
        
        if st.button("ğŸš€ ç”Ÿæˆ Prompt (å‡†å¤‡å¤åˆ¶)", type="primary"):
            final_prompt = generate_strict_prompt(target_words)
            
            st.markdown("### ğŸ‘‡ ç‚¹å‡»ä»£ç å—å³ä¸Šè§’å¤åˆ¶")
            st.code(final_prompt, language="markdown")
            
            st.info("ğŸ’¡ å¤åˆ¶åï¼Œå‘é€ç»™ ChatGPT/Claudeã€‚å»ºè®®è¦æ±‚å®ƒç”Ÿæˆå¯ä¸‹è½½çš„ .csv æ–‡ä»¶ä»¥ä¾¿ç›´æ¥å¯¼å…¥ Ankiã€‚")