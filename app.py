import streamlit as st
import pandas as pd
import os
import sys
import subprocess

# ==========================================
# 1. Google Translate é£æ ¼é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Analyzer", page_icon="ğŸ…°ï¸")

# è‡ªå®šä¹‰ CSS è®©ç•Œé¢æ›´åƒ Google Translate (å¤§æ–‡æœ¬æ¡†ã€æ¸…çˆ½å­—ä½“)
st.markdown("""
<style>
    .stTextArea textarea {
        font-size: 18px !important;
        line-height: 1.5 !important;
        font-family: 'Roboto', sans-serif;
    }
    .stNumberInput input {
        font-weight: bold;
        color: #1a73e8;
    }
    /* éšè—éƒ¨åˆ†å¤šä½™çš„å…ƒç´  */
    header {visibility: hidden;}
    footer {visibility: hidden;}
    .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ ¸å¿ƒå¼•æ“ (spaCy + è‡ªåŠ¨ä¿®å¤)
# ==========================================
@st.cache_resource
def load_nlp():
    try:
        import spacy
    except ImportError:
        return None
    
    model_name = "en_core_web_sm"
    try:
        return spacy.load(model_name)
    except:
        # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰
        try:
            subprocess.check_call([sys.executable, "-m", "spacy", "download", model_name])
            return spacy.load(model_name)
        except:
            return None

# ==========================================
# 3. è¯åº“åŠ è½½ (é™é»˜åŠ è½½ï¼Œä¸æŠ¥é”™)
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv"]

@st.cache_data
def load_vocab():
    file_path = next((f for f in POSSIBLE_FILES if os.path.exists(f)), None)
    if not file_path: return None
    
    try:
        df = pd.read_csv(file_path)
        # æç®€æ¸…æ´—
        cols = [str(c).strip().lower() for c in df.columns]
        df.columns = cols
        
        # æ™ºèƒ½æ‰¾åˆ—
        w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
        r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
        
        df[w_col] = df[w_col].astype(str).str.lower().str.strip()
        df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
        
        return pd.Series(df[r_col].values, index=df[w_col]).to_dict()
    except:
        return None

# åˆå§‹åŒ–èµ„æº
nlp = load_nlp()
vocab = load_vocab()

# ==========================================
# 4. ç•Œé¢å¸ƒå±€ (Top Bar + Split View)
# ==========================================

# --- é¡¶éƒ¨ï¼šè®¾ç½®æ  ---
c1, c2, c3 = st.columns([1, 1, 3])
with c1:
    # æ­¥é•¿ 500ï¼Œé»˜è®¤ 6000
    current_level = st.number_input("å½“å‰è¯æ±‡é‡", min_value=0, max_value=20000, value=6000, step=500)
with c2:
    # æ­¥é•¿ 500ï¼Œé»˜è®¤ 8000
    target_level = st.number_input("ç›®æ ‡è¯æ±‡é‡", min_value=0, max_value=20000, value=8000, step=500)
with c3:
    st.write("") # å ä½

# --- ä¸»ä½“ï¼šå·¦å³åˆ†æ  ---
st.divider()
left_col, right_col = st.columns([1, 1])

# === å·¦ä¾§ï¼šè¾“å…¥åŒº ===
with left_col:
    st.markdown("### ğŸ“ è¾“å…¥æ–‡æœ¬")
    text_input = st.text_area(
        label="hidden_label",
        placeholder="åœ¨æ­¤ç²˜è´´è‹±è¯­æ–‡ç« ...",
        height=500,
        label_visibility="collapsed"
    )
    
    # æ”¾åœ¨å·¦ä¾§åº•éƒ¨çš„æŒ‰é’®
    analyze_btn = st.button("å¼€å§‹åˆ†æ / Analyze", type="primary", use_container_width=True)

# === å³ä¾§ï¼šç»“æœåŒº ===
with right_col:
    st.markdown("### ğŸ“Š åˆ†æç»“æœ")
    
    if not nlp:
        st.error("æ­£åœ¨åˆå§‹åŒ– NLP å¼•æ“ï¼Œè¯·ç¨ç­‰æˆ–åˆ·æ–°...")
    elif not vocab:
        st.error("æœªæ‰¾åˆ°è¯åº“æ–‡ä»¶ (coca_cleaned.csv)ï¼Œè¯·å…ˆä¸Šä¼ ã€‚")
    elif analyze_btn and text_input:
        
        with st.spinner("æ­£åœ¨æ‹†è§£æ–‡æœ¬..."):
            # 1. spaCy å¤„ç† (å¢åŠ  max_length é˜²æ­¢å¤§æ–‡æœ¬æŠ¥é”™)
            nlp.max_length = 2000000 
            doc = nlp(text_input.lower())
            
            # 2. æå–ä¸è¿˜åŸ
            seen = set()
            data = []
            
            for token in doc:
                if token.is_alpha and len(token.text) > 1:
                    lemma = token.lemma_ # è¿˜åŸ: families -> family
                    if lemma not in seen:
                        rank = vocab.get(lemma, 99999)
                        
                        # äºŒæ¬¡æŸ¥æ‰¾é€»è¾‘ (é˜²æ­¢ spaCy è¿˜åŸè¿‡åº¦)
                        if rank == 99999 and token.text in vocab:
                            rank = vocab[token.text]
                            lemma = token.text
                            
                        data.append({'word': lemma, 'rank': int(rank)})
                        seen.add(lemma)
            
            # 3. åˆ†ç»„
            df = pd.DataFrame(data)
            if not df.empty:
                df = df.sort_values('rank')
                
                # ä¸‰ä¸ªæ¡¶
                known = df[df['rank'] <= current_level]
                target = df[(df['rank'] > current_level) & (df['rank'] <= target_level)]
                beyond = df[df['rank'] > target_level]
                
                # 4. æ˜¾ç¤ºç»“æœ (Tabs)
                t1, t2, t3 = st.tabs([
                    f"ğŸŸ¡ é‡ç‚¹è¯ ({len(target)})", 
                    f"ğŸ”´ è¶…çº²è¯ ({len(beyond)})", 
                    f"ğŸŸ¢ å·²æŒæ¡ ({len(known)})"
                ])
                
                # å®šä¹‰ä¸€ä¸ªç®€å•çš„æ–‡æœ¬æ¸²æŸ“å‡½æ•°
                def render_list(dataframe, color_code):
                    if dataframe.empty:
                        st.info("åˆ—è¡¨ä¸ºç©º")
                        return
                    
                    # ç”Ÿæˆç®€å•çš„æ–‡æœ¬åˆ—è¡¨æ ¼å¼
                    # æ ¼å¼: 1. word (Rank: 123)
                    lines = []
                    for _, row in dataframe.iterrows():
                        lines.append(f"â€¢ **{row['word']}** _(Rank: {row['rank']})_")
                    
                    # ä½¿ç”¨ markdown æ˜¾ç¤ºï¼Œå¸¦æ»šåŠ¨æ¡å®¹å™¨
                    with st.container(height=400):
                        st.markdown("\n".join(lines))

                with t1:
                    render_list(target, "orange")
                with t2:
                    render_list(beyond, "red")
                with t3:
                    render_list(known, "green")
            else:
                st.warning("æœªæ£€æµ‹åˆ°æœ‰æ•ˆè‹±æ–‡å•è¯ã€‚")
                
    elif analyze_btn and not text_input:
        st.warning("è¯·å…ˆåœ¨å·¦ä¾§ç²˜è´´æ–‡æœ¬ã€‚")
    else:
        st.info("ç­‰å¾…è¾“å…¥...")