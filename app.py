import streamlit as st
import pandas as pd
import re
import time
from collections import Counter
import nltk
from nltk.stem import WordNetLemmatizer

# =====================================
# é¡µé¢é…ç½®
# =====================================
st.set_page_config(
    page_title="Vocab Master - Pro",
    layout="wide",
    page_icon="ğŸš€"
)

st.title("ğŸš€ Vocab Master - æ™ºèƒ½ NLP å¼•æ“")

# =====================================
# 1. åˆå§‹åŒ– NLTK èµ„æº (ç¼“å­˜ä¼˜åŒ–)
# =====================================
@st.cache_resource
def setup_nltk():
    """
    è‡ªåŠ¨ä¸‹è½½å¹¶ç¼“å­˜å¿…è¦çš„ NLTK æ•°æ®åŒ…ã€‚
    """
    try:
        nltk.data.find('corpora/wordnet')
        nltk.data.find('corpora/omw-1.4')
    except LookupError:
        nltk.download('wordnet')
        nltk.download('omw-1.4')
    
    return WordNetLemmatizer()

lemmatizer = setup_nltk()

# =====================================
# 2. ç¡¬ç¼–ç åœç”¨è¯è¡¨ (å»å™ªå…³é”®)
# =====================================
STOPWORDS = {
    'the', 'of', 'and', 'a', 'to', 'in', 'is', 'you', 'that', 'it', 'he', 'was', 
    'for', 'on', 'are', 'as', 'with', 'his', 'they', 'i', 'at', 'be', 'this', 
    'have', 'from', 'or', 'one', 'had', 'by', 'word', 'but', 'not', 'what', 
    'all', 'were', 'we', 'when', 'your', 'can', 'said', 'there', 'use', 'an', 
    'each', 'which', 'she', 'do', 'how', 'their', 'if', 'will', 'up', 'other', 
    'about', 'out', 'many', 'then', 'them', 'these', 'so', 'some', 'her', 
    'would', 'make', 'like', 'him', 'into', 'time', 'has', 'look', 'two', 
    'more', 'write', 'go', 'see', 'number', 'no', 'way', 'could', 'people', 
    'my', 'than', 'first', 'water', 'been', 'call', 'who', 'oil', 'its', 'now', 
    'find'
}

# =====================================
# 3. æ ¸å¿ƒä¿®æ”¹ï¼šåŠ è½½ CSV è¯åº“
# =====================================
@st.cache_data
def load_vocab():
    """
    è¯»å–åŒç›®å½•ä¸‹çš„ coca_cleaned.csv
    æœŸæœ›æ ¼å¼ï¼šåŒ…å« 'word' å’Œ 'rank' ä¸¤åˆ—
    """
    csv_path = "coca_cleaned.csv"
    
    try:
        # è¯»å– CSV
        df = pd.read_csv(csv_path)
        
        # 1. æ ‡å‡†åŒ–åˆ—åï¼šè½¬å°å†™å¹¶å»ç©ºæ ¼ (é˜²æ­¢ ' Rank' è¿™ç§æƒ…å†µ)
        df.columns = [c.strip().lower() for c in df.columns]
        
        # 2. æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
        # å‡è®¾ä½ çš„ CSV åˆ—åæ˜¯ 'word' å’Œ 'rank'
        # å¦‚æœä½ çš„åˆ—åæ˜¯ 'lemma', 'id' ç­‰ï¼Œè¯·ä¿®æ”¹ä¸‹é¢çš„å­—ç¬¦ä¸²
        word_col = 'word'
        rank_col = 'rank'
        
        if word_col not in df.columns or rank_col not in df.columns:
            st.error(f"CSV æ ¼å¼é”™è¯¯ï¼šæœªæ‰¾åˆ° '{word_col}' æˆ– '{rank_col}' åˆ—ã€‚ç°æœ‰åˆ—å: {list(df.columns)}")
            return {}

        # 3. æ•°æ®æ¸…æ´—ï¼šç¡®ä¿ word åˆ—æ˜¯å­—ç¬¦ä¸²ï¼Œå¹¶è½¬å°å†™
        df[word_col] = df[word_col].astype(str).str.lower()
        
        # 4. è½¬æ¢ä¸ºå­—å…¸ {word: rank}ï¼ŒæŸ¥æ‰¾é€Ÿåº¦ O(1)
        return dict(zip(df[word_col], df[rank_col]))
        
    except FileNotFoundError:
        st.error(f"æœªæ‰¾åˆ°æ–‡ä»¶ï¼š{csv_path}ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ è¯¥æ–‡ä»¶ã€‚")
        return {}
    except Exception as e:
        st.error(f"è¯»å–è¯åº“å¤±è´¥: {e}")
        return {}

vocab_dict = load_vocab()

# =====================================
# 4. NLP é€»è¾‘å‡½æ•°
# =====================================

def is_valid_word(w: str) -> bool:
    """è¿‡æ»¤æ‰è¿‡çŸ­çš„å•è¯ã€ä¹±ç æˆ–çº¯æ•°å­—"""
    if len(w) < 2 and w not in ("a", "i"):
        return False
    if w.count("'") > 1:
        return False
    if w.isdigit():
        return False
    return True

def get_lemma(word: str) -> str:
    """
    è¯å½¢è¿˜åŸï¼šRunning -> run
    """
    w = word.lower()
    return lemmatizer.lemmatize(w, pos='v')

def stream_analyze_text(text):
    """
    åŒè½¨åˆ†æï¼š
    lemma_tokens -> ç”¨äºæŸ¥è¯é¢‘è¡¨ (åŒ¹é… CSV ä¸­çš„ word)
    raw_tokens   -> ç”¨äºçŸ­è¯­è¯†åˆ« (ä¿ç•™ 'United States' åŸè²Œ)
    """
    freq = Counter()
    lemma_tokens = [] 
    raw_tokens = []   

    pattern = re.compile(r"[a-zA-Z']+")

    for match in pattern.finditer(text):
        original_word = match.group()
        
        # è¿˜åŸè¯å½¢ (ç”¨äºå’Œ CSV åŒ¹é…)
        lemma = get_lemma(original_word)
        
        if not is_valid_word(lemma):
            continue

        freq[lemma] += 1
        lemma_tokens.append(lemma)
        raw_tokens.append(original_word.lower())

    return lemma_tokens, raw_tokens, freq

def detect_phrases(raw_tokens, min_freq=2):
    """æ™ºèƒ½çŸ­è¯­æ£€æµ‹ + åœç”¨è¯è¿‡æ»¤"""
    if not raw_tokens:
        return []

    bigrams = zip(raw_tokens, raw_tokens[1:])
    trigrams = zip(raw_tokens, raw_tokens[1:], raw_tokens[2:])

    phrase_cnt = Counter()

    # è¿‡æ»¤è§„åˆ™ï¼šçŸ­è¯­é¦–å°¾ä¸èƒ½æ˜¯åœç”¨è¯
    for bg in bigrams:
        if bg[0] not in STOPWORDS and bg[-1] not in STOPWORDS:
            phrase_cnt[" ".join(bg)] += 1
            
    for tg in trigrams:
        if tg[0] not in STOPWORDS and tg[-1] not in STOPWORDS:
            phrase_cnt[" ".join(tg)] += 1

    # æ ¼å¼åŒ–è¾“å‡º
    results = []
    for p, f in phrase_cnt.items():
        if f >= min_freq:
            results.append((p, f))
            
    results.sort(key=lambda x: x[1], reverse=True)
    return results

def analyze_words(freq_dict):
    """
    æ ¸å¿ƒé€»è¾‘ï¼šç»“åˆ CSV è¯åº“è®¡ç®—æ’å
    """
    results = []
    
    # å¦‚æœè¯åº“åŠ è½½å¤±è´¥ï¼Œç»™å‡ºé»˜è®¤å€¼é˜²æ­¢æŠ¥é”™
    safe_vocab = vocab_dict if vocab_dict else {}
    
    for w, f in freq_dict.items():
        if w in STOPWORDS:
            continue
            
        # ä» CSV å­—å…¸ä¸­è·å–æ’åï¼Œæ‰¾ä¸åˆ°åˆ™è®¾ä¸º 99999 (ç”Ÿåƒ»è¯)
        rank = safe_vocab.get(w, 99999)
        
        # ç®€å•çš„éš¾åº¦åˆ†çº§é€»è¾‘ï¼ˆå¯é€‰ï¼‰
        tag = "ğŸŸ¢ åŸºç¡€"
        if rank > 5000: tag = "ğŸŸ¡ è¿›é˜¶" 
        if rank > 15000: tag = "ğŸ”´ é«˜éš¾/ç”Ÿåƒ»"
        if rank == 99999: tag = "âšª æœªæ”¶å½•"

        results.append({
            "word": w,
            "rank": rank,
            "freq": f,
            "tag": tag
        })
        
    # æŒ‰æ’åæ’åº (è¶Šå°è¶Šé‡è¦)ï¼Œå…¶æ¬¡æŒ‰é¢‘ç‡
    results.sort(key=lambda x: (x["rank"], -x["freq"]))
    return results

# =====================================
# UI é€»è¾‘
# =====================================

text_input = st.text_area(
    "ğŸ“¥ ç²˜è´´æ–‡æœ¬",
    height=220,
    placeholder="åœ¨æ­¤ç²˜è´´æ‚¨çš„è‹±æ–‡æ–‡ç« ..."
)

col1, col2 = st.columns(2)
with col1:
    min_phrase_freq = st.slider("çŸ­è¯­è¯†åˆ«æœ€ä½é¢‘ç‡", 2, 10, 2)

if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):

    if not text_input.strip():
        st.warning("è¯·è¾“å…¥æ–‡æœ¬")
        st.stop()
        
    # æ£€æŸ¥è¯åº“çŠ¶æ€
    if not vocab_dict:
        st.warning("âš ï¸ è­¦å‘Šï¼šè¯åº“æ–‡ä»¶åŠ è½½å¤±è´¥æˆ–ä¸ºç©ºï¼Œæ’ååŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")

    start = time.time()

    with st.spinner("æ­£åœ¨è§£ææ–‡æœ¬ & åŒ¹é…è¯åº“..."):
        
        lemma_tokens, raw_tokens, freq_dict = stream_analyze_text(text_input)
        phrases = detect_phrases(raw_tokens, min_phrase_freq)
        results = analyze_words(freq_dict)

    duration = time.time() - start

    # =================================
    # ç»“æœå±•ç¤º
    # =================================
    st.success(f"âœ… åˆ†æå®Œæˆï¼Œè€—æ—¶ {duration:.3f} ç§’")
    
    m1, m2, m3 = st.columns(3)
    m1.metric("æ€»è¯æ•° (Tokens)", len(raw_tokens))
    m2.metric("æ ¸å¿ƒè¯æ±‡é‡", len(results))
    m3.metric("è¯åº“è¦†ç›–ç‡", f"{len([r for r in results if r['rank'] < 99999]) / len(results) * 100:.1f}%" if results else "0%")

    st.divider()

    left_col, right_col = st.columns([1.3, 0.7])

    with left_col:
        st.subheader("ğŸ“Š è¯æ±‡åˆ†çº§ç»Ÿè®¡")
        if results:
            st.dataframe(
                results, 
                column_config={
                    "word": "å•è¯",
                    "rank": "COCAæ’å",
                    "freq": "æœ¬æ–‡é¢‘æ¬¡",
                    "tag": "éš¾åº¦åˆ†çº§"
                },
                use_container_width=True,
                height=600
            )
        else:
            st.info("æ²¡æœ‰å‘ç°æœ‰æ•ˆå•è¯")

    with right_col:
        st.subheader("ğŸ”— æ™ºèƒ½çŸ­è¯­")
        if phrases:
            st.dataframe(
                [{"Phrase": p, "Freq": f} for p, f in phrases], 
                use_container_width=True,
                height=600
            )
        else:
            st.info("æœªæ£€æµ‹åˆ°é«˜é¢‘çŸ­è¯­")