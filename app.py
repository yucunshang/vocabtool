import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import json
import time
import requests
import zipfile
import concurrent.futures  # å¤šæ ¸å¹¶å‘å¼•æ“

# å°è¯•å¯¼å…¥å¤šæ ¼å¼æ–‡æ¡£å¤„ç†åº“
try:
    import PyPDF2
    import docx
except ImportError:
    st.error("âš ï¸ ç¼ºå°‘æ–‡ä»¶å¤„ç†ä¾èµ–ã€‚è¯·åœ¨ç»ˆç«¯è¿è¡Œ: pip install PyPDF2 python-docx")

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode { font-family: 'Consolas', 'Courier New', monospace !important; font-size: 16px !important; }
    header {visibility: hidden;} footer {visibility: hidden;}
    .block-container { padding-top: 1rem; }
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    [data-testid="stMetricValue"] { font-size: 28px !important; color: var(--primary-color) !important; }
    .param-box { background-color: var(--secondary-background-color); padding: 15px 20px 5px 20px; border-radius: 10px; border: 1px solid var(--border-color-light); margin-bottom: 20px; }
    .copy-hint { color: #888; font-size: 14px; margin-bottom: 5px; margin-top: 10px; padding-left: 5px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ•°æ®ä¸ NLP åˆå§‹åŒ–
# ==========================================
@st.cache_data
def load_knowledge_base():
    try:
        with open('data/terms.json', 'r', encoding='utf-8') as f: terms = {k.lower(): v for k, v in json.load(f).items()}
        with open('data/proper.json', 'r', encoding='utf-8') as f: proper = {k.lower(): v for k, v in json.load(f).items()}
        with open('data/patch.json', 'r', encoding='utf-8') as f: patch = json.load(f)
        with open('data/ambiguous.json', 'r', encoding='utf-8') as f: ambiguous = set(json.load(f))
        return terms, proper, patch, ambiguous
    except FileNotFoundError:
        # st.error("âš ï¸ ç¼ºå°‘ data/ æ–‡ä»¶å¤¹ä¸‹çš„ JSON çŸ¥è¯†åº“æ–‡ä»¶ï¼") # æš‚æ—¶å±è”½æŠ¥é”™ä»¥ä¾¿æ¼”ç¤º
        return {}, {}, {}, set()

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    for pkg in ['averaged_perceptron_tagger', 'punkt']:
        try: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
        except: pass
setup_nltk()

def get_lemma(w):
    lemmas_dict = lemminflect.getAllLemmas(w)
    if not lemmas_dict: return w.lower()
    for pos in ['ADJ', 'ADV', 'VERB', 'NOUN']:
        if pos in lemmas_dict: return lemmas_dict[pos][0]
    return list(lemmas_dict.values())[0][0]

@st.cache_data
def load_vocab():
    vocab = {}
    file_path = next((f for f in ["coca_cleaned.csv", "data.csv"] if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
            df = df.sort_values(r_col, ascending=True).drop_duplicates(subset=[w_col], keep='first')
            vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except: pass
    
    for word, rank in BUILTIN_PATCH_VOCAB.items(): vocab[word] = rank
    URGENT_OVERRIDES = {
        "china": 400, "turkey": 1500, "march": 500, "may": 100, "august": 1500, "polish": 2500,
        "monday": 300, "tuesday": 300, "wednesday": 300, "thursday": 300, "friday": 300, "saturday": 300, "sunday": 300,
        "january": 400, "february": 400, "april": 400, "june": 400, "july": 400, "september": 400, "october": 400, "november": 400, "december": 400,
        "usa": 200, "uk": 200, "google": 1000, "apple": 1000, "microsoft": 1500
    }
    for word, rank in URGENT_OVERRIDES.items(): vocab[word] = rank
    return vocab

vocab_dict = load_vocab()

# ==========================================
# 3. æ–‡æ¡£è§£æ & AI æç¤ºè¯å¼•æ“
# ==========================================
def extract_text_from_file(uploaded_file):
    ext = uploaded_file.name.split('.')[-1].lower()
    uploaded_file.seek(0)
    try:
        if ext == 'txt':
            return uploaded_file.getvalue().decode("utf-8", errors="ignore")
        elif ext == 'pdf':
            reader = PyPDF2.PdfReader(uploaded_file)
            return " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif ext == 'docx':
            doc = docx.Document(uploaded_file)
            return " ".join([p.text for p in doc.paragraphs])
        elif ext == 'epub':
            text_blocks = []
            with zipfile.ZipFile(uploaded_file) as z:
                for filename in z.namelist():
                    if filename.endswith(('.html', '.xhtml', '.htm', '.xml')):
                        try:
                            content = z.read(filename).decode('utf-8', errors='ignore')
                            clean_text = re.sub(r'<[^>]+>', ' ', content)
                            text_blocks.append(clean_text)
                        except: pass
            return " ".join(text_blocks)
    except Exception as e:
        st.error(f"æ–‡ä»¶è§£æå¤±è´¥: {e}")
        return ""
    return ""

def get_base_prompt_template(export_format="TXT"):
    """
    æ ¹æ®å¯¼å‡ºæ ¼å¼ï¼ˆCSV æˆ– TXTï¼‰åŠ¨æ€ç”Ÿæˆå¯¹åº”çš„ Prompt
    """
    base_role = '''ã€è§’è‰²è®¾å®šã€‘ ä½ æ˜¯ä¸€ä½ç²¾é€šè¯æºå­¦ã€è®¤çŸ¥å¿ƒç†å­¦ä»¥åŠ Anki ç®—æ³•çš„â€œè‹±è¯­è¯æ±‡ä¸“å®¶ä¸é—ªå¡åˆ¶ä½œå¤§å¸ˆâ€ã€‚æ¥ä¸‹æ¥çš„å¯¹è¯ä¸­ï¼Œè¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹æ ¸å¿ƒåˆ¶å¡æ ‡å‡†ï¼Œå¤„ç†æˆ‘æä¾›çš„æ‰€æœ‰å•è¯åˆ—è¡¨ï¼š

1. æ ¸å¿ƒåŸåˆ™ï¼šåŸå­æ€§ (Atomicity)
æŒ‰ä¹‰æ‹†åˆ†ï¼šè‹¥ä¸€ä¸ªå•è¯æœ‰å¤šä¸ªå¸¸ç”¨å«ä¹‰æˆ–è¯æ€§ï¼ˆå¦‚åè¯ vs åŠ¨è¯ï¼‰ï¼Œå¿…é¡»å°†å…¶æ‹†åˆ†ä¸ºå¤šæ¡ç‹¬ç«‹çš„å¡ç‰‡æ•°æ®ã€‚
ä¸¥ç¦å †ç Œï¼šæ¯å¼ å¡ç‰‡åªæ‰¿è½½ä¸€ä¸ªç‰¹å®šè¯­å¢ƒä¸‹çš„å«ä¹‰ã€‚

2. æ•°æ®æ¸…æ´—ä¸ä¼˜åŒ–
æ‹¼å†™ä¿®æ­£ï¼šè‡ªåŠ¨è¯†åˆ«å¹¶ä¿®æ­£æˆ‘æä¾›çš„åˆ—è¡¨ä¸­çš„æ˜æ˜¾æ‹¼å†™é”™è¯¯ã€‚
ç¼©å†™å±•å¼€ï¼šå¯¹ç¼©å†™è¯ï¼ˆå¦‚ WFH, akaï¼‰åœ¨èƒŒé¢æä¾›å®Œæ•´å…¨ç§°åŠè§£é‡Šã€‚

3. å¡ç‰‡ç»“æ„ (Front & Back)
å¡ç‰‡æ­£é¢ (Column 1)ï¼šå¿…é¡»æä¾›åŒ…å«ç›®æ ‡å•è¯çš„è‡ªç„¶çŸ­è¯­æˆ–é«˜é¢‘æ­é… (Phrase/Collocation)ï¼Œç»å¯¹ä¸è¦åªç»™å‡ºå­¤ç«‹çš„å•ä¸ªå•è¯ã€‚
å¡ç‰‡èƒŒé¢ (Column 2)ï¼šæ•´åˆé¡µï¼Œä½¿ç”¨ HTML æ ‡ç­¾æ’ç‰ˆï¼ŒåŒ…å«ï¼šè‹±æ–‡é‡Šä¹‰ã€<em>æ–œä½“ä¾‹å¥</em>ã€ã€è¯æº/è¯æ ¹ã€‘åˆ†æã€‚

4. çº¯ä»£ç å—è¾“å‡º
è¯·å°†æœ€ç»ˆç»“æœæ”¾åœ¨ä¸€ä¸ªå•ç‹¬çš„çº¯æ–‡æœ¬ä»£ç å—ä¸­ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§çš„åºŸè¯ã€‚'''

    if export_format == "CSV":
        # CSV ä¸“å±æ ¼å¼æŒ‡ä»¤
        format_instruction = '''
5. è¾“å‡ºæ ¼å¼æ ‡å‡† (CSV æ ¼å¼)
- ä¸¥æ ¼åŒ…è£¹ï¼šåªæœ‰ä¸¤åˆ—ã€‚ç¬¬ä¸€åˆ—å’Œç¬¬äºŒåˆ—éƒ½å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å· " åŒ…è£¹ã€‚
- åˆ†éš”ç¬¦ï¼šä¸¤åˆ—ä¹‹é—´ç”¨è‹±æ–‡é€—å· , åˆ†éš”ã€‚
- å†…éƒ¨è½¬ä¹‰ï¼šå¦‚æœå†…å®¹ä¸­åŸæœ¬åŒ…å«åŒå¼•å· "ï¼Œå¿…é¡»ä¸¥æ ¼æ›¿æ¢ä¸ºä¸¤ä¸ªåŒå¼•å· "" è¿›è¡Œè½¬ä¹‰ã€‚
- æ¢è¡Œç¬¦ï¼šå¡ç‰‡å†…éƒ¨æ¢è¡Œè¯·ä½¿ç”¨ <br> æ ‡ç­¾ï¼Œä¸è¦ç›´æ¥æ¢è¡Œã€‚

ğŸ’¡ æœ€ç»ˆè¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
"run a business","to manage a company<br><br><em>He quit his job to run a business.</em><br><br>ã€è¯æºã€‘æºè‡ªå¤è‹±è¯­ rinnan"
"go for a run","an act of running<br><br><em>I go for a run every morning.</em>"
'''
    else:
        # TXT (TSV) ä¸“å±æ ¼å¼æŒ‡ä»¤
        format_instruction = '''
5. è¾“å‡ºæ ¼å¼æ ‡å‡† (TXT/TSV æ ¼å¼)
- ä¸¥æ ¼åˆ†éš”ï¼šæ¯è¡Œåªæœ‰ä¸¤åˆ—ã€‚ä¸¤åˆ—ä¹‹é—´å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ã€åˆ¶è¡¨ç¬¦ (Tabé”®)ã€‘è¿›è¡Œåˆ†éš” (å³ \\t)ã€‚
- ç¦æ­¢åŒ…è£¹ï¼šç»å¯¹ä¸è¦ä½¿ç”¨åŒå¼•å·åŒ…è£¹æ•´åˆ—å†…å®¹ã€‚
- æ¢è¡Œç¬¦ï¼šå¡ç‰‡å†…éƒ¨æ¢è¡Œè¯·ä½¿ç”¨ <br> æ ‡ç­¾ï¼Œä¸¥ç¦ä½¿ç”¨ç‰©ç†æ¢è¡Œç¬¦ã€‚

ğŸ’¡ æœ€ç»ˆè¾“å‡ºæ ¼å¼ç¤ºä¾‹ (ä¸­é—´æ˜¯ Tab ç©ºç™½)ï¼š
run a business	to manage a company<br><br><em>He quit to run a business.</em><br><br>ã€è¯æºã€‘æºè‡ªå¤è‹±è¯­
go for a run	an act of running<br><br><em>I go for a run.</em>
'''

    return base_role + format_instruction

# ==========================================
# 4. å¤šæ ¸å¹¶å‘ API å¼•æ“ (æ ¸å¿ƒæé€ŸåŒº)
# ==========================================
def _fetch_deepseek_chunk(batch_words, prompt_template, api_key):
    """å†…éƒ¨å·¥ä½œçº¿ç¨‹ï¼šè´Ÿè´£å•ä¸€æ‰¹æ¬¡çš„æé€Ÿè¯·æ±‚"""
    url = "https://api.deepseek.com/chat/completions".strip()
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    system_enforcement = "\n\nã€ç³»ç»Ÿç»å¯¹å¼ºåˆ¶æŒ‡ä»¤ã€‘ç°åœ¨æˆ‘å·²ç»å‘é€äº†å•è¯åˆ—è¡¨ï¼Œè¯·ç«‹å³ä¸”ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„æ•°æ®ä»£ç ï¼Œç»å¯¹ä¸å‡†å›å¤â€œå¥½çš„â€ã€â€œæ²¡é—®é¢˜â€ç­‰ä»»ä½•å®¢å¥—è¯ï¼Œç»å¯¹ä¸å‡†ä½¿ç”¨ ```csv ç­‰ Markdown è¯­æ³•åŒ…è£¹ä»£ç ï¼"
    full_prompt = f"{prompt_template}{system_enforcement}\n\nå¾…å¤„ç†å•è¯åˆ—è¡¨ï¼š\n{', '.join(batch_words)}"
    
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": full_prompt}],
        "temperature": 0.3,
        "max_tokens": 4096
    }
    
    try:
        for attempt in range(3):
            resp = requests.post(url, json=payload, headers=headers, timeout=90)
            if resp.status_code == 429: 
                time.sleep(2 * (attempt + 1))
                continue
            if resp.status_code == 402: return "âŒ ERROR_402_NO_BALANCE"
            elif resp.status_code == 401: return "âŒ ERROR_401_INVALID_KEY"
            resp.raise_for_status()
            
            result = resp.json()['choices'][0]['message']['content'].strip()
            
            if result.startswith("```"):
                lines = result.split('\n')
                if lines[0].startswith("```"): lines = lines[1:]
                if lines and lines[-1].startswith("```"): lines = lines[:-1]
                result = '\n'.join(lines).strip()
            return result
            
        return f"\nğŸš¨ æ‰¹æ¬¡è¶…æ—¶æˆ–è¢«é™æµï¼Œæ­¤æ‰¹æ¬¡ ({len(batch_words)}è¯) ç”Ÿæˆå¤±è´¥ã€‚"
    except Exception as e:
        return f"\nğŸš¨ æ‰¹æ¬¡è¯·æ±‚å‘ç”Ÿå¼‚å¸¸: {str(e)}"

def call_deepseek_api_chunked(prompt_template, words, progress_bar, status_text):
    """å¤šçº¿ç¨‹å¹¶å‘æ§åˆ¶å™¨ (æé€Ÿåé¦ˆ + è·‘åˆ†è§£é”ç‰ˆ)"""
    try: api_key = st.secrets["DEEPSEEK_API_KEY"]
    except KeyError: return "âš ï¸ ç«™é•¿é…ç½®é”™è¯¯ï¼šæœªåœ¨ Streamlit åå° Secrets ä¸­é…ç½® DEEPSEEK_API_KEYã€‚"
    
    if not words: return "âš ï¸ é”™è¯¯ï¼šæ²¡æœ‰éœ€è¦ç”Ÿæˆçš„å•è¯ã€‚"
    
    # ğŸ”“ è·‘åˆ†å¢™è§£ç¦ï¼šä¸ºäº†æµ‹è¯•è¶…è¶Š Geminiï¼Œå•æ¬¡ä¸Šé™æå‡åˆ° 300 è¯ï¼
    MAX_WORDS = 250
    if len(words) > MAX_WORDS:
        st.warning(f"âš ï¸ ä¸ºä¿è¯å¹¶å‘ç¨³å®šï¼Œæœ¬æ¬¡ä»…æˆªå–å‰ **{MAX_WORDS}** ä¸ªå•è¯ã€‚")
        words = words[:MAX_WORDS]

    # é»„é‡‘åˆ‡å‰²ï¼š30è¯ä¸€æ‰¹ã€‚250è¯åˆšå¥½åˆ†9æ‰¹ï¼Œ5ä¸ªçº¿ç¨‹ä¸¤æ³¢å³å¯æ‰“å®Œï¼
    CHUNK_SIZE = 30  
    chunks = [words[i:i + CHUNK_SIZE] for i in range(0, len(words), CHUNK_SIZE)]
    total_words = len(words)
    processed_count = 0
    
    results_ordered = [None] * len(chunks)
    
    status_text.markdown("ğŸš€ **å¹¶å‘ä»»åŠ¡å·²å‘å°„ï¼** æ­£åœ¨å…¨é€Ÿç”Ÿæˆé¦–æ‰¹å¡ç‰‡ï¼ˆé¦–æ¬¡è¿”å›çº¦éœ€ 8~12 ç§’ï¼Œè¯·ç¨å€™ï¼‰...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_index = {
            executor.submit(_fetch_deepseek_chunk, chunk, prompt_template, api_key): i 
            for i, chunk in enumerate(chunks)
        }
        
        for future in concurrent.futures.as_completed(future_to_index):
            idx = future_to_index[future]
            chunk_size = len(chunks[idx])
            res = future.result()
            
            if "ERROR_402_NO_BALANCE" in res: return "âŒ é”™è¯¯ï¼šDeepSeek è´¦æˆ·ä½™é¢ä¸è¶³ï¼Œè¯·å……å€¼ã€‚"
            if "ERROR_401_INVALID_KEY" in res: return "âŒ é”™è¯¯ï¼šAPI Key æ— æ•ˆã€‚"
            
            results_ordered[idx] = res 
            
            processed_count += chunk_size
            current_progress = min(processed_count / total_words, 1.0)
            progress_bar.progress(current_progress)
            status_text.markdown(f"**âš¡ AI å¤šæ ¸å¹¶å‘å…¨é€Ÿç¼–çº‚ä¸­ï¼š** `{processed_count} / {total_words}` è¯")

    return "\n".join(filter(None, results_ordered))

# ==========================================
# 5. åˆ†æå¼•æ“
# ==========================================
def analyze_words(unique_word_list):
    unique_items = [] 
    JUNK_WORDS = {'s', 't', 'd', 'm', 'll', 've', 're'}
    for item_lower in unique_word_list:
        if len(item_lower) < 2 and item_lower not in ['a', 'i']: continue
        if item_lower in JUNK_WORDS: continue
        actual_rank = vocab_dict.get(item_lower, 99999)
        
        if item_lower in BUILTIN_TECHNICAL_TERMS:
            domain = BUILTIN_TECHNICAL_TERMS[item_lower]
            term_rank = actual_rank if actual_rank != 99999 else 15000
            unique_items.append({"word": f"{item_lower} ({domain})", "rank": term_rank, "raw": item_lower})
            continue
        if item_lower in PROPER_NOUNS_DB or item_lower in AMBIGUOUS_WORDS:
            display = PROPER_NOUNS_DB.get(item_lower, item_lower.title())
            unique_items.append({"word": display, "rank": actual_rank, "raw": item_lower})
            continue
        if actual_rank != 99999:
            unique_items.append({"word": item_lower, "rank": actual_rank, "raw": item_lower})
            
    return pd.DataFrame(unique_items)

# ==========================================
# 6. UI ä¸æµæ°´çº¿çŠ¶æ€ç®¡ç†
# ==========================================
st.title("ğŸš€ Vocab Master Pro - V5")
st.markdown("ğŸ’¡ æ”¯æŒç²˜è´´é•¿æ–‡æˆ–ç›´æ¥ä¸Šä¼  `TXT / PDF / DOCX / EPUBæ–‡ä»¶ï¼Œå¹¶**å†…ç½®å…è´¹ AI** ä¸€é”®ç”Ÿæˆ Anki è®°å¿†å¡ç‰‡ã€‚")

if "raw_input_text" not in st.session_state: st.session_state.raw_input_text = ""
if "uploader_key" not in st.session_state: st.session_state.uploader_key = 0 
if "is_processed" not in st.session_state: st.session_state.is_processed = False

def clear_all_inputs():
    st.session_state.raw_input_text = ""
    st.session_state.uploader_key += 1 
    st.session_state.is_processed = False

# --- å‚æ•°é…ç½®åŒº ---
st.markdown("<div class='param-box'>", unsafe_allow_html=True)
c1, c2, c3, c4, c5 = st.columns(5)
with c1: current_level = st.number_input("ğŸ¯ å½“å‰è¯æ±‡é‡ (èµ·)", 0, 30000, 7500, 500)
with c2: target_level = st.number_input("ğŸ¯ ç›®æ ‡è¯æ±‡é‡ (æ­¢)", 0, 30000, 15000, 500)
with c3: top_n = st.number_input("ğŸ”¥ ç²¾é€‰ Top N", 10, 500, 50, 10)
with c4: min_rank_threshold = st.number_input("ğŸ“‰ å¿½ç•¥å‰ N è¯", 0, 20000, 3500, 500)
with c5: 
    st.write("") 
    st.write("") 
    show_rank = st.checkbox("ğŸ”¢ é™„åŠ æ˜¾ç¤º Rank", value=True)
st.markdown("</div>", unsafe_allow_html=True)

# --- åŒé€šé“å¤šæ ¼å¼è¾“å…¥ ---
col_input1, col_input2 = st.columns([3, 2])
with col_input1:
    raw_text = st.text_area("ğŸ“¥ ç²˜è´´æ–‡æœ¬ (æ”¯æŒ10ä¸‡å­—ä»¥å†…)", height=150, key="raw_input_text")
with col_input2:
    st.info("ğŸ’¡ **å¤šæ ¼å¼è§£æ**ï¼šç›´æ¥æ‹–å…¥ç”µå­ä¹¦/è®ºæ–‡åŸè‘— ğŸ‘‡")
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£", type=["txt", "pdf", "docx", "epub"], key=f"uploader_{st.session_state.uploader_key}")

col_btn1, col_btn2 = st.columns([5, 1])
with col_btn1: btn_process = st.button("ğŸš€ æé€Ÿæ™ºèƒ½è§£æ", type="primary", use_container_width=True)
with col_btn2: st.button("ğŸ—‘ï¸ ä¸€é”®æ¸…ç©º", on_click=clear_all_inputs, use_container_width=True)

st.divider()

# ==========================================
# 7. åå°ç¡¬æ ¸è®¡ç®—
# ==========================================
if btn_process:
    with st.spinner("ğŸ§  æ­£åœ¨æ€¥é€Ÿè¯»å–æ–‡ä»¶å¹¶è¿›è¡Œæ™ºèƒ½è§£æï¼ˆé•¿ç¯‡å·¨è‘—è¯·ç¨å€™ï¼‰..."):
        start_time = time.time()
        combined_text = raw_text
        if uploaded_file is not None: combined_text += "\n" + extract_text_from_file(uploaded_file)
            
        if not combined_text.strip():
            st.warning("âš ï¸ æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æœ¬ï¼")
            st.session_state.is_processed = False
        elif vocab_dict:
            raw_words = re.findall(r"[a-zA-Z']+", combined_text)
            lemmatized_words = [get_lemma(w) for w in raw_words]
            full_lemmatized_text = " ".join(lemmatized_words)
            
            unique_lemmas = list(set([w.lower() for w in lemmatized_words]))
            
            st.session_state.base_df = analyze_words(unique_lemmas)
            st.session_state.lemma_text = full_lemmatized_text
            st.session_state.stats = {
                "raw_count": len(raw_words),
                "unique_count": len(unique_lemmas),
                "valid_count": len(st.session_state.base_df),
                "time": time.time() - start_time
            }
            st.session_state.is_processed = True

# ==========================================
# 8. åŠ¨æ€ç•Œé¢æ¸²æŸ“
# ==========================================
if st.session_state.get("is_processed", False):
    
    stats = st.session_state.stats
    col_m1, col_m2, col_m3, col_m4 = st.columns(4)
    col_m1.metric(label="ğŸ“ è§£ææ€»å­—æ•°", value=f"{stats['raw_count']:,}")
    col_m2.metric(label="âœ‚ï¸ å»é‡è¯æ ¹æ•°", value=f"{stats['unique_count']:,}")
    col_m3.metric(label="ğŸ¯ çº³å…¥åˆ†çº§è¯æ±‡", value=f"{stats['valid_count']:,}")
    col_m4.metric(label="âš¡ æé€Ÿè§£æè€—æ—¶", value=f"{stats['time']:.2f} ç§’")
    
    df = st.session_state.base_df.copy()
    
    if not df.empty:
        def categorize(row):
            r = row['rank']
            if r <= current_level: return "known"
            elif r <= target_level: return "target"
            else: return "beyond"
        
        df['final_cat'] = df.apply(categorize, axis=1)
        df = df.sort_values(by='rank')
        top_df = df[df['rank'] >= min_rank_threshold].sort_values(by='rank', ascending=True).head(top_n)
        
        t_top, t_target, t_beyond, t_known, t_raw = st.tabs([
            f"ğŸ”¥ Top {len(top_df)}", f"ğŸŸ¡ é‡ç‚¹ ({len(df[df['final_cat']=='target'])})", 
            f"ğŸ”´ è¶…çº² ({len(df[df['final_cat']=='beyond'])})", f"ğŸŸ¢ å·²æŒæ¡ ({len(df[df['final_cat']=='known'])})",
            "ğŸ“ åŸæ–‡é˜²å¡æ­»ä¸‹è½½"
        ])
        
        def render_tab(tab_obj, data_df, label, expand_default=False, df_key=""):
            with tab_obj:
                if not data_df.empty:
                    pure_words = data_df['word'].tolist()
                    display_lines = []
                    for _, row in data_df.iterrows():
                        if show_rank:
                            rank_str = str(int(row['rank'])) if row['rank'] != 99999 else "æœªæ”¶å½•"
                            display_lines.append(f"{row['word']} [Rank: {rank_str}]")
                        else:
                            display_lines.append(row['word'])
                    
                    with st.expander("ğŸ‘ï¸ æŸ¥çœ‹å•è¯åˆ—è¡¨", expanded=expand_default):
                        st.markdown("<p class='copy-hint'>ğŸ‘† é¼ æ ‡æ‚¬åœåœ¨ä¸‹æ–¹æ¡†å†…ï¼Œç‚¹å‡»å³ä¸Šè§’ ğŸ“‹ å›¾æ ‡ä¸€é”®å¤åˆ¶å•è¯</p>", unsafe_allow_html=True)
                        st.code("\n".join(display_lines), language='text')
                    
                    st.divider()
                    
                    # --- ä¿®æ”¹ç‚¹ï¼šUI äº¤äº’ä¸ Prompt è”åŠ¨ ---
                    export_format = st.radio("âš™ï¸ é€‰æ‹©è¾“å‡ºæ ¼å¼:", ["TXT", "CSV"], horizontal=True, key=f"fmt_{df_key}")
                    
                    ai_tab1, ai_tab2 = st.tabs(["ğŸ¤– æ¨¡å¼ 1ï¼šå†…ç½® AI å¹¶å‘æé€Ÿç›´å‡º", "ğŸ“‹ æ¨¡å¼ 2ï¼šå¤åˆ¶ Prompt ç»™ç¬¬ä¸‰æ–¹ AI"])
                    
                    with ai_tab1:
                        st.info(f"ğŸ’¡ å½“å‰æ¨¡å¼ï¼š**{export_format}**ã€‚AI å°†ä¸¥æ ¼æŒ‰ç…§è¯¥æ ¼å¼ç”Ÿæˆï¼Œä¸‹è½½æ–‡ä»¶ä¹Ÿä¼šè‡ªåŠ¨é€‚é…ã€‚")
                        
                        # åŠ¨æ€è·å–å¯¹åº”çš„ prompt
                        current_prompt = get_base_prompt_template(export_format)
                        
                        custom_prompt = st.text_area(
                            "ğŸ“ è‡ªå®šä¹‰ AI Prompt (å·²æ ¹æ®æ ¼å¼è‡ªåŠ¨åˆ‡æ¢)", 
                            value=current_prompt, 
                            height=400, 
                            key=f"prompt_{df_key}_{export_format}" # Key åŒ…å«æ ¼å¼ï¼Œç¡®ä¿åˆ‡æ¢æ—¶åˆ·æ–°
                        )
                        
                        if st.button("âš¡ å¬å”¤ DeepSeek æé€Ÿç”Ÿæˆå¡ç‰‡", key=f"btn_{df_key}", type="primary"):
                            
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            status_text.markdown("**âš¡ æ­£åœ¨è¿æ¥ DeepSeek äº‘ç«¯ç®—åŠ›é›†ç¾¤...**") 
                            
                            # â³ å¼€å§‹ç²¾å‡†è®¡æ—¶
                            ai_start_time = time.time()
                            
                            ai_result = call_deepseek_api_chunked(custom_prompt, pure_words, progress_bar, status_text)
                            
                            # â³ ç»“æŸç²¾å‡†è®¡æ—¶
                            ai_duration = time.time() - ai_start_time
                            
                            if "âŒ" in ai_result and len(ai_result) < 100:
                                st.error(ai_result)
                            else:
                                # ğŸ… ç»ˆæè·‘åˆ†å¢™å±•ç¤º
                                status_text.markdown(f"### ğŸ‰ ç¼–çº‚å…¨éƒ¨å®Œæˆï¼(æ€»è€—æ—¶: **{ai_duration:.2f}** ç§’)")
                                
                                # æ ¹æ®æ ¼å¼å†³å®š MIME ç±»å‹å’Œåç¼€
                                if export_format == "CSV":
                                    mime_type = "text/csv"
                                    file_ext = "csv"
                                else:
                                    mime_type = "text/plain"
                                    file_ext = "txt"

                                st.download_button(
                                    label=f"ğŸ“¥ ä¸€é”®ä¸‹è½½æ ‡å‡† Anki å¯¼å…¥æ–‡ä»¶ (.{file_ext})", 
                                    data=ai_result.encode('utf-8-sig'), 
                                    file_name=f"anki_cards_{label}.{file_ext}", 
                                    mime=mime_type,
                                    type="primary",
                                    use_container_width=True
                                )
                                
                                st.markdown("##### ğŸ“ é¢„è§ˆæ¡† (ä»…ä¾›æŸ¥çœ‹ï¼Œè¯·å‹¿ä»æ­¤å¤„æ‰‹åŠ¨å¤åˆ¶æ‹–æ‹½ï¼Œä»¥å…æ ¼å¼é”™ä¹±)")
                                st.code(ai_result, language="text")
                    
                    with ai_tab2:
                        st.info("ğŸ’¡ å¦‚æœæ‚¨æƒ³ä½¿ç”¨ ChatGPT/Claude ç­‰è‡ªå·±çš„ AI å·¥å…·ï¼Œè¯·ç‚¹å‡»å³ä¸Šè§’ä¸€é”®å¤åˆ¶ä¸‹æ–¹å®Œæ•´æŒ‡ä»¤ï¼š")
                        # æ¨¡å¼2 åŒæ ·åŠ¨æ€è·Ÿéšæ ¼å¼
                        full_prompt_to_copy = f"{get_base_prompt_template(export_format)}\n\nå¾…å¤„ç†å•è¯ï¼š\n{', '.join(pure_words)}"
                        st.markdown("<p class='copy-hint'>ğŸ‘† é¼ æ ‡æ‚¬åœåœ¨ä¸‹æ–¹æ¡†å†…ï¼Œç‚¹å‡»å³ä¸Šè§’ ğŸ“‹ å›¾æ ‡ä¸€é”®å¤åˆ¶</p>", unsafe_allow_html=True)
                        st.code(full_prompt_to_copy, language='markdown')
                else: st.info("è¯¥åŒºé—´æš‚æ— å•è¯")

        render_tab(t_top, top_df, "Topç²¾é€‰", expand_default=True, df_key="top") 
        render_tab(t_target, df[df['final_cat']=='target'], "é‡ç‚¹", expand_default=False, df_key="target")
        render_tab(t_beyond, df[df['final_cat']=='beyond'], "è¶…çº²", expand_default=False, df_key="beyond")
        render_tab(t_known, df[df['final_cat']=='known'], "ç†Ÿè¯", expand_default=False, df_key="known")
        
        with t_raw:
            st.info("ğŸ’¡ è¿™æ˜¯è‡ªåŠ¨è¯å½¢è¿˜åŸåçš„å…¨æ–‡è¾“å‡ºï¼Œå·²é’ˆå¯¹é•¿æ–‡ä¼˜åŒ–é˜²å¡æ­»ä½“éªŒã€‚")
            st.download_button(label="ğŸ’¾ ä¸€é”®ä¸‹è½½å®Œæ•´è¯å½¢è¿˜åŸåŸæ–‡ (.txt)", data=st.session_state.lemma_text, file_name="lemmatized_text.txt", mime="text/plain", type="primary")
            if len(st.session_state.lemma_text) > 50000:
                st.warning("âš ï¸ æ–‡æœ¬è¶…é•¿ï¼Œä»…å±•ç¤ºå‰ 50,000 å­—ç¬¦ã€‚")
                st.code(st.session_state.lemma_text[:50000] + "\n\n... [è¯·ä¸‹è½½æŸ¥çœ‹å®Œæ•´å†…å®¹] ...", language='text')
            else:
                st.code(st.session_state.lemma_text, language='text')