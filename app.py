import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import io

# ==========================================
# 0. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(
    page_title="Prompt Gen", 
    page_icon="ğŸ“±", 
    layout="centered", 
    initial_sidebar_state="collapsed"
)

st.markdown("""
<style>
    .block-container { padding-top: 1rem; padding-bottom: 3rem; }
    #MainMenu {visibility: hidden;} footer {visibility: hidden;} header {visibility: hidden;}
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    .stButton>button {
        width: 100%; border-radius: 12px; height: 3.5em; font-weight: bold; font-size: 18px !important;
        margin-top: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    .stTextArea>div>div>textarea { font-size: 16px !important; border-radius: 10px; }
    .stNumberInput input { font-size: 18px !important; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºåŠ è½½
# ==========================================
@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    try: 
        nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_dir, quiet=True)
        nltk.download('punkt', download_dir=nltk_data_dir, quiet=True)
    except: pass
setup_nltk()

@st.cache_data
def load_data():
    """
    ä¿®å¤ç‰ˆåŠ è½½é€»è¾‘ï¼š
    1. ä¸å†å¯¹å•è¯è¿›è¡Œå»é‡ (drop_duplicates)ã€‚
    2. å…è®¸åŒä¸€ä¸ªå•è¯å‡ºç°åœ¨ä¸åŒçš„æ’åï¼ˆè§£å†³ä¸€è¯å¤šä¹‰å¯¼è‡´çš„æ¼è¯é—®é¢˜ï¼‰ã€‚
    """
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    
    vocab_dict = {} # Word -> Rank (ä¾›æ–‡æœ¬æå–æ¨¡å¼ç”¨ï¼Œé»˜è®¤ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„Rank)
    rank_map = {}   # Rank -> [Words] (ä¾›åˆ·è¯æ¨¡å¼ç”¨ï¼Œä¿ç•™æ‰€æœ‰)

    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            
            w_col = next((c for c in cols if 'word' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c), cols[1])
            
            # åŸºç¡€æ¸…æ´—ï¼šå»ç©ºã€å°å†™
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            df = df.dropna(subset=[r_col])
            
            # ã€å…³é”®ä¿®æ”¹ã€‘ä¸å†æ‰§è¡Œ drop_duplicates(subset=[w_col])
            # æˆ‘ä»¬ä¿ç•™æ‰€æœ‰è¡Œï¼Œç¡®ä¿ Rank 8000 çš„ splash ä¹Ÿèƒ½è¢«ç´¢å¼•åˆ°
            
            # æ„å»º Rank -> Word åˆ—è¡¨ (ä¸€å¯¹å¤š)
            for index, row in df.iterrows():
                r = int(row[r_col])
                w = row[w_col]
                if r not in rank_map:
                    rank_map[r] = []
                rank_map[r].append(w)
            
            # æ„å»º Word -> Rank (æ–‡æœ¬æ¨¡å¼ç”¨)
            # è¿™é‡Œå¦‚æœä¸ºäº†ä¸¥è°¨ï¼Œæˆ‘ä»¬å€’åºéå†ï¼Œä¿ç•™æ’åé å‰çš„é‚£ä¸ª
            # æˆ–è€…ç›´æ¥ç”± pandas é»˜è®¤å¤„ç†
            df_unique = df.sort_values(r_col).drop_duplicates(subset=[w_col])
            vocab_dict = pd.Series(df_unique[r_col].values, index=df_unique[w_col]).to_dict()

        except Exception as e:
            st.error(f"æ•°æ®åŠ è½½å‡ºé”™: {e}")
            
    return vocab_dict, rank_map

VOCAB_DICT, RANK_MAP = load_data()

def get_lemma(word): return lemminflect.getLemma(word, upos='VERB')[0] 

# ==========================================
# 2. Prompt ç”Ÿæˆé€»è¾‘
# ==========================================
def generate_strict_prompt(words):
    word_list_str = ", ".join(words)
    prompt = f"""Role: High-Efficiency Anki Card Creator
Task: Convert the provided word list into a strict CSV data block.

--- OUTPUT FORMAT RULES ---
1. Structure: 2 Columns only. Comma-separated. All fields double-quoted.
   Format: "Front","Back"
   Header: **Do NOT output a header row.** Only output the data rows.

2. Column 1 (Front):
   - Content: A natural, short English phrase or collocation containing the target word.
   - Style: Plain text.

3. Column 2 (Back):
   - Content: Definition + Example + Etymology.
   - HTML Layout: Definition <br> <br> <em>Example Sentence</em> <br> <br> ã€æºã€‘Etymology
   - Constraints: 
     - Use double <br> tags ( <br> <br> ) between sections to ensure clear visual spacing.
     - Example sentence must be wrapped in <em> tags.

4. Atomicity Principle (Strict):
   - If a word has distinct meanings, **generate SEPARATE rows**.

5. Output: 
   - Code Block ONLY. 
   - NO header line.

--- WORD LIST ---
{word_list_str}
"""
    return prompt

# ==========================================
# 3. è¾…åŠ©åŠŸèƒ½
# ==========================================
def extract_text_from_file(uploaded_file):
    try:
        ext = uploaded_file.name.split('.')[-1].lower()
        if ext == 'txt': return uploaded_file.getvalue().decode("utf-8", errors="ignore")
        elif ext == 'pdf':
            import PyPDF2; reader = PyPDF2.PdfReader(uploaded_file)
            return " ".join([p.extract_text() for p in reader.pages if p.extract_text()])
        elif ext == 'docx':
            import docx; doc = docx.Document(uploaded_file)
            return " ".join([p.text for p in doc.paragraphs])
    except: return ""
    return ""

def process_text_input(text, min_rank, max_rank):
    words = re.findall(r"[a-zA-Z']+", text)
    lemmas = set(get_lemma(w).lower() for w in words if len(w)>=2)
    filtered = [(w, VOCAB_DICT.get(w, 99999)) for w in lemmas]
    filtered = [w for w, r in filtered if min_rank <= r <= max_rank]
    filtered.sort(key=lambda w: VOCAB_DICT.get(w, 99999))
    return filtered

# ==========================================
# 4. ä¸»ç•Œé¢
# ==========================================
st.title("âš¡ï¸ Anki Master")

if not RANK_MAP:
    st.error("âš ï¸ ç¼ºå°‘è¯é¢‘æ–‡ä»¶æˆ–åŠ è½½å¤±è´¥")
else:
    mode = st.radio("åŠŸèƒ½", ["ğŸ”¢ åˆ·è¯", "ğŸ“– æå–", "ğŸ› ï¸ è½¬æ¢"], horizontal=True, label_visibility="collapsed")
    
    # ------------------------------------------------
    # æ¨¡å¼ 1: åˆ·è¯ (çº¯å‡€ç‰ˆ)
    # ------------------------------------------------
    if mode == "ğŸ”¢ åˆ·è¯":
        st.caption("ä»æŒ‡å®šæ’åæå– (ä¿ç•™é‡å¤è¯)")
        
        col1, col2 = st.columns(2)
        with col1:
            start_rank = st.number_input("èµ·å§‹æ’å", value=8000, step=50)
        with col2:
            end_rank = st.number_input("ç»“æŸæ’å", value=8050, step=50)
            
        if start_rank >= end_rank:
            st.warning("èŒƒå›´é”™è¯¯")
        else:
            target_words = []
            # ç®€å•éå†èŒƒå›´ï¼Œç›´æ¥å– map é‡Œçš„å€¼
            for r in range(start_rank, end_rank + 1):
                if r in RANK_MAP:
                    target_words.extend(RANK_MAP[r])
            
            # ä¸å†è¿›è¡Œå»é‡ (dict.fromkeys)ï¼Œä¿ç•™æ‰€æœ‰æå–åˆ°çš„è¯
            
            if target_words:
                st.info(f"âœ… åŒºé—´ {start_rank}-{end_rank} æå–åˆ° **{len(target_words)}** ä¸ªå•è¯")
                
                # é¢„è§ˆ
                with st.expander("æŸ¥çœ‹å•è¯åˆ—è¡¨"):
                    st.text(", ".join(target_words))

                if st.button("ğŸš€ ç”Ÿæˆ Prompt"):
                    prompt = generate_strict_prompt(target_words)
                    st.code(prompt, language="markdown")
                    st.success("è¯·å¤åˆ¶ä¸Šæ–¹ä»£ç  -> å‘é€ç»™ ChatGPT")
            else:
                st.warning("è¯¥åŒºé—´æ²¡æœ‰å•è¯ã€‚")

    # ------------------------------------------------
    # æ¨¡å¼ 2: æå–
    # ------------------------------------------------
    elif mode == "ğŸ“– æå–":
        inp = st.radio("æ–¹å¼", ["ç²˜è´´", "ä¸Šä¼ "], horizontal=True, label_visibility="collapsed")
        txt = ""
        target_words = []
        
        if inp == "ç²˜è´´": txt = st.text_area("æ–‡æœ¬", height=100)
        else: 
            up = st.file_uploader("æ–‡ä»¶", type=["txt","pdf","docx"])
            if up: txt = extract_text_from_file(up)
        
        if txt and st.button("æå–"):
            target_words = process_text_input(txt, 3000, 20000)
            st.session_state['temp_ext'] = target_words
        
        if 'temp_ext' in st.session_state: target_words = st.session_state['temp_ext']

        if target_words:
            if len(target_words)>100: 
                target_words=target_words[:100]
                st.warning("å·²æˆªå–å‰ 100 ä¸ª")
            
            st.info(f"æå–åˆ° {len(target_words)} ä¸ªç”Ÿè¯")
            if st.button("ğŸš€ ç”Ÿæˆ Prompt"):
                prompt = generate_strict_prompt(target_words)
                st.code(prompt, language="markdown")

    # ------------------------------------------------
    # æ¨¡å¼ 3: è½¬æ¢
    # ------------------------------------------------
    elif mode == "ğŸ› ï¸ è½¬æ¢":
        st.markdown("### ğŸ“¥ AI ç»“æœè½¬ Anki æ–‡ä»¶")
        st.caption("ç²˜è´´ ChatGPT çš„ä»£ç å—ï¼Œè‡ªåŠ¨ç”Ÿæˆæ ‡å‡† CSV")
        
        csv_input = st.text_area("ç²˜è´´å†…å®¹", height=200, placeholder='"phrase","def..."')
        
        if csv_input:
            csv_content = csv_input.strip()
            # è‡ªåŠ¨è¡¥å…¨ Header
            if '"Front","Back"' not in csv_content and "Front,Back" not in csv_content:
                final_csv = '"Front","Back"\n' + csv_content
            else:
                final_csv = csv_content
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½ .csv (è‡ªåŠ¨è¡¥å…¨æ ¼å¼)",
                data=final_csv.encode('utf-8'),
                file_name="anki_import.csv",
                mime="text/csv",
                type="primary"
            )
            st.success("ä¸‹è½½å -> åˆ†äº«åˆ° Anki -> ç›´æ¥ Import")