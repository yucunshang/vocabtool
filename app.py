import streamlit as st
import pandas as pd
import re
import os
import json
import time
import requests
import concurrent.futures
import lemminflect
import nltk
import tempfile
import random
from pathlib import Path

# ==========================================
# 0. ä¾èµ–æ£€æŸ¥ä¸ NLTK åˆå§‹åŒ–
# ==========================================
try:
    import PyPDF2
    import docx
    import genanki
except ImportError:
    st.error("âš ï¸ ç¼ºå°‘å¿…è¦ä¾èµ–ã€‚è¯·åœ¨ç»ˆç«¯è¿è¡Œ: pip install PyPDF2 python-docx genanki")
    st.stop()

def download_nltk_resources():
    """é™é»˜ä¸‹è½½å¿…è¦çš„ NLTK æ•°æ®"""
    resources = ['punkt', 'averaged_perceptron_tagger', 'names', 'wordnet', 'omw-1.4']
    for r in resources:
        try:
            nltk.data.find(f'tokenizers/{r}') if r == 'punkt' else nltk.data.find(f'corpora/{r}')
        except LookupError:
            nltk.download(r, quiet=True)

download_nltk_resources()

# ==========================================
# 1. é¡µé¢é…ç½® & æ ·å¼ä¼˜åŒ– (æ— ä¾§è¾¹æ )
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€", initial_sidebar_state="collapsed")

st.markdown("""
<style>
    /* éšè—é¡¶éƒ¨ Hamburger èœå•å’Œé»˜è®¤ Footer */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    
    /* å¼ºåˆ¶éšè—ä¾§è¾¹æ  (é˜²æ­¢è¯¯è§¦) */
    [data-testid="stSidebar"] { display: none; }
    
    /* å­—ä½“ä¸æ’ç‰ˆä¼˜åŒ– */
    .stCode { font-family: 'Consolas', 'Courier New', monospace !important; }
    .block-container { padding-top: 2rem; padding-bottom: 5rem; max-width: 1200px; }
    
    /* ç§»åŠ¨ç«¯é€‚é…ï¼šè°ƒæ•´æŒ‡æ ‡æ•°å­—å¤§å° */
    @media (max-width: 640px) {
        [data-testid="stMetricValue"] { font-size: 22px !important; }
        .stButton button { width: 100%; border-radius: 8px; }
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. API Key è·å– (ä½¿ç”¨ st.secrets)
# ==========================================
def get_api_key():
    """ä¼˜å…ˆä» st.secrets è·å–ï¼Œå®‰å…¨ä¸”éšè”½"""
    try:
        return st.secrets["DEEPSEEK_API_KEY"]
    except (FileNotFoundError, KeyError):
        return None

API_KEY = get_api_key()

# ==========================================
# 3. æ•°æ®åŠ è½½ (JSON æ”¯æŒ)
# ==========================================
@st.cache_data
def load_data_resources():
    data_dir = Path("data")
    
    # 1. Safe Names (åŸºç¡€ + æ‰©å±•)
    safe_names = set(nltk.corpus.names.words())
    if (data_dir / "safe_names.json").exists():
        with open(data_dir / "safe_names.json", "r", encoding="utf-8") as f:
            safe_names.update(json.load(f))
            
    # 2. Terms & Proper Nouns
    tech_terms = json.load(open(data_dir / "terms.json", encoding="utf-8")) if (data_dir / "terms.json").exists() else {}
    proper_map = json.load(open(data_dir / "proper.json", encoding="utf-8")) if (data_dir / "proper.json").exists() else {}
    
    # 3. Global Ranks (COCA)
    entity_ranks = {}
    if (data_dir / "global_ranks.json").exists():
        with open(data_dir / "global_ranks.json", "r", encoding="utf-8") as f:
            entity_ranks = json.load(f)
    else:
        # Fallback é˜²æ­¢æŠ¥é”™
        entity_ranks = {"the": 1, "be": 2, "python": 500, "code": 600} 
            
    return safe_names, tech_terms, proper_map, entity_ranks

SAFE_NAMES, TECH_TERMS, PROPER_MAP, GLOBAL_RANKS = load_data_resources()

# ==========================================
# 4. æ ¸å¿ƒ NLP å¤„ç†é€»è¾‘
# ==========================================
def clean_text(text):
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def get_word_info(word):
    w_lower = word.lower()
    # æœ¯è¯­åº“ä¼˜å…ˆ
    if w_lower in TECH_TERMS: return w_lower, 0, 'Tech', TECH_TERMS[w_lower]
    
    # è¯å½¢è¿˜åŸ
    lemma = lemminflect.getLemma(w_lower, upos="VERB")[0]
    if lemma not in GLOBAL_RANKS:
        lemma = lemminflect.getLemma(w_lower, upos="NOUN")[0]
        
    rank = GLOBAL_RANKS.get(lemma, 99999)
    display = PROPER_MAP.get(lemma, lemma) # ä¿®æ­£å¤§å°å†™
    return display, rank, 'General', ''

def process_text(text, min_rank, top_n):
    tokens = nltk.word_tokenize(text)
    # è¿‡æ»¤é€»è¾‘ï¼šçº¯å­—æ¯ï¼Œé•¿åº¦>2ï¼Œéäººå
    valid_words = [w for w in tokens if w.isalpha() and len(w)>2 and w not in SAFE_NAMES]
    
    data = []
    seen = set()
    for w in valid_words:
        lemma, rank, cat, tag = get_word_info(w)
        if lemma not in seen:
            seen.add(lemma)
            data.append({"raw": w, "lemma": lemma, "rank": rank, "category": cat})
            
    df = pd.DataFrame(data)
    if df.empty: return df
    
    # ç­›é€‰ä¸æ’åº
    df = df[df['rank'] > min_rank].sort_values('rank')
    
    # æ ‡è®° Top N
    df['is_top'] = False
    if not df.empty:
        df.iloc[:top_n, df.columns.get_loc('is_top')] = True
        
    return df

# ==========================================
# 5. AI & Anki ç”Ÿæˆé€»è¾‘
# ==========================================
def get_prompt(lang):
    """V2 Prompt: åŒ…å« One-Shot Example"""
    ex_out = '"book","<b>book</b> [n.]<br>ä¹¦ï¼Œä¹¦ç±<br><em>I read a book.</em>"' if lang == "Chinese" else '"book","<b>book</b> [n.]<br>A written work...<br><em>I read a book.</em>"'
    return f"""Role: Expert Linguist.
Task: Create Anki cards.
Format: CSV "Front","Back"
Rules:
1. Front: The word.
2. Back: Definition in {lang} + 1 Example (wrapped in <em>).
3. Output ONLY CSV lines.

Example:
Input: book
Output:
{ex_out}

Words:
"""

def generate_anki_pkg(cards, deck_name="VocabMaster"):
    """ç”Ÿæˆ .apkg æ–‡ä»¶"""
    model_id = random.randrange(1 << 30, 1 << 31)
    deck_id = random.randrange(1 << 30, 1 << 31)
    
    my_model = genanki.Model(
        model_id, 'VocabMaster Model',
        fields=[{'name': 'Front'}, {'name': 'Back'}],
        templates=[{
            'name': 'Card 1',
            'qfmt': '<div style="text-align:center; font-size:28px; font-weight:bold; color:#333;">{{Front}}</div>',
            'afmt': '{{FrontSide}}<hr id="answer"><div style="text-align:left; font-size:18px; line-height:1.6;">{{Back}}</div>',
        }]
    )
    my_deck = genanki.Deck(deck_id, deck_name)
    for f, b in cards:
        my_deck.add_note(genanki.Note(model=my_model, fields=[f, b]))
        
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix='.apkg')
    genanki.Package(my_deck).write_to_file(tmp.name)
    return tmp.name

def call_ai_batch(prompt):
    """è°ƒç”¨ AI æ¥å£"""
    if not API_KEY: return "Error: API Key missing"
    
    headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": "deepseek-chat", # æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹æ¨¡å‹åç§°
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.3
    }
    try:
        r = requests.post("https://api.deepseek.com/chat/completions", json=payload, headers=headers, timeout=60)
        return r.json()['choices'][0]['message']['content']
    except Exception as e:
        return f"Error: {e}"

# ==========================================
# 6. UI ä¸»ç¨‹åº
# ==========================================
st.title("ğŸš€ Vocab Master Pro")

# --- å‚æ•°é…ç½®åŒº (æŠ˜å é¢æ¿) ---
with st.expander("âš™ï¸ ç­›é€‰å‚æ•°è®¾ç½® (Settings)", expanded=False):
    c1, c2, c3 = st.columns(3)
    with c1: min_rank = st.number_input("å¿½ç•¥å‰ N é«˜é¢‘è¯", 0, 20000, 3000, 500)
    with c2: top_n = st.number_input("ç²¾é€‰ Top N", 10, 500, 50, 10)
    with c3: 
        if not API_KEY:
            st.error("âŒ æœªæ£€æµ‹åˆ° Secrets Key")
        else:
            st.success("âœ… API Key å·²è¿æ¥")

# --- è¾“å…¥åŒº ---
raw_text = ""
tab1, tab2 = st.tabs(["ğŸ“ æ–‡æœ¬è¾“å…¥", "ğŸ“‚ æ–‡ä»¶ä¸Šä¼ "])
with tab1:
    txt_in = st.text_area("åœ¨æ­¤ç²˜è´´...", height=150)
    if txt_in: raw_text = txt_in
with tab2:
    up_file = st.file_uploader("æ”¯æŒ PDF, Docx, Txt", type=['pdf', 'docx', 'txt'])
    if up_file:
        if up_file.name.endswith(".pdf"):
            reader = PyPDF2.PdfReader(up_file)
            raw_text = " ".join([p.extract_text() for p in reader.pages])
        elif up_file.name.endswith(".docx"):
            doc = docx.Document(up_file)
            raw_text = " ".join([p.text for p in doc.paragraphs])
        else:
            raw_text = up_file.read().decode("utf-8")

# --- åˆ†ææŒ‰é’® ---
if st.button("ğŸš€ å¼€å§‹åˆ†æ (Analyze)", type="primary", use_container_width=True):
    if not raw_text:
        st.warning("è¯·å…ˆè¾“å…¥å†…å®¹")
    else:
        with st.spinner("NLP å¤„ç†ä¸­..."):
            st.session_state.df = process_text(clean_text(raw_text), min_rank, top_n)

# --- ç»“æœåŒº ---
if "df" in st.session_state and not st.session_state.df.empty:
    df = st.session_state.df
    top_df = df[df['is_top']].copy()
    
    st.divider()
    # æŒ‡æ ‡å±•ç¤º
    m1, m2, m3 = st.columns(3)
    m1.metric("æ€»è¯æ±‡", len(df))
    m2.metric("é‡ç‚¹è¯", len(top_df))
    m3.metric("éš¾åº¦ç³»æ•°", int(df['rank'].mean()) if not df.empty else 0)
    
    t_res1, t_res2 = st.tabs(["ğŸ”¥ é‡ç‚¹è¯ & åˆ¶å¡", "ğŸ“‹ å®Œæ•´åˆ—è¡¨"])
    
    with t_res1:
        st.dataframe(top_df[['lemma', 'rank', 'category']], use_container_width=True)
        
        st.markdown("### ğŸ¤– AI åˆ¶å¡ (Anki)")
        if not API_KEY:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ .streamlit/secrets.toml ä¸­é…ç½® DEEPSEEK_API_KEY æ‰èƒ½ä½¿ç”¨ AI åŠŸèƒ½")
        else:
            col_a, col_b = st.columns(2)
            with col_a: lang = st.selectbox("é‡Šä¹‰è¯­è¨€", ["Chinese", "English"])
            with col_b: fmt = st.selectbox("å¯¼å‡ºæ ¼å¼", ["Anki (.apkg)", "CSV"])
            
            # é¢„è§ˆæŒ‰é’®
            if st.button("ğŸ‘ï¸ é¢„è§ˆé¦–è¯ (Preview 1 Card)"):
                word = top_df.iloc[0]['lemma']
                p = get_prompt(lang) + word
                with st.spinner("ç”Ÿæˆé¢„è§ˆ..."):
                    st.code(call_ai_batch(p), language="csv")
            
            # æ‰¹é‡ç”ŸæˆæŒ‰é’®
            if st.button("âš¡ ç”Ÿæˆå…¨éƒ¨å¡ç‰‡ (Batch Generate)", type="primary"):
                words = top_df['lemma'].tolist()
                batches = [words[i:i+10] for i in range(0, len(words), 10)]
                all_cards = []
                
                prog_bar = st.progress(0)
                status_txt = st.empty()
                
                # å¹¶å‘å¤„ç†
                with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                    futures = {}
                    for i, batch in enumerate(batches):
                        p = get_prompt(lang) + "\n".join(batch)
                        futures[executor.submit(call_ai_batch, p)] = batch
                    
                    done_count = 0
                    for future in concurrent.futures.as_completed(futures):
                        res = future.result()
                        # ç®€å•çš„ CSV è§£æ
                        for line in res.strip().split('\n'):
                            parts = line.split('","')
                            if len(parts) >= 2:
                                all_cards.append((parts[0].strip('"'), parts[-1].strip('"')))
                        
                        done_count += 1
                        prog_bar.progress(done_count / len(batches))
                        status_txt.text(f"å·²å¤„ç†: {done_count}/{len(batches)} æ‰¹æ¬¡")
                
                st.success(f"ç”Ÿæˆå®Œæˆï¼å…± {len(all_cards)} å¼ å¡ç‰‡")
                
                # ä¸‹è½½é€»è¾‘
                if fmt == "Anki (.apkg)":
                    path = generate_anki_pkg(all_cards)
                    with open(path, "rb") as f:
                        st.download_button("ğŸ“¥ ä¸‹è½½ Anki ç‰Œç»„ (.apkg)", f, file_name="vocab.apkg", mime="application/apkg")
                else:
                    csv_data = "\n".join([f'"{f}","{b}"' for f,b in all_cards])
                    st.download_button("ğŸ“¥ ä¸‹è½½ CSV", csv_data, file_name="vocab.csv")

    with t_res2:
        st.dataframe(df, use_container_width=True)