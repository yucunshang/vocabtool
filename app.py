import streamlit as st
import pandas as pd
import re
import os
import simplemma

st.set_page_config(page_title="Vibe Vocab Studio", page_icon="ğŸ§ ", layout="wide")

# --- 1. è‡ªåŠ¨é€‚é… Simplemma ç‰ˆæœ¬ ---
try:
    test_res = simplemma.lemmatize("testing", lang="en")
    def get_lemma(word):
        return simplemma.lemmatize(word, lang="en")
except TypeError:
    if hasattr(simplemma, 'load_data'):
        lang_data = simplemma.load_data('en')
        def get_lemma(word):
            return simplemma.lemmatize(word, lang_data)
    else:
        def get_lemma(word):
            return word 

# --- 2. å¼ºåŠ›åŠ è½½è¯åº“ (å…³é”®ä¿®å¤) ---
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv", "COCA20000è¯Excelç‰ˆ.xlsx - Sheet1.csv"]

@st.cache_data
def load_vocab():
    file_path = None
    for f in POSSIBLE_FILES:
        if os.path.exists(f):
            file_path = f
            break
            
    if not file_path:
        return None

    try:
        # 1. å°è¯•è¯»å– (åŠ å¼ºç¼–ç å…¼å®¹æ€§)
        df = None
        for enc in ['utf-8', 'gbk', 'gb18030', 'utf-8-sig']:
            try:
                df = pd.read_csv(file_path, encoding=enc)
                # å¦‚æœæˆåŠŸè¯»å‡ºå¤šåˆ—ï¼Œè¯´æ˜ç¼–ç å¯¹äº†
                if len(df.columns) > 1:
                    break
            except:
                continue
        
        if df is None: return None

        # 2. æš´åŠ›é”å®šåˆ— (ä¸å†ä¾èµ–åˆ—å)
        # ä½ çš„æ–‡ä»¶ç»“æ„ï¼šColumn 0 æ˜¯å•è¯ï¼ŒColumn 3 æ˜¯æ’å
        if len(df.columns) >= 4:
            word_col = df.columns[0] # ç¬¬1åˆ—
            rank_col = df.columns[3] # ç¬¬4åˆ—
        else:
            # å…œåº•ï¼šå¦‚æœç”¨æˆ·æ¢äº†æ–‡ä»¶ï¼Œå°è¯•æ™ºèƒ½æŸ¥æ‰¾
            df.columns = [str(c).strip().lower().replace('\n', '') for c in df.columns]
            rank_col = next((c for c in df.columns if any(k in c for k in ['rank', 'æ’å', 'åºå·', 'è¯é¢‘'])), df.columns[0])
            word_col = next((c for c in df.columns if any(k in c for k in ['word', 'å•è¯', 'è¯æ±‡'])), df.columns[1])

        # 3. å»ºç«‹å­—å…¸ (æ¸…æ´—æ•°æ®)
        # å¼ºåˆ¶æŠŠæ’åè½¬ä¸ºæ•°å­—ï¼Œæ— æ³•è½¬æ¢çš„(æ¯”å¦‚è¡¨å¤´)å˜NaNç„¶åå¡«å……99999
        df['rank_clean'] = pd.to_numeric(df[rank_col], errors='coerce').fillna(99999)
        df['word_clean'] = df[word_col].astype(str).str.lower().str.strip()
        
        # è¿‡æ»¤æ‰æ— æ•ˆè¡Œ
        df = df[df['rank_clean'] < 99990] 
        
        vocab_dict = pd.Series(
            df['rank_clean'].values, 
            index=df['word_clean']
        ).to_dict()
        
        return vocab_dict
    except Exception as e:
        st.error(f"è¯åº“åŠ è½½å‡ºé”™: {e}")
        return None

# --- 3. æ ¸å¿ƒé€»è¾‘ ---
def process_text_smart(text, vocab_dict, range_start, range_end):
    text_lower = text.lower()
    words = re.findall(r'\b[a-z\']{2,}\b', text_lower)
    unique_words = sorted(list(set(words)))
    
    tier_known = []   
    tier_target = []  
    tier_beyond = []  
    
    for w in unique_words:
        rank = 999999
        match_word = w
        
        # 1. æŸ¥åŸå½¢
        if w in vocab_dict:
            rank = vocab_dict[w]
            match_word = w
        else:
            # 2. æŸ¥è¿˜åŸ
            lemma = get_lemma(w)
            if lemma in vocab_dict:
                rank = vocab_dict[lemma]
                match_word = lemma
            else:
                # 3. æŸ¥å˜ä½“
                if w.endswith("'s") and w[:-2] in vocab_dict:
                    rank = vocab_dict[w[:-2]]
                    match_word = w[:-2]

        item = {'å•è¯': match_word, 'åŸæ–‡': w, 'æ’å': int(rank)}
        
        if rank <= range_start:
            tier_known.append(item)
        elif range_start < rank <= range_end:
            tier_target.append(item)
        else:
            if item['å•è¯'] == item['åŸæ–‡']:
                item['åŸæ–‡'] = '-'
            tier_beyond.append(item)

    def to_df(data):
        if not data: return pd.DataFrame()
        return pd.DataFrame(data).sort_values('æ’å').drop_duplicates(subset=['å•è¯'])

    return to_df(tier_known), to_df(tier_target), to_df(tier_beyond)

# --- 4. ç•Œé¢ UI ---
st.title("ğŸ§  Vibe Vocab v5.1 (å¼ºåŠ›ä¿®å¤ç‰ˆ)")
st.caption("å¼ºåˆ¶åˆ—å¯¹é½ Â· è§£å†³ç®€å•è¯æ’åé”™è¯¯")

vocab_dict = load_vocab()
if not vocab_dict:
    st.error("âŒ æ‰¾ä¸åˆ°è¯åº“æˆ–è¯»å–å¤±è´¥ï¼")
    st.stop()

st.sidebar.header("âš™ï¸ å­¦ä¹ è§„åˆ’")
st.sidebar.success(f"ğŸ“š è¯åº“åŠ è½½æˆåŠŸ: {len(vocab_dict)} è¯")

st.sidebar.subheader("è®¾å®šå­¦ä¹ åŒºé—´")
vocab_range = st.sidebar.slider(
    "æ‹–åŠ¨æ»‘å—ï¼š", 1, 20000, (6000, 8000), 500
)
range_start, range_end = vocab_range

st.sidebar.info(
    f"ğŸŸ¢ **ç†Ÿè¯**: 1 - {range_start}\n\n"
    f"ğŸŸ¡ **é‡ç‚¹**: {range_start} - {range_end}\n\n"
    f"ğŸ”´ **è¶…çº²**: {range_end}+"
)

with st.expander("ğŸ“ æ–‡æœ¬è¾“å…¥", expanded=True):
    tab_paste, tab_upload = st.tabs(["ç²˜è´´æ–‡æœ¬", "ä¸Šä¼  TXT"])
    with tab_paste:
        text_input = st.text_area("åœ¨æ­¤ç²˜è´´:", height=150)
    with tab_upload:
        uploaded = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type="txt")
        if uploaded:
            text_input = uploaded.read().decode("utf-8")

final_text = text_input if text_input else ""

def show_download_buttons(df, prefix):
    if df.empty: return
    col1, col2 = st.columns(2)
    csv = df.to_csv(index=False).encode('utf-8')
    col1.download_button(f"ğŸ“¥ ä¸‹è½½ Excel", csv, f"{prefix}.csv", "text/csv")
    txt = "\n".join(df['å•è¯'].tolist())
    col2.download_button(f"ğŸ“„ ä¸‹è½½ TXT", txt, f"{prefix}.txt", "text/plain")

if st.button("ğŸš€ å¼€å§‹æ™ºèƒ½åˆ†æ", type="primary"):
    if not final_text.strip():
        st.warning("è¯·å…ˆè¾“å…¥æ–‡æœ¬ï¼")
    else:
        df_known, df_target, df_beyond = process_text_smart(final_text, vocab_dict, range_start, range_end)
        
        st.success(f"åˆ†æå®Œæˆï¼")
        
        t1, t2, t3 = st.tabs([
            f"ğŸŸ¡ é‡ç‚¹çªç ´ ({len(df_target)})", 
            f"ğŸ”´ è¶…çº²/ç”Ÿè¯ ({len(df_beyond)})", 
            f"ğŸŸ¢ å·²æŒæ¡ ({len(df_known)})"
        ])
        
        with t1:
            st.markdown(f"### ğŸ¯ é‡ç‚¹å­¦ä¹  ({range_start}-{range_end})")
            if not df_target.empty:
                st.dataframe(df_target, use_container_width=True)
                show_download_buttons(df_target, "target_words")
            else:
                st.info("æ­¤åŒºé—´æ— ç”Ÿè¯ã€‚")

        with t2:
            st.markdown(f"### ğŸš€ è¶…çº²è¯ (>{range_end})")
            # è°ƒè¯•ä¿¡æ¯ï¼šå¦‚æœè¿™é‡Œå‡ºç°äº†ç®€å•çš„è¯ï¼Œè¯´æ˜è¯åº“æ²¡è¯»å¯¹
            if not df_beyond.empty:
                st.dataframe(df_beyond, use_container_width=True)
                show_download_buttons(df_beyond, "beyond_words")
            else:
                st.info("æ²¡æœ‰è¶…çº²è¯ã€‚")

        with t3:
            st.markdown(f"### âœ… å·²æŒæ¡ (<{range_start})")
            if not df_known.empty:
                st.dataframe(df_known, use_container_width=True)
                show_download_buttons(df_known, "known_words")
            else:
                st.info("æ— ç†Ÿè¯ã€‚")