import streamlit as st
import pandas as pd
import os
import sys
import subprocess

# è®¾ç½®é¡µé¢
st.set_page_config(page_title="Vibe Vocab Studio", page_icon="ğŸ§ ", layout="wide")

# ==========================================
# 0. æ ¸å¿ƒå¼•æ“ï¼šè‡ªä¿®å¤å¼åŠ è½½é€»è¾‘
# ==========================================
@st.cache_resource
def load_nlp():
    import spacy
    model_name = "en_core_web_sm"
    try:
        # å°è¯•æ­£å¸¸åŠ è½½
        return spacy.load(model_name)
    except OSError:
        # å¦‚æœåŠ è½½å¤±è´¥ï¼ˆè¯´æ˜æ²¡å®‰è£…ï¼‰ï¼Œåˆ™åœ¨è¿è¡Œæ—¶å¼ºè¡Œä¸‹è½½
        st.warning(f"æ­£åœ¨åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ ({model_name})... é¦–æ¬¡è¿è¡Œéœ€è¦ 30-60 ç§’ï¼Œè¯·å‹¿åˆ·æ–°ã€‚")
        try:
            # ä½¿ç”¨å­è¿›ç¨‹è°ƒç”¨å‘½ä»¤è¡Œä¸‹è½½
            subprocess.run([sys.executable, "-m", "spacy", "download", model_name], check=True)
            st.success("æ¨¡å‹å®‰è£…æˆåŠŸï¼æ­£åœ¨åŠ è½½...")
            return spacy.load(model_name)
        except Exception as e:
            st.error(f"æ¨¡å‹è‡ªåŠ¨å®‰è£…å¤±è´¥: {str(e)}")
            return None

nlp = load_nlp()

# ==========================================
# 1. è¯åº“åŠ è½½é€»è¾‘
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv"]

@st.cache_data
def load_vocab():
    file_path = next((f for f in POSSIBLE_FILES if os.path.exists(f)), None)
    if not file_path:
        return None, "âŒ æœªæ‰¾åˆ°è¯åº“æ–‡ä»¶ï¼ˆcoca_cleaned.csvï¼‰"
    try:
        df = pd.read_csv(file_path)
        # æ¸…æ´—åˆ—å
        df.columns = [str(c).strip().lower() for c in df.columns]
        
        # æ™ºèƒ½åŒ¹é…åˆ—
        w_col = 'word' if 'word' in df.columns else df.columns[0]
        r_col = 'rank' if 'rank' in df.columns else df.columns[1]
        
        # ç»Ÿä¸€æ ¼å¼
        df[w_col] = df[w_col].astype(str).str.lower().str.strip()
        
        # æ„å»ºå­—å…¸
        vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        return vocab, f"âœ… è¯åº“åŠ è½½æˆåŠŸ: {file_path}"
    except Exception as e:
        return None, f"âŒ è¯»å–å¤±è´¥: {str(e)}"

vocab_dict, status_msg = load_vocab()

# ==========================================
# 2. ä¾§è¾¹æ ä¸ UI
# ==========================================
st.title("ğŸ§  Vibe Vocab v13.0 (è‡ªä¿®å¤ç‰ˆ)")

if nlp is None:
    st.error("ğŸš¨ æ ¸å¿ƒç»„ä»¶åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")
    st.stop()

if not vocab_dict:
    st.error(status_msg)
    st.stop()

with st.sidebar:
    st.success("å¼•æ“çŠ¶æ€ï¼šåœ¨çº¿ ğŸŸ¢")
    st.info(status_msg)
    st.divider()
    v_range = st.slider("è®¾å®šå­¦ä¹ åŒºé—´", 1, 20000, (6000, 8000), 500)
    r_start, r_end = v_range
    st.write(f"ğŸŸ¢ ç†Ÿè¯: 1-{r_start}")
    st.write(f"ğŸŸ¡ é‡ç‚¹: {r_start}-{r_end}")
    st.write(f"ğŸ”´ è¶…çº²: {r_end}+")

# ==========================================
# 3. æ ¸å¿ƒå¤„ç†é€»è¾‘
# ==========================================
def process_text_pro(text):
    doc = nlp(text.lower())
    results = []
    seen_lemmas = set()
    
    for token in doc:
        if token.is_alpha and len(token.text) > 1:
            # è¯å½¢è¿˜åŸæ ¸å¿ƒ
            lemma = token.lemma_.lower()
            original = token.text.lower()
            
            if lemma not in seen_lemmas:
                rank = vocab_dict.get(lemma, 99999)
                
                # äºŒæ¬¡æŸ¥æ‰¾é€»è¾‘
                if rank == 99999 and original in vocab_dict:
                    rank = vocab_dict[original]
                    display_word = original
                else:
                    display_word = lemma
                
                results.append({
                    'å•è¯': display_word,
                    'åŸæ–‡': original if original != display_word else "-",
                    'æ’å': int(rank)
                })
                seen_lemmas.add(lemma)

    if not results:
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    df = pd.DataFrame(results).sort_values('æ’å')
    known = df[df['æ’å'] <= r_start]
    target = df[(df['æ’å'] > r_start) & (df['æ’å'] <= r_end)]
    beyond = df[df['æ’å'] > r_end]
    return known, target, beyond

# ==========================================
# 4. ä¸»ç•Œé¢äº¤äº’
# ==========================================
text_input = st.text_area("åœ¨æ­¤ç²˜è´´ä½ çš„è‹±æ–‡æ–‡ç« /å°è¯´å†…å®¹:", height=200)

if st.button("ğŸš€ å¼€å§‹ç²¾å‡†åˆ†æ", type="primary"):
    if not text_input.strip():
        st.warning("è¯·è¾“å…¥æ–‡æœ¬å†…å®¹")
    else:
        with st.spinner("æ­£åœ¨åˆ†æä¸­..."):
            df_k, df_t, df_b = process_text_pro(text_input)
        
        st.success(f"åˆ†æå®Œæˆï¼æ‰¾åˆ°é‡ç‚¹è¯: {len(df_t)} ä¸ª")
        
        tab1, tab2, tab3 = st.tabs([
            f"ğŸŸ¡ é‡ç‚¹çªç ´ ({len(df_t)})", 
            f"ğŸ”´ ç”Ÿè¯/è¶…çº² ({len(df_b)})", 
            f"ğŸŸ¢ ç†Ÿè¯è¡¨ ({len(df_k)})"
        ])
        
        with tab1:
            st.dataframe(df_t, use_container_width=True)
            if not df_t.empty:
                csv_t = df_t.to_csv(index=False).encode('utf-8')
                st.download_button("ğŸ“¥ ä¸‹è½½é‡ç‚¹è¯ CSV", csv_t, "target.csv", "text/csv")
            
        with tab2:
            st.dataframe(df_b, use_container_width=True)
            
        with tab3:
            st.dataframe(df_k, use_container_width=True)