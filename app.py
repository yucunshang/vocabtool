import streamlit as st
import pandas as pd
import re
import os
import io
import time
import lemminflect
import nltk
import genanki
import random
import tempfile
from bs4 import BeautifulSoup

# --- æ–‡ä»¶å¤„ç†åº“ ---
import pypdf
import docx
import ebooklib
from ebooklib import epub

# ==========================================
# 0. é¡µé¢åŸºç¡€é…ç½® & æ ·å¼
# ==========================================
st.set_page_config(
    page_title="Vocab Flow Ultra", 
    page_icon="âš¡ï¸", 
    layout="centered",
    initial_sidebar_state="collapsed" 
)

st.markdown("""
<style>
    .stTextArea textarea { font-family: 'Consolas', monospace; font-size: 14px; }
    .stButton>button { border-radius: 8px; font-weight: 600; width: 100%; }
    .stat-box { padding: 15px; background-color: #f0fdf4; border: 1px solid #bbf7d0; border-radius: 8px; text-align: center; color: #166534; margin-bottom: 20px; }
    .or-divider { text-align: center; margin: 10px 0; color: #888; font-size: 0.9em; font-weight: bold; }
    /* è°ƒæ•´ä¸Šä¼ ç»„ä»¶çš„å†…è¾¹è· */
    [data-testid='stFileUploader'] { padding-top: 10px; }
    /* è°ƒæ•´æŒ‰é’®é—´è· */
    .stButton { margin-top: 5px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºåŠ è½½ & å·¥å…·å‡½æ•°
# ==========================================
@st.cache_resource
def setup_nltk():
    try:
        root_dir = os.path.dirname(os.path.abspath(__file__))
        nltk_data_dir = os.path.join(root_dir, 'nltk_data')
        os.makedirs(nltk_data_dir, exist_ok=True)
        nltk.data.path.append(nltk_data_dir)
        for pkg in ['averaged_perceptron_tagger', 'punkt', 'punkt_tab']:
            try: nltk.data.find(f'tokenizers/{pkg}')
            except LookupError: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
    except: pass
setup_nltk()

@st.cache_data
def load_vocab_data():
    """åŠ è½½è¯é¢‘è¡¨"""
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            df.columns = [c.strip().lower() for c in df.columns]
            w_col = next((c for c in df.columns if 'word' in c), df.columns[0])
            r_col = next((c for c in df.columns if 'rank' in c), df.columns[1])
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            df = df.sort_values(r_col).drop_duplicates(subset=[w_col], keep='first')
            return pd.Series(df[r_col].values, index=df[w_col]).to_dict(), df
        except: return {}, None
    return {}, None

VOCAB_DICT, FULL_DF = load_vocab_data()

def get_lemma(word):
    try: return lemminflect.getLemma(word, upos='VERB')[0]
    except: return word

def clear_all_state():
    """ä¸€é”®æ¸…ç©ºçš„å›è°ƒå‡½æ•°"""
    st.session_state.clear()

# ==========================================
# 2. æ–‡æœ¬æå–ä¸åˆ†æ
# ==========================================
def extract_text_from_file(uploaded_file):
    """å¤šæ ¼å¼æ–‡ä»¶è§£æ"""
    text = ""
    file_type = uploaded_file.name.split('.')[-1].lower()
    try:
        if file_type == 'txt':
            text = uploaded_file.getvalue().decode("utf-8", errors='ignore')
        elif file_type == 'pdf':
            reader = pypdf.PdfReader(uploaded_file)
            text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif file_type == 'docx':
            doc = docx.Document(uploaded_file)
            text = "\n".join([p.text for p in doc.paragraphs])
        elif file_type == 'epub':
            with tempfile.NamedTemporaryFile(delete=False, suffix='.epub') as tmp:
                tmp.write(uploaded_file.getvalue())
                tmp_path = tmp.name
            book = epub.read_epub(tmp_path)
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    text += soup.get_text() + " "
            os.remove(tmp_path)
    except Exception as e:
        return f"Error: {e}"
    return text

def analyze_logic(text, current_lvl, target_lvl):
    """æ ¸å¿ƒåˆ†æé€»è¾‘"""
    raw_tokens = re.findall(r"[a-z]+", text.lower())
    total_words = len(raw_tokens)
    unique_tokens = set(raw_tokens)
    
    target_words = []
    for w in unique_tokens:
        if len(w) < 3: continue 
        lemma = get_lemma(w)
        rank = VOCAB_DICT.get(lemma, 99999)
        
        # ç­›é€‰: Current < Rank <= Target
        if rank > current_lvl and rank <= target_lvl:
            target_words.append((lemma, rank))
            
    target_words.sort(key=lambda x: x[1])
    final_list = [x[0] for x in target_words]
    return final_list, total_words

# ==========================================
# 3. Anki ç”Ÿæˆé€»è¾‘
# ==========================================
def generate_anki_package(cards_data, deck_name="Vocab_Deck"):
    # å­—ä½“å¤§å°: Examples -> 20px, Etymology -> 17px
    CSS = """
    .card { font-family: arial; font-size: 20px; text-align: center; color: #333; background-color: white; padding: 20px; }
    .nightMode .card { background-color: #2f2f31; color: #f5f5f5; }
    .word { font-size: 40px; font-weight: bold; color: #007AFF; margin-bottom: 10px; }
    .nightMode .word { color: #5FA9FF; }
    .phonetic { color: #888; font-size: 18px; font-family: sans-serif; }
    .def-container { text-align: left; margin-top: 20px; border-top: 1px solid #ddd; padding-top: 10px; }
    .definition { font-weight: bold; color: #444; margin-bottom: 10px; }
    .nightMode .definition { color: #ddd; }
    .examples { background: #f4f4f4; padding: 10px; border-radius: 5px; color: #555; font-style: italic; font-size: 20px; }
    .nightMode .examples { background: #333; color: #ccc; }
    .etymology { margin-top: 15px; font-size: 17px; color: #888; border: 1px dashed #ccc; padding: 5px; display: inline-block;}
    """
    
    model = genanki.Model(
        random.randrange(1 << 30, 1 << 31), 'VocabFlow Model',
        fields=[{'name': 'Word'}, {'name': 'IPA'}, {'name': 'Meaning'}, {'name': 'Examples'}, {'name': 'Etymology'}],
        templates=[{
            'name': 'Card 1',
            'qfmt': '<div class="word">{{Word}}</div><div class="phonetic">{{IPA}}</div>',
            'afmt': '{{FrontSide}}<div class="def-container"><div class="definition">{{Meaning}}</div><div class="examples">{{Examples}}</div><div class="etymology">ğŸŒ± {{Etymology}}</div></div>',
        }], css=CSS
    )
    
    deck = genanki.Deck(random.randrange(1 << 30, 1 << 31), deck_name)
    for c in cards_data:
        deck.add_note(genanki.Note(model=model, fields=[c['word'], c['ipa'], c['meaning'], c['examples'].replace('\n','<br>'), c['etymology']]))
        
    with tempfile.NamedTemporaryFile(delete=False, suffix='.apkg') as tmp:
        genanki.Package(deck).write_to_file(tmp.name)
        return tmp.name

def get_ai_prompt(words):
    """ä¼˜åŒ–åçš„ Prompt"""
    w_list = ", ".join(words)
    return f"""
You are a strictly compliant dictionary data generator. 
Convert the provided words into Anki card format using the rules below.

**Input Words:** {w_list}

**Strict Output Rules:**
1. **Format:** `Word | IPA | Definition | Examples | Etymology`
2. **Separator:** Use `|` strictly as the field separator. Do NOT use `|` inside the content text.
3. **No Fluff:** Output ONLY the raw text lines. NO headers, NO markdown code blocks, NO conversational filler (e.g., "Here is the list").
4. **Newlines:** Use `<br>` for line breaks inside examples. Do NOT generate actual newlines within a single entry.

**Content Requirements:**
- **IPA:** US pronunciation.
- **Definition:** Simple B2/C1 English. Keep it concise (< 12 words).
- **Examples:** 1 or 2 short, high-context sentences. Separate them with `<br>`. Highlight the keyword in **bold** if possible.
- **Etymology:** Brief root analysis (e.g., "bene(good) + vol(wish)"). If unknown, leave empty.

**Example Output:**
benevolent | /bÉ™ËˆnevÉ™lÉ™nt/ | kind and helpful | He was a **benevolent** old man.<br>The fund is for **benevolent** purposes. | bene(good) + vol(wish)
ephemeral | /É™ËˆfemÉ™rÉ™l/ | lasting for a very short time | Fashions are **ephemeral**, changing with every season. | epi(on) + hemera(day)
"""

# ==========================================
# 4. ä¸»ç¨‹åº
# ==========================================
st.title("âš¡ï¸ Vocab Flow Ultra")

if not VOCAB_DICT:
    st.error("âš ï¸ ç¼ºå¤± `coca_cleaned.csv`")

# Input Tabs
tab_extract, tab_anki = st.tabs(["1ï¸âƒ£ å†…å®¹æå– & ç”Ÿæˆ", "2ï¸âƒ£ æ‰“åŒ… Anki"])

# ------------------------------------------
# TAB 1: æå–é€»è¾‘
# ------------------------------------------
with tab_extract:
    # å­ Tabï¼šåŒºåˆ†â€œè¯­å¢ƒåˆ†æâ€å’Œâ€œçº¯Rankåˆ—è¡¨â€
    mode_context, mode_rank = st.tabs(["ğŸ“„ è¯­å¢ƒåˆ†æ (æ–‡æœ¬/æ–‡ä»¶)", "ğŸ”¢ è¯é¢‘åˆ—è¡¨ (Rank)"])
    
    # --- A. è¯­å¢ƒåˆ†ææ¨¡å¼ ---
    with mode_context:
        st.markdown("#### 1. è®¾å®šè¯æ±‡åˆ†çº§")
        c1, c2 = st.columns(2)
        curr = c1.number_input("å¿½ç•¥å¤ªç®€å•çš„ (Current Level)", 1000, 20000, 4000, step=500, help="å°äºæ­¤æ’åçš„è¯ä¼šè¢«è®¤ä¸ºæ˜¯å·²æŒæ¡è¯æ±‡")
        targ = c2.number_input("å¿½ç•¥å¤ªéš¾çš„ (Target Level)", 2000, 50000, 15000, step=500, help="åªæå–æ­¤æ’åå†…çš„è¯")
        
        st.markdown("#### 2. è¾“å…¥å†…å®¹ (æ–‡ä»¶æˆ–æ–‡æœ¬)")
        
        # ç»Ÿä¸€è¾“å…¥åŒº
        uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£ (PDF/TXT/DOCX/EPUB)", type=['txt','pdf','docx','epub'])
        
        st.markdown('<div class="or-divider">- OR -</div>', unsafe_allow_html=True)
        
        pasted_text = st.text_area("ğŸ“„ ...æˆ–åœ¨æ­¤ç›´æ¥ç²˜è´´æ–‡æœ¬", height=150, placeholder="åœ¨æ­¤å¤„ç²˜è´´è‹±æ–‡æ–‡ç« ...")
        
        # ç»Ÿä¸€çš„åˆ†ææŒ‰é’®
        if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
            raw_text = ""
            is_file = False
            
            # ä¼˜å…ˆå¤„ç†æ–‡ä»¶
            if uploaded_file:
                with st.spinner(f"æ­£åœ¨è¯»å– {uploaded_file.name}..."):
                    raw_text = extract_text_from_file(uploaded_file)
                    is_file = True
            elif pasted_text.strip():
                raw_text = pasted_text
                
            # æ‰§è¡Œåˆ†æ
            if raw_text and len(raw_text) > 10:
                final_words, total = analyze_logic(raw_text, curr, targ)
                st.session_state['gen_words'] = final_words
                st.session_state['total_count'] = total
                if is_file:
                    st.toast(f"æ–‡ä»¶è§£ææˆåŠŸï¼Œå‘ç° {total} ä¸ªè¯", icon="âœ…")
            else:
                st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ–‡ä»¶æˆ–ç²˜è´´æ–‡æœ¬å†…å®¹")

        # ç§»åŠ¨åçš„æ¸…ç©ºæŒ‰é’®ï¼šç›´æ¥æ˜¾ç¤ºåœ¨åˆ†ææŒ‰é’®ä¸‹æ–¹
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ® (Reset)", type="secondary", on_click=clear_all_state):
            pass

    # --- B. çº¯è¯é¢‘ç”Ÿæˆæ¨¡å¼ ---
    with mode_rank:
        st.info("ç›´æ¥ä» COCA è¯é¢‘è¡¨ä¸­æå–æŒ‡å®šæ®µè½çš„å•è¯ã€‚")
        c_a, c_b = st.columns(2)
        s_rank = c_a.number_input("èµ·å§‹æ’å (Start Rank)", 1, 20000, 8000, step=100)
        count = c_b.number_input("ç”Ÿæˆæ•°é‡ (Count)", 10, 500, 50, step=10)
        
        if st.button("ğŸ”¢ ç”Ÿæˆåˆ—è¡¨", type="primary"):
            if FULL_DF is not None:
                try:
                    r_col = next(c for c in FULL_DF.columns if 'rank' in c)
                    w_col = next(c for c in FULL_DF.columns if 'word' in c)
                    subset = FULL_DF[FULL_DF[r_col] >= s_rank].sort_values(r_col).head(count)
                    st.session_state['gen_words'] = subset[w_col].tolist()
                    st.session_state['total_count'] = count
                except: st.error("æ•°æ®æºæ ¼å¼é”™è¯¯")
        
        # åŒæ ·ç»™è¿™é‡Œä¹ŸåŠ ä¸€ä¸ªé‡ç½®æŒ‰é’®æ–¹ä¾¿æ“ä½œ
        if st.button("ğŸ—‘ï¸ æ¸…ç©º (Reset)", type="secondary", key="reset_rank", on_click=clear_all_state):
            pass

    # --- å…±é€šç»“æœå±•ç¤ºåŒº ---
    if 'gen_words' in st.session_state:
        words = st.session_state['gen_words']
        
        st.divider()
        st.markdown(f"""
        <div class="stat-box">
            ğŸ“Š æ¥æºæ€»è¯æ•°: <b>{st.session_state.get('total_count', 0)}</b> | 
            ğŸ¯ ç­›é€‰åç”Ÿè¯: <b>{len(words)}</b> ä¸ª
        </div>
        """, unsafe_allow_html=True)

        if len(words) > 0:
            # ç»“æœé¢„è§ˆ
            with st.expander("ğŸ‘ï¸ é¢„è§ˆå•è¯åˆ—è¡¨", expanded=False):
                st.write(", ".join(words))

            st.markdown("### ğŸ¤– è·å– AI Prompt")
            c_batch, c_info = st.columns([1, 2])
            
            # é»˜è®¤ 50ï¼Œä¸Šé™ 200
            batch_size = c_batch.number_input("æ¯ç»„å•è¯æ•° (Batch Size)", 10, 200, 50, step=10)
            
            c_info.caption(f"ğŸ’¡ å»ºè®® 30-50 ä¸ªä¸€ç»„ã€‚å…±éœ€ {len(words)//batch_size + (1 if len(words)%batch_size else 0)} æ¬¡å¯¹è¯ã€‚")
            
            # è‡ªåŠ¨åˆ†æ‰¹é€»è¾‘
            batches = [words[i:i + batch_size] for i in range(0, len(words), batch_size)]
            
            for idx, batch in enumerate(batches):
                with st.expander(f"ç¬¬ {idx+1} ç»„ (å•è¯ {idx*batch_size+1} - {idx*batch_size+len(batch)})", expanded=(idx==0)):
                    prompt = get_ai_prompt(batch)
                    st.code(prompt, language="markdown")
                    st.caption("ğŸ‘† ç‚¹å‡»å³ä¸Šè§’å¤åˆ¶ -> å‘ç»™ AI -> å¤åˆ¶å›å¤ -> ç²˜è´´åˆ° 'æ‰“åŒ… Anki' é¡µé¢")

# ------------------------------------------
# TAB 2: æ‰“åŒ… Anki
# ------------------------------------------
with tab_anki:
    st.markdown("### ğŸ“¦ åˆ¶ä½œ Anki ç‰Œç»„")
    st.info("ğŸ’¡ æç¤ºï¼šå°† AI å¯¹è¯ä¸­çš„å›å¤å†…å®¹ï¼ˆåŒ…å« | åˆ†éš”ç¬¦çš„è¡Œï¼‰å…¨éƒ¨ç²˜è´´åˆ°ä¸‹æ–¹ã€‚æ”¯æŒå¤šæ¬¡ç²˜è´´ã€‚")
    
    ai_resp = st.text_area("åœ¨æ­¤ç²˜è´´ AI çš„å›å¤å†…å®¹", height=300, placeholder="word1 | /ipa/ | meaning... \nword2 | ...")
    deck_name = st.text_input("ç‰Œç»„åç§° (.apkg)", "VocabFlow_Deck")
    
    if st.button("ğŸ”¨ ç”Ÿæˆ .apkg æ–‡ä»¶", type="primary"):
        if not ai_resp.strip():
            st.error("âŒ å†…å®¹ä¸ºç©ºï¼Œè¯·å…ˆç²˜è´´ AI çš„å›å¤")
        else:
            cards = []
            skipped = 0
            # å®½å®¹è§£æ
            for line in ai_resp.strip().split('\n'):
                line = line.strip()
                if not line: continue
                if "|" not in line: continue
                
                # å¢å¼ºè¿‡æ»¤é€»è¾‘ï¼šè¿‡æ»¤è¡¨å¤´å’Œåˆ†å‰²çº¿
                if "Word" in line and "IPA" in line: continue  
                if set(line.strip()) == {'-', '|'} or "---" in line: continue 
                
                parts = [p.strip() for p in line.split('|')]
                if len(parts) >= 3:
                    cards.append({
                        'word': parts[0],
                        'ipa': parts[1] if len(parts) > 1 else '',
                        'meaning': parts[2] if len(parts) > 2 else '',
                        'examples': parts[3] if len(parts) > 3 else '',
                        'etymology': parts[4] if len(parts) > 4 else ''
                    })
                else:
                    skipped += 1
            
            if cards:
                f_path = generate_anki_package(cards, deck_name)
                with open(f_path, "rb") as f:
                    st.download_button(
                        f"ğŸ“¥ ç‚¹å‡»ä¸‹è½½ {deck_name}.apkg", 
                        f, 
                        file_name=f"{deck_name}.apkg", 
                        mime="application/octet-stream", 
                        type="primary"
                    )
                st.balloons()
                st.success(f"ğŸ‰ æˆåŠŸæ‰“åŒ… {len(cards)} å¼ å¡ç‰‡ï¼")
                if skipped > 0:
                    st.caption(f"æ³¨ï¼šè·³è¿‡äº† {skipped} è¡Œæ ¼å¼ä¸ç¬¦çš„æ•°æ®")
            else:
                st.error("âš ï¸ æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥åˆ†éš”ç¬¦æ˜¯å¦ä¸º '|'")