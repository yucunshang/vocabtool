# app.py
import streamlit as st
import pandas as pd
import time
import random

# å¼•å…¥æœ¬åœ°æ¨¡å—
import utils
import logic
import styles
from utils import VOCAB_DICT, FULL_DF

# ==========================================
# 0. é¡µé¢é…ç½®
# ==========================================
st.set_page_config(
    page_title="Vocab Flow Ultra", 
    page_icon="âš¡ï¸", 
    layout="centered", 
    initial_sidebar_state="collapsed"
)

# åŠ¨æ€ Key åˆå§‹åŒ– (ç”¨äºä¸€é”®æ¸…ç©º)
if 'uploader_id' not in st.session_state:
    st.session_state['uploader_id'] = "1000"

st.markdown(styles.CUSTOM_CSS, unsafe_allow_html=True)

# ==========================================
# 5. UI ä¸»ç¨‹åº
# ==========================================
st.title("âš¡ï¸ Vocab Flow Ultra")

if not VOCAB_DICT:
    st.error("âš ï¸ ç¼ºå¤± `coca_cleaned.csv`")

tab_guide, tab_extract, tab_anki = st.tabs(["ğŸ“– ä½¿ç”¨æŒ‡å—", "1ï¸âƒ£ å•è¯æå–", "2ï¸âƒ£ Anki åˆ¶ä½œ"])

with tab_guide:
    st.markdown("""
    ### ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ Vocab Flow Ultra
    è¿™æ˜¯ä¸€ä¸ª**ä»é˜…è¯»ææ–™ä¸­æå–ç”Ÿè¯**ï¼Œå¹¶åˆ©ç”¨ **AI** è‡ªåŠ¨ç”Ÿæˆ **Anki å¡ç‰‡**çš„æ•ˆç‡å·¥å…·ã€‚
    
    ---
    
    <div class="guide-step">
    <span class="guide-title">Step 1: æå–ç”Ÿè¯ (Extract)</span>
    åœ¨ <code>1ï¸âƒ£ å•è¯æå–</code> æ ‡ç­¾é¡µï¼š<br><br>
    <strong>1. ä¸Šä¼ æ–‡ä»¶</strong><br>
    æ”¯æŒ PDF, TXT, EPUB, DOCXã€‚æ— è®ºæ˜¯å°è¯´ã€æ–‡ç« è¿˜æ˜¯å•è¯è¡¨ï¼Œç›´æ¥ä¸¢è¿›å»å³å¯ã€‚<br>
    ç³»ç»Ÿä¼šè‡ªåŠ¨è¿›è¡Œ <strong>NLP è¯å½¢è¿˜åŸ</strong>ï¼ˆå°† went è¿˜åŸä¸º goï¼‰å¹¶æ¸…æ´—åƒåœ¾è¯ï¼ˆä¹±ç ã€é‡å¤å­—ç¬¦ï¼‰ã€‚<br>
    <br>
    <strong>2. è®¾ç½®è¿‡æ»¤èŒƒå›´ (Rank Filter)</strong><br>
    åˆ©ç”¨ COCA 20000 è¯é¢‘è¡¨è¿›è¡Œç§‘å­¦ç­›é€‰ï¼š
    <ul>
        <li><strong>å¿½ç•¥æ’åå‰ N</strong> (Min Rank)ï¼šä¾‹å¦‚è®¾ä¸º <code>2000</code>ï¼Œä¼šè¿‡æ»¤æ‰ `the, is, you` ç­‰æœ€åŸºç¡€çš„é«˜é¢‘è¯ã€‚</li>
        <li><strong>å¿½ç•¥æ’åå N</strong> (Max Rank)ï¼šä¾‹å¦‚è®¾ä¸º <code>15000</code>ï¼Œä¼šè¿‡æ»¤æ‰æå…¶ç”Ÿåƒ»çš„è¯ã€‚</li>
        <li><strong>ğŸ”“ åŒ…å«ç”Ÿåƒ»è¯</strong> (Unknown)ï¼šå‹¾é€‰åï¼Œå°†å¼ºåˆ¶åŒ…å«è¯é¢‘è¡¨ä¸­æ²¡æœ‰çš„è¯ï¼ˆå¦‚äººåã€åœ°åã€æ–°é€ è¯ï¼‰ã€‚</li>
    </ul>
    <br>
    <strong>3. ç‚¹å‡» ğŸš€ å¼€å§‹åˆ†æ</strong><br>
    ç³»ç»Ÿä¼šèåˆå¤„ç†ï¼Œè‡ªåŠ¨å»é‡å¹¶æŒ‰è¯é¢‘æ’åºï¼Œæœ€å¤§åŒ–æå–æœ‰æ•ˆå•è¯ã€‚
    </div>

    <div class="guide-step">
    <span class="guide-title">Step 2: è·å– Prompt (AI Generation)</span>
    åˆ†æå®Œæˆåï¼š<br><br>
    <strong>1. è‡ªå®šä¹‰è®¾ç½®</strong><br>
    ç‚¹å‡» <code>âš™ï¸ è‡ªå®šä¹‰ Prompt è®¾ç½®</code>ï¼Œé€‰æ‹©æ­£é¢æ˜¯å•è¯è¿˜æ˜¯çŸ­è¯­ï¼Œé‡Šä¹‰è¯­è¨€ç­‰ã€‚<br>
    <br>
    <strong>2. å¤åˆ¶ Prompt</strong><br>
    ç³»ç»Ÿä¼šè‡ªåŠ¨å°†å•è¯åˆ†ç»„ã€‚ç”Ÿæˆçš„å•è¯è¡¨æ”¯æŒ<strong>æŠ˜å </strong>å’Œ<strong>æ»šåŠ¨æŸ¥çœ‹</strong>ã€‚<br>
    <ul>
        <li>ğŸ“± <strong>æ‰‹æœº/é¸¿è’™ç«¯</strong>ï¼šä½¿ç”¨ä¸‹æ–¹çš„â€œçº¯æ–‡æœ¬æ¡†â€ï¼Œé•¿æŒ‰å…¨é€‰ -> å¤åˆ¶ã€‚</li>
        <li>ğŸ’» <strong>ç”µè„‘ç«¯</strong>ï¼šç‚¹å‡»ä»£ç å—å³ä¸Šè§’çš„ Copy ğŸ“„ å›¾æ ‡ã€‚</li>
    </ul>
    <br>
    <strong>3. å‘é€ç»™ AI</strong><br>
    å°†å¤åˆ¶çš„å†…å®¹å‘é€ç»™ ChatGPT / Claude / Gemini / DeepSeekã€‚AI ä¼šè¿”å›ä¸€ä¸² JSON æ•°æ®ã€‚
    </div>

    <div class="guide-step">
    <span class="guide-title">Step 3: åˆ¶ä½œ Anki ç‰Œç»„ (Create Deck)</span>
    åœ¨ <code>2ï¸âƒ£ Anki åˆ¶ä½œ</code> æ ‡ç­¾é¡µï¼š<br><br>
    <strong>1. ç²˜è´´ AI å›å¤</strong><br>
    å°† AI ç”Ÿæˆçš„ JSON å†…å®¹ç²˜è´´åˆ°è¾“å…¥æ¡†ä¸­ã€‚<br>
    <div class="guide-tip">ğŸ’¡ <strong>æ”¯æŒè¿½åŠ ç²˜è´´</strong>ï¼šå¦‚æœä½ æœ‰ 5 ç»„å•è¯ï¼Œå¯ä»¥æŠŠ AI çš„ 5 æ¬¡å›å¤ä¾æ¬¡ç²˜è´´åœ¨åŒä¸€ä¸ªæ¡†é‡Œï¼Œä¸éœ€è¦åˆ†æ‰¹ä¸‹è½½ã€‚</div>
    <br>
    <strong>2. ä¸‹è½½ä¸å¯¼å…¥</strong><br>
    ç‚¹å‡» <strong>ğŸ“¥ ä¸‹è½½ .apkg</strong>ï¼Œç„¶ååŒå‡»è¯¥æ–‡ä»¶ï¼Œå®ƒä¼šè‡ªåŠ¨å¯¼å…¥åˆ°ä½ çš„ Anki è½¯ä»¶ä¸­ã€‚
    </div>
    """, unsafe_allow_html=True)

with tab_extract:
    mode_context, mode_rank = st.tabs(["ğŸ“„ è¯­å¢ƒåˆ†æ", "ğŸ”¢ è¯é¢‘åˆ—è¡¨"])
    
    with mode_context:
        # V29: ç»Ÿä¸€æ¨¡å¼ï¼Œåªä¿ç•™ç­›é€‰å™¨
        st.info("ğŸ’¡ **å…¨èƒ½æ¨¡å¼**ï¼šç³»ç»Ÿå°†è‡ªåŠ¨è¿›è¡Œ NLP è¯å½¢è¿˜åŸã€å»é‡ã€åƒåœ¾è¯æ¸…æ´—ã€‚æ— è®ºæ˜¯æ–‡ç« è¿˜æ˜¯å•è¯è¡¨ï¼Œç›´æ¥ä¸Šä¼ å³å¯ã€‚")
        
        c1, c2 = st.columns(2)
        curr = c1.number_input("å¿½ç•¥æ’åå‰ N çš„è¯", 1, 20000, 100, step=100)
        targ = c2.number_input("å¿½ç•¥æ’åå N çš„è¯", 2000, 50000, 20000, step=500)
        include_unknown = st.checkbox("ğŸ”“ åŒ…å«ç”Ÿåƒ»è¯/äººå (Rank > 20000)", value=False)

        uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£ (TXT/PDF/DOCX/EPUB)", key=st.session_state['uploader_id'])
        pasted_text = st.text_area("ğŸ“„ ...æˆ–ç²˜è´´æ–‡æœ¬", height=100, key="paste_key")
        
        if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
            with st.status("æ­£åœ¨å¤„ç†...", expanded=True) as status:
                start_time = time.time()
                status.write("ğŸ“‚ è¯»å–æ–‡ä»¶å¹¶æ¸…æ´—åƒåœ¾è¯...")
                raw_text = logic.extract_text_from_file(uploaded_file) if uploaded_file else pasted_text
                
                if len(raw_text) > 2:
                    status.write("ğŸ” æ™ºèƒ½åˆ†æä¸è¯é¢‘æ¯”å¯¹...")
                    
                    # ç»Ÿä¸€è°ƒç”¨ï¼Œä¸å†åŒºåˆ†æ¨¡å¼
                    final_data, raw_count = logic.analyze_logic(raw_text, curr, targ, include_unknown)
                    
                    st.session_state['gen_words_data'] = final_data # [(word, rank), ...]
                    st.session_state['raw_count'] = raw_count
                    st.session_state['process_time'] = time.time() - start_time
                    
                    status.update(label="âœ… åˆ†æå®Œæˆ", state="complete", expanded=False)
                else:
                    status.update(label="âš ï¸ å†…å®¹å¤ªçŸ­", state="error")
        
        if st.button("ğŸ—‘ï¸ æ¸…ç©º", type="secondary", on_click=utils.clear_all_state): pass

    with mode_rank:
        gen_type = st.radio("æ¨¡å¼", ["ğŸ”¢ é¡ºåº", "ğŸ”€ éšæœº"], horizontal=True)
        if "é¡ºåº" in gen_type:
             c_a, c_b = st.columns(2)
             s_rank = c_a.number_input("èµ·å§‹æ’å", 1, 20000, 1000, step=100)
             count = c_b.number_input("æ•°é‡", 10, 500, 50, step=10)
             if st.button("ğŸš€ ç”Ÿæˆ"):
                 start_time = time.time()
                 if FULL_DF is not None:
                     r_col = next(c for c in FULL_DF.columns if 'rank' in c)
                     w_col = next(c for c in FULL_DF.columns if 'word' in c)
                     subset = FULL_DF[FULL_DF[r_col] >= s_rank].sort_values(r_col).head(count)
                     # æ„é€ ç»Ÿä¸€æ ¼å¼ [(word, rank), ...]
                     data_list = list(zip(subset[w_col], subset[r_col]))
                     st.session_state['gen_words_data'] = data_list
                     st.session_state['raw_count'] = 0
                     st.session_state['process_time'] = time.time() - start_time
        else:
             c_min, c_max, c_cnt = st.columns([1,1,1])
             min_r = c_min.number_input("Min Rank", 1, 20000, 1, step=100)
             max_r = c_max.number_input("Max Rank", 1, 25000, 5000, step=100)
             r_count = c_cnt.number_input("Count", 10, 200, 50, step=10)
             if st.button("ğŸ² æŠ½å–"):
                 start_time = time.time()
                 if FULL_DF is not None:
                     r_col = next(c for c in FULL_DF.columns if 'rank' in c)
                     w_col = next(c for c in FULL_DF.columns if 'word' in c)
                     mask = (FULL_DF[r_col] >= min_r) & (FULL_DF[r_col] <= max_r)
                     candidates = FULL_DF[mask]
                     if len(candidates) > 0:
                         subset = candidates.sample(n=min(r_count, len(candidates))).sort_values(r_col)
                         data_list = list(zip(subset[w_col], subset[r_col]))
                         st.session_state['gen_words_data'] = data_list
                         st.session_state['raw_count'] = 0
                         st.session_state['process_time'] = time.time() - start_time

    if 'gen_words_data' in st.session_state and st.session_state['gen_words_data']:
        # è§£åŒ…æ•°æ®
        data_pairs = st.session_state['gen_words_data']
        words_only = [p[0] for p in data_pairs]
        
        st.divider()
        st.markdown("### ğŸ“Š åˆ†ææŠ¥å‘Š")
        k1, k2, k3 = st.columns(3)
        raw_c = st.session_state.get('raw_count', 0)
        p_time = st.session_state.get('process_time', 0.1)
        k1.metric("ğŸ“„ æ–‡æ¡£æ€»å­—æ•°", f"{raw_c:,}")
        k2.metric("ğŸ¯ ç­›é€‰ç”Ÿè¯ (å·²å»é‡)", f"{len(words_only)}")
        k3.metric("âš¡ è€—æ—¶", f"{p_time:.2f}s")
        
        # --- V29: å¢å¼ºç‰ˆé¢„è§ˆåŒº (æŠ˜å +Rank) ---
        show_rank = st.checkbox("æ˜¾ç¤ºå•è¯ Rank", value=False)
        
        # æ„é€ æ˜¾ç¤ºæ–‡æœ¬
        if show_rank:
            display_text = ", ".join([f"{w}[{r}]" for w, r in data_pairs])
        else:
            display_text = ", ".join(words_only)
            
        with st.expander("ğŸ“‹ **å…¨éƒ¨ç”Ÿè¯é¢„è§ˆ (ç‚¹å‡»å±•å¼€/æŠ˜å )**", expanded=False):
            # ä½¿ç”¨è‡ªå®šä¹‰ CSS å®ç°æ»šåŠ¨å®¹å™¨
            st.markdown(f'<div class="scrollable-text">{display_text}</div>', unsafe_allow_html=True)
            st.caption("æç¤ºï¼šé•¿æŒ‰ä¸Šæ–¹æ–‡æœ¬æ¡†å¯å…¨é€‰å¤åˆ¶ï¼Œæˆ–ç‚¹å‡»ä¸‹æ–¹ä»£ç å—å¤åˆ¶æŒ‰é’®ã€‚")
            st.code(display_text, language="text")

        with st.expander("âš™ï¸ **è‡ªå®šä¹‰ Prompt è®¾ç½® (ç‚¹å‡»å±•å¼€)**", expanded=True):
            col_s1, col_s2 = st.columns(2)
            front_mode = col_s1.selectbox("æ­£é¢å†…å®¹", ["çŸ­è¯­æ­é… (Phrase)", "å•è¯ (Word)"])
            def_mode = col_s2.selectbox("èƒŒé¢é‡Šä¹‰", ["è‹±æ–‡", "ä¸­æ–‡", "ä¸­è‹±åŒè¯­"])
            
            col_s3, col_s4 = st.columns(2)
            ex_count = col_s3.slider("ä¾‹å¥æ•°é‡", 1, 3, 1)
            need_ety = col_s4.checkbox("åŒ…å«è¯æº/è¯æ ¹", value=True)

        batch_size = st.number_input("AI åˆ†ç»„å¤§å°", 10, 200, 100, step=10)
        batches = [words_only[i:i + batch_size] for i in range(0, len(words_only), batch_size)]
        
        for idx, batch in enumerate(batches):
            with st.expander(f"ğŸ“Œ ç¬¬ {idx+1} ç»„ (å…± {len(batch)} è¯)", expanded=(idx==0)):
                prompt_text = logic.get_ai_prompt(batch, front_mode, def_mode, ex_count, need_ety)
                st.caption("ğŸ“± æ‰‹æœºç«¯ä¸“ç”¨ï¼š")
                st.text_area(f"text_area_{idx}", value=prompt_text, height=100, label_visibility="collapsed")
                st.caption("ğŸ’» ç”µè„‘ç«¯ï¼š")
                st.code(prompt_text, language="text")

with tab_anki:
    st.markdown("### ğŸ“¦ åˆ¶ä½œ Anki")
    bj_time_str = utils.get_beijing_time_str()
    if 'anki_input_text' not in st.session_state: st.session_state['anki_input_text'] = ""

    st.caption("ğŸ‘‡ ç²˜è´´ AI å›å¤ï¼š")
    ai_resp = st.text_area("JSON è¾“å…¥æ¡†", height=300, key="anki_input_text")
    deck_name = st.text_input("ç‰Œç»„å", f"Vocab_{bj_time_str}")
    
    if ai_resp.strip():
        parsed_data = logic.parse_anki_data(ai_resp)
        if parsed_data:
            st.success(f"âœ… æˆåŠŸè§£æ {len(parsed_data)} æ¡æ•°æ®")
            df_view = pd.DataFrame(parsed_data)
            df_view.rename(columns={'front_phrase': 'æ­£é¢', 'meaning': 'èƒŒé¢', 'etymology': 'è¯æº'}, inplace=True)
            st.dataframe(df_view[['æ­£é¢', 'èƒŒé¢', 'è¯æº']], use_container_width=True, hide_index=True)
            
            f_path = logic.generate_anki_package(parsed_data, deck_name)
            with open(f_path, "rb") as f:
                st.download_button(f"ğŸ“¥ ä¸‹è½½ {deck_name}.apkg", f, file_name=f"{deck_name}.apkg", mime="application/octet-stream", type="primary")
        else:
            st.warning("âš ï¸ ç­‰å¾…ç²˜è´´...")