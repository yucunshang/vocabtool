import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import time  # å¼•å…¥æ—¶é—´åº“ç”¨äºè®¡æ—¶

# ==========================================
# 0. åŸºç¡€é…ç½®ä¸ CSS (é€‚é…æ‰‹æœº)
# ==========================================
st.set_page_config(
    page_title="Vocab Master", 
    page_icon="ğŸ“±", 
    layout="centered", 
    initial_sidebar_state="collapsed"
)

st.markdown("""
<style>
    /* ç•Œé¢ç´§å‡‘åŒ– */
    .block-container { padding-top: 1rem; padding-bottom: 3rem; }
    #MainMenu {visibility: hidden;} footer {visibility: hidden;} header {visibility: hidden;}
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    
    /* æŒ‰é’®ä¼˜åŒ–ï¼šå¤§å°ºå¯¸é€‚åˆæ‰‹æŒ‡ç‚¹å‡» */
    .stButton>button {
        width: 100%; border-radius: 12px; height: 3.5em; font-weight: bold; font-size: 16px !important;
        margin-top: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    
    /* æ–‡æœ¬æ¡†ä¼˜åŒ–ï¼šæ–¹ä¾¿å¤åˆ¶ */
    .stTextArea textarea { font-size: 14px !important; border-radius: 10px; }
    
    /* è®¾ç½®æ æ ·å¼ */
    [data-testid="stExpander"] { border-radius: 10px; border: 1px solid #ddd; margin-bottom: 20px; }
    
    /* æ ‡ç­¾é¡µä¼˜åŒ– */
    .stTabs [data-baseweb="tab-list"] { gap: 10px; }
    .stTabs [data-baseweb="tab"] { height: 50px; white-space: pre-wrap; background-color: #f0f2f6; border-radius: 5px; }
    .stTabs [aria-selected="true"] { background-color: #ff4b4b !important; color: white !important; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºåŠ è½½
# ==========================================
@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    try: 
        nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_dir, quiet=True)
        nltk.download('punkt', download_dir=nltk_data_dir, quiet=True)
    except: pass
setup_nltk()

@st.cache_data
def load_data():
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c), cols[1])
            
            # æ¸…æ´—ä¸ç±»å‹è½¬æ¢
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            df = df.dropna(subset=[r_col])
            df = df.sort_values(r_col)
            
            # å­—å…¸ï¼šç”¨äºå¿«é€ŸæŸ¥è¯¢ Rank
            vocab_dict = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
            return vocab_dict, df, r_col, w_col
        except Exception as e:
            st.error(f"æ•°æ®åŠ è½½å‡ºé”™: {e}")
            return {}, None, None, None
    return {}, None, None, None

VOCAB_DICT, FULL_DF, RANK_COL, WORD_COL = load_data()
def get_lemma(word): return lemminflect.getLemma(word, upos='VERB')[0] 

# ==========================================
# 2. åŠ¨æ€ Prompt ç”Ÿæˆå™¨
# ==========================================
def generate_dynamic_prompt(words, settings):
    # å¦‚æœ words åˆ—è¡¨é‡ŒåŒ…å« rank (ä¾‹å¦‚ "apple (1000)"), æ¸…æ´—æ‰ rank åªç•™å•è¯ç»™ AI
    clean_words = [w.split(' (')[0] for w in words]
    word_list_str = ", ".join(clean_words)
    
    fmt = settings.get("format", "CSV")
    ex_count = settings.get("example_count", 1)
    lang = settings.get("lang", "Chinese")
    
    prompt = f"""Role: High-Efficiency Anki Card Creator
Task: Convert the provided word list into a strict {fmt} data block.

--- OUTPUT FORMAT RULES ---
1. Structure: {'2 Columns (Front, Back)' if fmt=='CSV' else 'Custom Text Format'}.
   Format: "Front","Back"
   Header: **Do NOT output a header row.**

2. Column 1 (Front):
   - Content: A natural, short English phrase/collocation.
   - Style: **ALL LOWERCASE**.

3. Column 2 (Back):
   - Content: Definition + {ex_count} Example(s) + Etymology.
   - HTML Layout: Definition <br> <br> <em>Example</em> <br> <br> ã€æºã€‘Etymology
   - Definition Language: {lang} & English concise (Start with lowercase).
   - Example Style: **Start with UPPERCASE** (Normal sentence case). Wrapped in <em>.
   - Spacing: Double <br> tags.

4. Etymology Style:
   - Only explain roots/affixes in {lang}.
   - Format: ã€æºã€‘Root (Meaning) + Affix (Meaning)
   - Do NOT explain the final word meaning.

5. Atomicity: Separate rows for distinct meanings.

--- WORD LIST ---
{word_list_str}
"""
    return prompt

# ==========================================
# 3. è¾…åŠ©åŠŸèƒ½
# ==========================================
def extract_text(file_obj):
    try:
        ext = file_obj.name.split('.')[-1].lower()
        if ext == 'txt': return file_obj.getvalue().decode("utf-8", errors="ignore")
        elif ext == 'pdf':
            import PyPDF2; reader = PyPDF2.PdfReader(file_obj)
            return " ".join([p.extract_text() for p in reader.pages if p.extract_text()])
        elif ext == 'docx':
            import docx; doc = docx.Document(file_obj)
            return " ".join([p.text for p in doc.paragraphs])
    except: return ""
    return ""

def classify_words(text, current_lvl, target_lvl):
    raw_words = re.findall(r"[a-zA-Z']+", text)
    lemmas = set(get_lemma(w).lower() for w in raw_words if len(w)>=2)
    
    mastered, target, beyond = [], [], []
    
    for w in lemmas:
        rank = VOCAB_DICT.get(w, 99999) 
        
        if rank <= current_lvl:
            mastered.append((w, rank))
        elif current_lvl < rank <= target_lvl:
            target.append((w, rank))
        else:
            beyond.append((w, rank))
            
    # æ’åº
    mastered.sort(key=lambda x: x[1])
    target.sort(key=lambda x: x[1])
    beyond.sort(key=lambda x: x[1])
    
    return [x[0] for x in mastered], [x[0] for x in target], [x[0] for x in beyond], mastered, target, beyond

# è¾…åŠ©æ ¼å¼åŒ–å‡½æ•°ï¼šæ˜¯å¦å¸¦ Rank
def format_list(word_tuple_list, show_rank=False):
    if show_rank:
        return [f"{w} ({r})" for w, r in word_tuple_list]
    else:
        return [w for w, r in word_tuple_list]

# ==========================================
# 4. ä¸»ç•Œé¢é€»è¾‘
# ==========================================
st.title("âš¡ï¸ Vocab Master")

if FULL_DF is None:
    st.error("âš ï¸ ç¼ºå°‘è¯é¢‘æ–‡ä»¶ (coca_cleaned.csv)")
else:
    # --- å…¨å±€è®¾ç½® ---
    with st.expander("âš™ï¸ ç”Ÿæˆè®¾ç½® (Prompt Settings)", expanded=False):
        c1, c2 = st.columns(2)
        with c1:
            set_format = st.selectbox("å¯¼å‡ºæ ¼å¼", ["CSV", "TXT"], index=0)
            set_lang = st.selectbox("é‡Šä¹‰è¯­è¨€", ["Chinese", "English"], index=0)
        with c2:
            set_ex_count = st.number_input("ä¾‹å¥æ•°é‡", 1, 3, 1)
            set_case = st.selectbox("é£æ ¼", ["Front:Phrase (Lower)", "Front:Word"], index=0)
    
    global_settings = {"format": set_format, "lang": set_lang, "example_count": set_ex_count}

    # --- å¯¼èˆª ---
    mode = st.radio("åŠŸèƒ½æ¨¡å¼", ["ğŸ”¢ è¯é¢‘åˆ·è¯", "ğŸ“– æ–‡æœ¬æå–", "ğŸ› ï¸ æ ¼å¼è½¬æ¢"], horizontal=True, label_visibility="collapsed")
    
    # ------------------------------------------------
    # æ¨¡å¼ 1: åˆ·è¯
    # ------------------------------------------------
    if mode == "ğŸ”¢ è¯é¢‘åˆ·è¯":
        st.caption("æŒ‰æ’åæ‰¹é‡ç”Ÿæˆå•è¯å¡")
        c1, c2 = st.columns(2)
        with c1: start_rank = st.number_input("èµ·å§‹æ’å", 8000, step=50)
        with c2: count = st.number_input("ç”Ÿæˆæ•°é‡", 50, step=10)
        
        # ğŸŸ¢ Rank å¼€å…³
        show_rank_mode1 = st.checkbox("æ˜¾ç¤ºæ’å (Show Rank)", value=False, key="rk_m1")

        filtered = FULL_DF[FULL_DF[RANK_COL] >= start_rank].sort_values(RANK_COL).head(count)
        
        # å‡†å¤‡æ•°æ®
        raw_words = filtered[WORD_COL].tolist()
        ranks = filtered[RANK_COL].tolist()
        
        # ç»„åˆæ˜¾ç¤º
        display_list = []
        for w, r in zip(raw_words, ranks):
            if show_rank_mode1:
                display_list.append(f"{w} ({int(r)})")
            else:
                display_list.append(w)
        
        if display_list:
            real_range = f"{int(filtered.iloc[0][RANK_COL])}-{int(filtered.iloc[-1][RANK_COL])}"
            st.info(f"æå– {len(display_list)} ä¸ªå•è¯ ({real_range})")
            
            st.text_area("ğŸ“‹ å•è¯åˆ—è¡¨ (å¯å¤åˆ¶)", ", ".join(display_list), height=100)
            
            # ğŸŸ¢ è­¦å‘Šæç¤º
            if len(display_list) > 300:
                st.warning("âš ï¸ å•è¯æ•°é‡è¾ƒå¤šï¼ŒAI å¯èƒ½ä¼šæˆªæ–­è¾“å‡ºï¼Œå»ºè®®åˆ†æ‰¹ç”Ÿæˆ (æ¯æ¬¡ < 200)ã€‚")

            if st.button("ğŸš€ ç”Ÿæˆ Prompt", type="primary"):
                # æ³¨æ„ï¼šä¼ ç»™ Prompt çš„æ°¸è¿œæ˜¯ä¸å¸¦ Rank çš„çº¯å•è¯
                prompt = generate_dynamic_prompt(raw_words, global_settings)
                st.code(prompt, language="markdown")
                st.success("ç‚¹å‡»å³ä¸Šè§’å¤åˆ¶ -> å‘ç»™ AI")
        else:
            st.warning("æ— æ•°æ®")

    # ------------------------------------------------
    # æ¨¡å¼ 2: æå– (Extract)
    # ------------------------------------------------
    elif mode == "ğŸ“– æ–‡æœ¬æå–":
        st.caption("åˆ†ææ–‡ç« ï¼ŒæŒ‰è¯æ±‡é‡åˆ†çº§")
        
        col_a, col_b = st.columns(2)
        with col_a: curr_lvl = st.number_input("å½“å‰æ°´å¹³ (Current)", value=4000, step=500)
        with col_b: targ_lvl = st.number_input("ç›®æ ‡æ°´å¹³ (Target)", value=8000, step=500)
        
        inp_type = st.radio("Input", ["ç²˜è´´æ–‡æœ¬", "ä¸Šä¼ æ–‡ä»¶"], horizontal=True, label_visibility="collapsed")
        raw_text = ""
        if inp_type == "ç²˜è´´æ–‡æœ¬":
            raw_text = st.text_area("åœ¨æ­¤ç²˜è´´", height=150)
        else:
            up = st.file_uploader("æ”¯æŒ TXT/PDF/DOCX", type=["txt","pdf","docx"])
            if up: raw_text = extract_text(up)
            
        # ğŸŸ¢ Rank å¼€å…³ (åœ¨ç”Ÿæˆå‰ä¹Ÿå¯ä»¥é€‰ï¼Œæˆ–è€…ç”Ÿæˆåé€‰)
        show_rank_extract = st.checkbox("åœ¨åˆ—è¡¨ä¸­æ˜¾ç¤ºæ’å (Show Rank)", value=False, key="rk_ext")

        if raw_text and st.button("ğŸ” åˆ†æå•è¯", type="primary"):
            # ğŸŸ¢ è¿›åº¦åé¦ˆ + è®¡æ—¶
            with st.spinner("æ­£åœ¨åˆ†ææ–‡æœ¬ä¸è¯é¢‘..."):
                t0 = time.time()
                # æ ¸å¿ƒåˆ†æé€»è¾‘
                w_m_clean, w_t_clean, w_b_clean, w_m_tuples, w_t_tuples, w_b_tuples = classify_words(raw_text, curr_lvl, targ_lvl)
                t1 = time.time()
            
            st.success(f"âœ… åˆ†æå®Œæˆï¼è€—æ—¶ {t1-t0:.2f} ç§’")
            
            # æ ¹æ®å¼€å…³æ ¼å¼åŒ–åˆ—è¡¨
            list_target = format_list(w_t_tuples, show_rank_extract)
            list_mastered = format_list(w_m_tuples, show_rank_extract)
            list_beyond = format_list(w_b_tuples, show_rank_extract)

            tab1, tab2, tab3 = st.tabs([
                f"ğŸ¯ é‡ç‚¹ ({len(list_target)})", 
                f"âœ… å·²æŒæ¡ ({len(list_mastered)})", 
                f"ğŸš€ è¶…çº² ({len(list_beyond)})"
            ])
            
            # --- Tab 1: é‡ç‚¹ ---
            with tab1:
                if list_target:
                    st.success("æ ¸å¿ƒèƒŒè¯µåŒº")
                    with st.expander("ğŸ“‹ å±•å¼€/å¤åˆ¶åˆ—è¡¨", expanded=True):
                        st.text_area("Target Words", ", ".join(list_target), height=150, key="txt_target")
                    
                    if len(list_target) > 200:
                        st.warning("âš ï¸ é‡ç‚¹è¯è¶…è¿‡ 200 ä¸ªï¼Œå»ºè®®åˆ†æ‰¹å¤åˆ¶ç»™ AIã€‚")

                    if st.button("ğŸš€ ä¸ºé‡ç‚¹è¯ç”Ÿæˆ Prompt"):
                        prompt = generate_dynamic_prompt(w_t_clean, global_settings)
                        st.code(prompt, language="markdown")
                else:
                    st.info("æ­¤åŒºé—´æ— å•è¯")

            # --- Tab 2: å·²æŒæ¡ ---
            with tab2:
                if list_mastered:
                    st.caption("ä½äºå½“å‰è¯æ±‡é‡çš„è¯")
                    with st.expander("ğŸ“‹ å±•å¼€/å¤åˆ¶åˆ—è¡¨"):
                        st.text_area("Mastered Words", ", ".join(list_mastered), height=150, key="txt_mastered")
                else: st.write("æ— ")

            # --- Tab 3: è¶…çº² ---
            with tab3:
                if list_beyond:
                    st.caption("é«˜äºç›®æ ‡è¯æ±‡é‡æˆ–ç”Ÿåƒ»è¯")
                    with st.expander("ğŸ“‹ å±•å¼€/å¤åˆ¶åˆ—è¡¨"):
                        st.text_area("Beyond Words", ", ".join(list_beyond), height=150, key="txt_beyond")
                else: st.write("æ— ")

    # ------------------------------------------------
    # æ¨¡å¼ 3: è½¬æ¢
    # ------------------------------------------------
    elif mode == "ğŸ› ï¸ æ ¼å¼è½¬æ¢":
        st.markdown("### ğŸ“¥ AI ç»“æœè½¬ Anki CSV")
        st.caption("ç²˜è´´ AI è¿”å›çš„çº¯æ•°æ® (No Header)ï¼Œè‡ªåŠ¨ä¸‹è½½")
        
        csv_in = st.text_area("ç²˜è´´å†…å®¹", height=200, placeholder='"phrase","def..."')
        
        if csv_in:
            csv_str = csv_in.strip()
            st.download_button(
                "ğŸ“¥ ä¸‹è½½ .csv (çº¯æ•°æ®)",
                csv_str.encode('utf-8'),
                "anki_import.csv",
                "text/csv",
                type="primary"
            )