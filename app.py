# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import re
import os
import random
import json
import time
from datetime import datetime, timedelta, timezone

# ==========================================
# 0. é¡µé¢é…ç½® (Page Config)
# ==========================================
st.set_page_config(
    page_title="Vocab Flow Ultra",
    page_icon="âš¡ï¸",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# è‡ªå®šä¹‰ CSS æ ·å¼
st.markdown("""
<style>
    .stTextArea textarea { font-family: 'Consolas', monospace; font-size: 14px; }
    .stButton>button { border-radius: 8px; font-weight: 600; width: 100%; margin-top: 5px; }
    .stat-box { padding: 15px; background-color: #f0fdf4; border: 1px solid #bbf7d0; border-radius: 8px; text-align: center; color: #166534; margin-bottom: 20px; }
    .scrollable-text {
        max-height: 250px;
        overflow-y: auto;
        padding: 10px;
        border: 1px solid #eee;
        border-radius: 5px;
        background-color: #fafafa;
        font-family: monospace;
        white-space: pre-wrap;
        font-size: 13px;
    }
    .guide-step { background-color: #f8f9fa; padding: 15px; border-radius: 8px; margin-bottom: 15px; border-left: 4px solid #0056b3; }
    .guide-title { font-weight: bold; color: #0f172a; display: block; margin-bottom: 5px; }
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ– Session State
if 'uploader_id' not in st.session_state:
    st.session_state['uploader_id'] = "1000"

# ==========================================
# 1. èµ„æºæ‡’åŠ è½½ (Resource Loading)
# ==========================================

@st.cache_resource(show_spinner="æ­£åœ¨åŠ è½½ NLP å¼•æ“ (é¦–æ¬¡è¿è¡Œè¾ƒæ…¢)...")
def load_nlp_resources():
    """
    æ‡’åŠ è½½ NLTK å’Œ Lemminflect ä»¥æå‡å¯åŠ¨é€Ÿåº¦
    """
    import nltk
    import lemminflect
    
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    
    # éœ€è¦çš„ NLTK åŒ…
    required_packages = ['averaged_perceptron_tagger', 'punkt', 'punkt_tab', 'wordnet']
    
    for pkg in required_packages:
        try:
            nltk.data.find(f'tokenizers/{pkg}')
        except LookupError:
            try:
                nltk.data.find(f'taggers/{pkg}')
            except LookupError:
                try:
                    nltk.data.find(f'corpora/{pkg}')
                except LookupError:
                    nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
    
    return nltk, lemminflect

@st.cache_data
def load_vocab_data():
    """
    åŠ è½½ COCA è¯é¢‘è¡¨ã€‚è¿”å› {word: rank} å­—å…¸å’Œå®Œæ•´ DataFrameã€‚
    """
    # æ–‡ä»¶åä¼˜å…ˆçº§
    possible_files = ["coca_cleaned.csv", "vocab.csv", "data.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    
    if file_path:
        try:
            df = pd.read_csv(file_path)
            # è§„èŒƒåŒ–åˆ—å
            df.columns = [c.strip().lower() for c in df.columns]
            
            # åŠ¨æ€è¯†åˆ«åˆ—
            w_col = next((c for c in df.columns if 'word' in c), None)
            r_col = next((c for c in df.columns if 'rank' in c), None)
            
            if not w_col or not r_col:
                return {}, None

            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            
            # å»é‡ï¼šä¿ç•™æ’åæœ€é«˜çš„ï¼ˆæ•°å€¼æœ€å°çš„ï¼‰
            df = df.sort_values(r_col).drop_duplicates(subset=[w_col], keep='first')
            
            vocab_dict = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
            return vocab_dict, df
        except Exception as e:
            st.error(f"è¯»å–è¯æ±‡è¡¨é”™è¯¯: {e}")
            return {}, None
    return {}, None

# å…¨å±€åŠ è½½ä¸€æ¬¡æ•°æ®
VOCAB_DICT, FULL_DF = load_vocab_data()

def get_beijing_time_str():
    """è·å–æ ¼å¼åŒ–çš„åŒ—äº¬æ—¶é—´å­—ç¬¦ä¸²"""
    utc_now = datetime.now(timezone.utc)
    beijing_now = utc_now + timedelta(hours=8)
    return beijing_now.strftime('%m%d_%H%M')

def clear_all_state():
    """å¼ºåˆ¶é‡ç½® Session State"""
    keys_to_drop = ['gen_words_data', 'raw_count', 'process_time', 'anki_input_text']
    for k in keys_to_drop:
        if k in st.session_state:
            del st.session_state[k]
    
    # éšæœºåŒ– uploader key ä»¥å¼ºåˆ¶é‡ç½®ä¸Šä¼ ç»„ä»¶
    st.session_state['uploader_id'] = str(random.randint(100000, 999999))
    
    if 'paste_key' in st.session_state:
        st.session_state['paste_key'] = ""

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ï¼šæå–ä¸åˆ†æ (Core Logic)
# ==========================================

def extract_text_from_file(uploaded_file):
    """è§£æ PDF, DOCX, EPUB, TXT"""
    import pypdf
    import docx
    import ebooklib
    from ebooklib import epub
    from bs4 import BeautifulSoup
    
    text = ""
    file_ext = uploaded_file.name.split('.')[-1].lower()
    
    try:
        if file_ext == 'txt':
            bytes_data = uploaded_file.getvalue()
            # å°è¯•å¸¸è§ç¼–ç 
            for encoding in ['utf-8', 'gb18030', 'latin-1']:
                try:
                    text = bytes_data.decode(encoding)
                    break
                except UnicodeDecodeError:
                    continue
                    
        elif file_ext == 'pdf':
            reader = pypdf.PdfReader(uploaded_file)
            text_parts = []
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text_parts.append(extracted)
            text = "\n".join(text_parts)
            
        elif file_ext == 'docx':
            doc = docx.Document(uploaded_file)
            text = "\n".join([p.text for p in doc.paragraphs])
            
        elif file_ext == 'epub':
            # å¤„ç† EPUB éœ€è¦ä¸´æ—¶æ–‡ä»¶
            with open("temp.epub", "wb") as f:
                f.write(uploaded_file.getvalue())
            
            book = epub.read_epub("temp.epub")
            text_parts = []
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    text_parts.append(soup.get_text(separator=' ', strip=True))
            text = " ".join(text_parts)
            if os.path.exists("temp.epub"):
                os.remove("temp.epub")
                
    except Exception as e:
        return f"è¯»å–æ–‡ä»¶é”™è¯¯: {str(e)}"
        
    return text

def is_valid_word(word):
    """å¯å‘å¼æ¸…æ´—ï¼šå»é™¤åƒåœ¾è¯"""
    if len(word) < 2: return False
    if len(word) > 25: return False
    # è¿‡æ»¤è¿ç»­3ä¸ªç›¸åŒå­—ç¬¦
    if re.search(r'(.)\1{2,}', word): return False
    # å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªå…ƒéŸ³ (è‹±æ–‡å¯å‘å¼è§„åˆ™)
    if not re.search(r'[aeiouy]', word): return False
    # ä¸å…è®¸åŒ…å«æ•°å­—æˆ–ä¸‹åˆ’çº¿
    if re.search(r'[0-9_]', word): return False
    return True

def analyze_logic(text, min_rank, max_rank, include_unknown):
    """
    æ ¸å¿ƒç®—æ³•: åˆ†è¯ -> è¯å½¢è¿˜åŸ -> æ’åæ£€æŸ¥ -> å»é‡
    è¿”å›: [(word, rank), ...], raw_word_count
    """
    nltk, lemminflect = load_nlp_resources()
    
    # 1. åˆ†è¯ (ä¿ç•™å†…éƒ¨è¿å­—ç¬¦å¦‚ 'well-known')
    raw_tokens = re.findall(r"[a-zA-Z]+(?:[-'][a-zA-Z]+)*", text)
    total_words = len(raw_tokens)
    
    # 2. åˆæ­¥æ¸…æ´—
    clean_tokens = set([t.lower() for t in raw_tokens if is_valid_word(t.lower())])
    
    final_candidates = [] 
    seen_lemmas = set()
    
    for w in clean_tokens:
        # è·å– Lemma (è¯åŸ)ï¼Œä¾‹å¦‚ went -> go
        # ä¼˜å…ˆå°è¯• VERBï¼Œå› ä¸ºå˜åŒ–æœ€å¤š
        try:
            lemma = lemminflect.getLemma(w, upos='VERB')[0]
        except:
            lemma = w
            
        # è·å–æ’å
        rank_lemma = VOCAB_DICT.get(lemma, 99999)
        rank_orig = VOCAB_DICT.get(w, 99999)
        
        # ç¡®å®šæœ‰æ•ˆæ’å (å–ä¸¤è€…ä¸­è¾ƒå°/é å‰çš„)
        best_rank = min(rank_lemma, rank_orig)
        
        # ç¡®å®šè¾“å‡ºå•è¯ (å¦‚æœ Lemma æœ‰æ•ˆåˆ™ä¼˜å…ˆè¾“å‡º Lemma)
        word_to_keep = lemma if rank_lemma != 99999 else w
        
        # è¿‡æ»¤é€»è¾‘
        is_in_range = (min_rank <= best_rank <= max_rank)
        is_unknown_included = (include_unknown and best_rank == 99999)
        
        if is_in_range or is_unknown_included:
            # å»é‡ï¼šä½¿ç”¨ lemma ä½œä¸º key
            # ç¡®ä¿ 'go' å’Œ 'went' ä¸ä¼šåŒæ—¶å‡ºç°
            dedupe_key = lemma
            
            if dedupe_key not in seen_lemmas:
                final_candidates.append((word_to_keep, best_rank))
                seen_lemmas.add(dedupe_key)
    
    # æ’åº: é«˜é¢‘(ä½ rank) -> ä½é¢‘ -> æœªçŸ¥
    final_candidates.sort(key=lambda x: x[1])
    
    return final_candidates, total_words

# ==========================================
# 3. Anki è§£æä¸ç”Ÿæˆ (Anki Generation)
# ==========================================

def parse_anki_data(raw_text):
    """
    ä» AI å›å¤ä¸­æå– JSON å¯¹è±¡ã€‚
    è¾“å…¥: å¯èƒ½åŒ…å« markdownã€æ–‡æœ¬å’Œå¤šä¸ª JSON å¯¹è±¡çš„å­—ç¬¦ä¸²ã€‚
    è¾“å‡º: å­—å…¸åˆ—è¡¨ã€‚
    """
    parsed_cards = []
    # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
    text = raw_text.replace("```json", "").replace("```", "").strip()
    
    # æ­£åˆ™åŒ¹é… JSON ç»“æ„ { ... }
    matches = re.finditer(r'\{.*?\}', text, re.DOTALL)
    seen_phrases = set()

    for match in matches:
        json_str = match.group()
        try:
            data = json.loads(json_str, strict=False)
            
            # æå–å­—æ®µï¼Œå¸¦é»˜è®¤å€¼
            front = str(data.get("w", "")).strip()
            meaning = str(data.get("m", "")).strip()
            examples = str(data.get("e", "")).strip()
            etymology = str(data.get("r", "")).strip()
            
            if etymology.lower() in ["none", "", "null"]:
                etymology = ""

            # åŸºç¡€éªŒè¯
            if not front or not meaning:
                continue
            
            # ç§»é™¤æ­£é¢å¯èƒ½å­˜åœ¨çš„ markdown åŠ ç²—
            front = front.replace('**', '')
            
            # æ‰¹æ¬¡å†…å»é‡
            if front.lower() in seen_phrases:
                continue
            seen_phrases.add(front.lower())

            parsed_cards.append({
                'front': front,
                'back': meaning,
                'examples': examples,
                'etymology': etymology
            })
        except json.JSONDecodeError:
            continue
            
    return parsed_cards

def generate_anki_package(cards_data, deck_name):
    """ä½¿ç”¨ genanki ç”Ÿæˆ .apkg æ–‡ä»¶"""
    import genanki
    import tempfile
    
    # å¡ç‰‡ CSS æ ·å¼
    CSS = """
    .card { font-family: 'Arial', sans-serif; font-size: 20px; text-align: center; color: #333; background-color: white; padding: 20px; }
    .nightMode .card { background-color: #2e2e2e; color: #f0f0f0; }
    .phrase { font-size: 28px; font-weight: 700; color: #0056b3; margin-bottom: 20px; }
    .definition { font-weight: bold; color: #222; margin-bottom: 15px; font-size: 20px; text-align: left; }
    .nightMode .definition { color: #e0e0e0; }
    .examples { background: #f7f9fa; padding: 12px; border-left: 4px solid #0056b3; font-style: italic; font-size: 18px; text-align: left; margin-bottom: 15px; }
    .nightMode .examples { background: #383838; color: #ccc; border-left-color: #66b0ff; }
    .etymology { font-size: 16px; color: #555; background-color: #fffdf5; padding: 10px; border: 1px solid #fef3c7; border-radius: 6px; text-align: left; }
    .nightMode .etymology { background-color: #333; color: #aaa; border-color: #444; }
    """
    
    # åˆ›å»ºå”¯ä¸€ Model ID
    model_id = random.randrange(1 << 30, 1 << 31)
    
    model = genanki.Model(
        model_id,
        f'VocabFlow Model {model_id}',
        fields=[
            {'name': 'Front'}, 
            {'name': 'Meaning'}, 
            {'name': 'Examples'}, 
            {'name': 'Etymology'}
        ],
        templates=[{
            'name': 'Standard Card',
            'qfmt': '<div class="phrase">{{Front}}</div>', 
            'afmt': '''
            {{FrontSide}}<hr>
            <div class="definition">{{Meaning}}</div>
            <div class="examples">{{Examples}}</div>
            {{#Etymology}}
            <div class="etymology">ğŸŒ± <b>Origin:</b> {{Etymology}}</div>
            {{/Etymology}}
            ''',
        }],
        css=CSS
    )
    
    deck = genanki.Deck(random.randrange(1 << 30, 1 << 31), deck_name)
    
    for c in cards_data:
        note = genanki.Note(
            model=model,
            fields=[
                c['front'], 
                c['back'], 
                c['examples'].replace('\n','<br>'), 
                c['etymology']
            ]
        )
        deck.add_note(note)
        
    with tempfile.NamedTemporaryFile(delete=False, suffix='.apkg') as tmp:
        genanki.Package(deck).write_to_file(tmp.name)
        return tmp.name

# ==========================================
# 4. Prompt Engineering (æç¤ºè¯ç”Ÿæˆ)
# ==========================================

def get_ai_prompt(words, front_mode, def_mode, ex_count, need_ety):
    w_list = ", ".join(words)
    
    # å¯é…ç½®çš„æŒ‡ä»¤
    w_instr = "Key `w`: The word itself (lemma)." if "å•è¯" in front_mode else "Key `w`: A common short phrase/collocation using the word."
    
    if def_mode == "ä¸­æ–‡":
        m_instr = "Key `m`: Concise Chinese definition."
    elif def_mode == "ä¸­è‹±åŒè¯­":
        m_instr = "Key `m`: English Definition <br> Chinese Definition."
    else:
        m_instr = "Key `m`: Simple English definition."

    e_instr = f"Key `e`: {ex_count} native example sentence(s). Use <br> for line breaks."
    r_instr = "Key `r`: Etymology/Root explanation (in Chinese)." if need_ety else "Key `r`: Empty string."

    return f"""
Task: Create high-quality Anki flashcards (JSON format).
Words to process: {w_list}

**Format:** NDJSON (Newline Delimited JSON). Do not use lists. One JSON object per line.

**Field Requirements:**
1. {w_instr}
2. {m_instr}
3. {e_instr}
4. {r_instr}

**Output keys:** `w`, `m`, `e`, `r`

**Example:**
{{"w": "example", "m": "an instance serving to illustrate", "e": "This is a good example.", "r": "from Latin exemplum"}}

**Start:**
"""

# ==========================================
# 5. UI å¸ƒå±€ä¸ä¸»ç¨‹åº (Main Execution)
# ==========================================

st.title("âš¡ï¸ Vocab Flow Ultra")

# æ£€æŸ¥ CSV æ–‡ä»¶
if not VOCAB_DICT:
    st.warning("âš ï¸ æœªæ‰¾åˆ°è¯é¢‘è¡¨æ–‡ä»¶ï¼è¯·å°† `coca_cleaned.csv` æ”¾å…¥æ ¹ç›®å½•ï¼Œå¦åˆ™è¯é¢‘ç­›é€‰åŠŸèƒ½å°†å¤±æ•ˆã€‚")

# æ ‡ç­¾é¡µ
tab_guide, tab_extract, tab_anki = st.tabs(["ğŸ“– ä½¿ç”¨æŒ‡å—", "1ï¸âƒ£ å•è¯æå–", "2ï¸âƒ£ Anki åˆ¶ä½œ"])

# --- Tab 1: æŒ‡å— ---
with tab_guide:
    st.markdown("""
    <div class="guide-step">
    <span class="guide-title">æ­¥éª¤ 1: æå– (Extract)</span>
    ä¸Šä¼ æ–‡æ¡£ (PDF, DOCX, EPUB, TXT) æˆ–ç²˜è´´æ–‡æœ¬ã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨æ¸…æ´—æ–‡æœ¬ï¼Œè¿˜åŸè¯å½¢ (<i>went -> go</i>)ï¼Œå¹¶æ ¹æ® COCA è¯é¢‘è¡¨è¿›è¡Œç­›é€‰ã€‚
    </div>
    
    <div class="guide-step">
    <span class="guide-title">æ­¥éª¤ 2: ç”Ÿæˆ Prompts (Generate)</span>
    ç³»ç»Ÿä¼šå°†å•è¯åˆ†ç»„ã€‚å¤åˆ¶ç”Ÿæˆçš„ Prompt å‘é€ç»™ AI (ChatGPT, Claude ç­‰)ã€‚
    </div>
    
    <div class="guide-step">
    <span class="guide-title">æ­¥éª¤ 3: åˆ¶ä½œ Anki (Create)</span>
    å°† AI è¿”å›çš„ JSON ç²˜è´´å› "Anki åˆ¶ä½œ" æ ‡ç­¾é¡µï¼Œå³å¯ç”Ÿæˆ <code>.apkg</code> æ–‡ä»¶ã€‚
    </div>
    """, unsafe_allow_html=True)

# --- Tab 2: æå– ---
with tab_extract:
    col1, col2 = st.columns(2)
    with col1:
        # é»˜è®¤ 8000ï¼Œæ­¥é•¿ 500
        min_r = st.number_input("å¿½ç•¥æ’åå‰ N çš„è¯ (Min Rank)", min_value=1, max_value=20000, value=8000, step=500, help="æ’åé«˜äºæ­¤ï¼ˆå¦‚ the, isï¼‰çš„å¸¸ç”¨è¯å°†è¢«å¿½ç•¥ã€‚")
    with col2:
        # é»˜è®¤ 15000ï¼Œæ­¥é•¿ 500
        max_r = st.number_input("å¿½ç•¥æ’åå N çš„è¯ (Max Rank)", min_value=1, max_value=50000, value=15000, step=500, help="æ’åä½äºæ­¤çš„ç”Ÿåƒ»è¯å°†è¢«å¿½ç•¥ã€‚")
    
    include_unknown = st.checkbox("ğŸ”“ åŒ…å«ç”Ÿåƒ»è¯/äººå (Rank > 20000)", value=False)
    
    # æ–‡ä»¶è¾“å…¥
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡ä»¶", type=['txt', 'pdf', 'docx', 'epub'], key=st.session_state['uploader_id'])
    pasted_text = st.text_area("ğŸ“„ æˆ–ç²˜è´´æ–‡æœ¬", height=100, key="paste_key")
    
    # æŒ‰é’®
    c_btn1, c_btn2 = st.columns([1, 4])
    with c_btn1:
        clear_btn = st.button("ğŸ—‘ï¸ æ¸…ç©º", on_click=clear_all_state)
    with c_btn2:
        analyze_btn = st.button("ğŸš€ å¼€å§‹åˆ†æä¸æå–", type="primary")

    if analyze_btn:
        text_content = ""
        if uploaded_file:
            with st.spinner("æ­£åœ¨è¯»å–æ–‡ä»¶..."):
                text_content = extract_text_from_file(uploaded_file)
        elif pasted_text:
            text_content = pasted_text
        
        if len(text_content.strip()) > 5:
            start_time = time.time()
            with st.status("æ­£åœ¨å¤„ç† NLP...", expanded=True) as status:
                status.write("ğŸ” åˆ†è¯ä¸è¯å½¢è¿˜åŸä¸­...")
                data, raw_count = analyze_logic(text_content, min_r, max_r, include_unknown)
                status.write(f"âœ… æ‰¾åˆ° {len(data)} ä¸ªç”Ÿè¯ã€‚")
                
                st.session_state['gen_words_data'] = data
                st.session_state['raw_count'] = raw_count
                st.session_state['process_time'] = time.time() - start_time
                status.update(label="åˆ†æå®Œæˆ", state="complete", expanded=False)
        else:
            st.error("âš ï¸ è¯·æä¾›æœ‰æ•ˆæ–‡æœ¬æˆ–æ–‡ä»¶ã€‚")

    # ç»“æœæ˜¾ç¤º
    if 'gen_words_data' in st.session_state and st.session_state['gen_words_data']:
        data_pairs = st.session_state['gen_words_data']
        words_only = [p[0] for p in data_pairs]
        
        st.divider()
        # æŒ‡æ ‡
        m1, m2, m3 = st.columns(3)
        m1.metric("åŸæ–‡æ€»è¯æ•°", f"{st.session_state['raw_count']:,}")
        m2.metric("ç›®æ ‡ç”Ÿè¯æ•°", f"{len(words_only)}")
        m3.metric("è€—æ—¶", f"{st.session_state['process_time']:.2f}s")
        
        # é¢„è§ˆ
        with st.expander("ğŸ“‹ ç”Ÿè¯åˆ—è¡¨é¢„è§ˆ", expanded=False):
            show_rank = st.toggle("æ˜¾ç¤ºæ’å (Rank)")
            preview_str = ", ".join([f"{w} ({r})" if show_rank else w for w, r in data_pairs])
            st.markdown(f'<div class="scrollable-text">{preview_str}</div>', unsafe_allow_html=True)
            st.button("ğŸ“‹ å¤åˆ¶åˆ—è¡¨åˆ°å‰ªè´´æ¿", on_click=lambda: st.write(st.clipboard(preview_str)) or st.toast("å·²å¤åˆ¶ï¼"))

        st.markdown("### âš™ï¸ Prompt è®¾ç½®")
        
        # Prompt é…ç½®
        pc1, pc2, pc3 = st.columns(3)
        with pc1:
            front_mode = st.selectbox("æ­£é¢å†…å®¹", ["å•è¯ (Word)", "çŸ­è¯­/æ­é… (Phrase)"])
        with pc2:
            def_mode = st.selectbox("é‡Šä¹‰è¯­è¨€", ["è‹±æ–‡", "ä¸­æ–‡", "ä¸­è‹±åŒè¯­"])
        with pc3:
            # é»˜è®¤100ï¼Œæœ€å¤§150ï¼Œæœ€å°1ï¼Œæ­¥é•¿1
            batch_size = st.number_input("AI åˆ†ç»„å¤§å° (Batch Size)", min_value=1, max_value=150, value=100, step=1)
            
        ex_count = st.slider("ä¾‹å¥æ•°é‡", 1, 3, 1)
        need_ety = st.checkbox("åŒ…å«è¯æº/è¯æ ¹", value=True)
        
        # ç”Ÿæˆæ‰¹æ¬¡
        batches = [words_only[i:i + batch_size] for i in range(0, len(words_only), batch_size)]
        
        st.info(f"å·²ç”Ÿæˆ {len(batches)} ç»„ Promptã€‚")
        
        for idx, batch in enumerate(batches):
            with st.expander(f"ğŸ“ Prompt ç¬¬ {idx+1} ç»„ (å…± {len(batch)} è¯)"):
                prompt = get_ai_prompt(batch, front_mode, def_mode, ex_count, need_ety)
                st.code(prompt, language="text")

# --- Tab 3: Anki åˆ¶ä½œ ---
with tab_anki:
    st.markdown("### ğŸ“¦ åˆ¶ä½œ Anki ç‰Œç»„")
    
    st.info("è¯·å°† AI çš„ JSON å›å¤ç²˜è´´åˆ°æ­¤å¤„ã€‚æ”¯æŒè¿ç»­ç²˜è´´å¤šæ¬¡å›å¤ã€‚")
    
    if 'anki_input_text' not in st.session_state:
        st.session_state['anki_input_text'] = ""
        
    ai_resp = st.text_area("JSON è¾“å…¥æ¡†", height=200, key="anki_input_text")
    deck_name = st.text_input("ç‰Œç»„åç§°", f"Vocab_{get_beijing_time_str()}")
    
    if st.button("ğŸ› ï¸ ç”Ÿæˆ .apkg æ–‡ä»¶", type="primary"):
        if ai_resp.strip():
            parsed_data = parse_anki_data(ai_resp)
            if parsed_data:
                # é¢„è§ˆè¡¨æ ¼
                df_view = pd.DataFrame(parsed_data)
                st.write(f"âœ… æˆåŠŸè§£æ {len(parsed_data)} å¼ å¡ç‰‡ã€‚")
                st.dataframe(df_view, use_container_width=True, hide_index=True)
                
                # ç”Ÿæˆæ–‡ä»¶
                f_path = generate_anki_package(parsed_data, deck_name)
                
                # ä¸‹è½½æŒ‰é’®
                with open(f_path, "rb") as f:
                    st.download_button(
                        label=f"ğŸ“¥ ä¸‹è½½ {deck_name}.apkg",
                        data=f,
                        file_name=f"{deck_name}.apkg",
                        mime="application/octet-stream"
                    )
            else:
                st.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ JSON æ•°æ®ï¼Œè¯·æ£€æŸ¥æ ¼å¼ã€‚")
        else:
            st.warning("âš ï¸ è¾“å…¥ä¸ºç©ºã€‚")