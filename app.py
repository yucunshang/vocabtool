import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import genanki
import random
import tempfile

# ==========================================
# 0. é¡µé¢åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(
    page_title="Vocab Flow Pro (English Def)", 
    page_icon="âš¡ï¸", 
    layout="centered",
    initial_sidebar_state="collapsed"
)

# æ³¨å…¥ CSSï¼šç¾åŒ–ç•Œé¢
st.markdown("""
<style>
    .stTextArea textarea { font-family: 'Consolas', monospace; font-size: 14px; }
    .stButton>button { border-radius: 8px; font-weight: 600; width: 100%; }
    .success-box { padding: 10px; background-color: #e6fffa; border-radius: 5px; color: #006d5b; margin-bottom: 10px; }
    .info-text { font-size: 0.9em; color: #555; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºåŠ è½½ (é€‚é… Streamlit Cloud)
# ==========================================
@st.cache_resource
def setup_nltk():
    """åœ¨äº‘ç«¯ç¯å¢ƒä¸‹å®‰å…¨ä¸‹è½½ NLTK æ•°æ®"""
    try:
        root_dir = os.path.dirname(os.path.abspath(__file__))
        nltk_data_dir = os.path.join(root_dir, 'nltk_data')
        os.makedirs(nltk_data_dir, exist_ok=True)
        nltk.data.path.append(nltk_data_dir)
        
        for pkg in ['averaged_perceptron_tagger', 'punkt', 'punkt_tab']:
            try:
                nltk.data.find(f'tokenizers/{pkg}')
            except LookupError:
                nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
    except Exception as e:
        st.warning(f"NLTK åˆå§‹åŒ–è­¦å‘Š: {e}")

setup_nltk()

@st.cache_data
def load_vocab_data():
    """åŠ è½½è¯é¢‘æ•°æ®ï¼Œå¢åŠ å®¹é”™"""
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    
    if file_path:
        try:
            df = pd.read_csv(file_path)
            # ç®€å•çš„åˆ—åæ¸…æ´—
            df.columns = [c.strip().lower() for c in df.columns]
            # è‡ªåŠ¨å¯»æ‰¾ word å’Œ rank åˆ—
            w_col = next((c for c in df.columns if 'word' in c), df.columns[0])
            r_col = next((c for c in df.columns if 'rank' in c), df.columns[1])
            
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            
            # å»é‡ä¿ç•™ rank æœ€å°çš„
            df = df.sort_values(r_col).drop_duplicates(subset=[w_col], keep='first')
            return pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except Exception as e:
            st.error(f"è¯é¢‘æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            return {}
    return {}

VOCAB_DICT = load_vocab_data()

def get_lemma(word):
    try: return lemminflect.getLemma(word, upos='VERB')[0]
    except: return word

# ==========================================
# 2. æ ¸å¿ƒåˆ†æé€»è¾‘ (ä¿ç•™ Current/Target ç­›é€‰)
# ==========================================
def analyze_text(text, current_lvl, target_lvl):
    raw_words = re.findall(r"[a-z]+", text.lower())
    unique_words = set(raw_words)
    
    target_words = [] 
    mastered_count = 0
    beyond_count = 0
    
    for w in unique_words:
        if len(w) < 2: continue
        lemma = get_lemma(w)
        rank = VOCAB_DICT.get(lemma, 99999) 
        
        # --- ç­›é€‰é€»è¾‘ ---
        if rank <= current_lvl:
            mastered_count += 1
        elif rank <= target_lvl:
            target_words.append((lemma, rank))
        else:
            beyond_count += 1
            
    target_words.sort(key=lambda x: x[1])
    final_list = [x[0] for x in target_words]
    
    return final_list, mastered_count, beyond_count

# ==========================================
# 3. Anki æ‰“åŒ…é€»è¾‘ (ç”Ÿæˆ .apkg)
# ==========================================
def generate_anki_package(cards_data, deck_name="Vocab_Deck"):
    """
    ç”Ÿæˆé«˜è´¨é‡ Anki åŒ…ï¼Œå†…ç½® CSS é€‚é… iOS æ·±è‰²æ¨¡å¼
    """
    
    # --- é«˜è´¨é‡ CSS æ¨¡æ¿ (è‹±è‹±é£æ ¼) ---
    CSS = """
    .card {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
        text-align: center;
        font-size: 20px;
        color: #333;
        background-color: #ffffff;
        padding: 20px 10px;
    }
    
    .nightMode .card { background-color: #2f2f31; color: #f5f5f5; }
    
    /* æ­£é¢ */
    .word { font-size: 38px; font-weight: 700; color: #007AFF; margin-bottom: 8px; }
    .nightMode .word { color: #5FA9FF; }
    .phonetic { font-family: "Lucida Sans Unicode", sans-serif; color: #888; font-size: 18px; }
    
    /* èƒŒé¢ */
    .def-container { 
        text-align: left; margin-top: 20px; padding-top: 15px; 
        border-top: 1px solid #eee; 
    }
    .nightMode .def-container { border-top: 1px solid #444; }
    
    /* è‹±æ–‡é‡Šä¹‰æ ·å¼ */
    .definition { font-weight: 500; font-size: 18px; color: #333; margin-bottom: 15px; line-height: 1.5; }
    .nightMode .definition { color: #eee; }
    
    .example-box {
        background: #f2f7fa; border-left: 4px solid #007AFF;
        padding: 10px; margin: 10px 0; border-radius: 4px;
        font-size: 16px; color: #555; text-align: left; font-style: italic;
    }
    .nightMode .example-box { background: #333333; border-left: 4px solid #5FA9FF; color: #ccc; }
    
    .etymology {
        margin-top: 20px; font-size: 15px; color: #666; 
        border: 1px dashed #bbb; padding: 8px; border-radius: 6px; display: block;
        text-align: left;
    }
    .nightMode .etymology { border-color: #555; color: #aaa; }
    .root-highlight { font-weight: bold; color: #d63031; }
    .nightMode .root-highlight { color: #ff7675; }
    """

    # --- Anki Model å®šä¹‰ ---
    model = genanki.Model(
        random.randrange(1 << 30, 1 << 31),
        'VocabFlow English Model',
        fields=[
            {'name': 'Word'},
            {'name': 'IPA'},
            {'name': 'Meaning'},
            {'name': 'Examples'},
            {'name': 'Etymology'},
        ],
        templates=[
            {
                'name': 'Card 1',
                'qfmt': '<div class="word">{{Word}}</div><div class="phonetic">{{IPA}}</div>',
                'afmt': '''
                {{FrontSide}}
                <div class="def-container">
                    <div class="definition">{{Meaning}}</div>
                    <div class="example-box">{{Examples}}</div>
                    <div class="etymology">ğŸŒ± <b>Roots & Affixes:</b><br>{{Etymology}}</div>
                </div>
                ''',
            },
        ],
        css=CSS
    )

    deck = genanki.Deck(random.randrange(1 << 30, 1 << 31), deck_name)

    for card in cards_data:
        deck.add_note(genanki.Note(
            model=model,
            fields=[
                card['word'],
                card['ipa'],
                card['meaning'],
                card['examples'].replace('\n', '<br>'),
                card['etymology']
            ]
        ))

    with tempfile.NamedTemporaryFile(delete=False, suffix='.apkg') as tmp:
        genanki.Package(deck).write_to_file(tmp.name)
        return tmp.name

# ==========================================
# 4. Prompt ç”Ÿæˆå™¨ (English Version)
# ==========================================
def get_ai_prompt(words):
    w_list = ", ".join(words)
    return f"""
Act as an expert Etymologist and Lexicographer. Create Anki card data.
Words: {w_list}

**Strict Output Format (Pipe Separated `|`, NO Header):**
Word | IPA | Concise English Definition | 2 English Sentences | Etymology (Roots/Affixes)

**Requirements:**
1. **Definition**: Concise English definition (B2/C1 level). Keep it short.
2. **Examples**: 2 authentic English sentences. Use `<br>` to separate them.
3. **Etymology**: Break down the word into roots/affixes. Explain the meaning of the root. 
   - Format: `root(meaning) + suffix(function)`
   - Example: `bene(good) + vol(wish)`
4. **No Header Row**.

**Example Line:**
benevolent | /bÉ™ËˆnevÉ™lÉ™nt/ | well meaning and kindly | He was a benevolent old man.<br>The fund provided benevolent assistance. | bene(good) + vol(wish) + -ent(adj suffix)
"""

# ==========================================
# 5. ä¸»ç•Œé¢é€»è¾‘
# ==========================================
st.title("âš¡ï¸ Vocab Flow (Eng Def)")
st.caption("æ–‡æœ¬åˆ†æ -> ç­›é€‰ -> AI ç”Ÿæˆ (è‹±è‹±é‡Šä¹‰+è¯æº) -> iOS Anki åŒ…")

if not VOCAB_DICT:
    st.error("âš ï¸ æœªåœ¨ç›®å½•ä¸‹æ£€æµ‹åˆ° `coca_cleaned.csv`ï¼Œæ— æ³•è¿›è¡Œè¯é¢‘ç­›é€‰ï¼")

t1, t2 = st.tabs(["1ï¸âƒ£ åˆ†æä¸æè¯", "2ï¸âƒ£ ç”Ÿæˆ Anki åŒ…"])

# --- Tab 1: åˆ†æ ---
with t1:
    c1, c2 = st.columns(2)
    curr_lvl = c1.number_input("å½“å‰è¯æ±‡é‡ (Current)", 1000, 20000, 4000, step=500, help="å¿½ç•¥å¤ªç®€å•çš„è¯")
    targ_lvl = c2.number_input("ç›®æ ‡è¯æ±‡é‡ (Target)", 1000, 30000, 12000, step=500, help="å¿½ç•¥å¤ªç”Ÿåƒ»çš„è¯")
    
    txt = st.text_area("åœ¨æ­¤ç²˜è´´è‹±æ–‡æ–‡æœ¬/æ–‡ç« ", height=150, placeholder="Paste English text here...")
    
    if st.button("ğŸ” å¼€å§‹åˆ†æ", type="primary"):
        if not txt.strip():
            st.warning("è¯·å…ˆè¾“å…¥æ–‡æœ¬")
        elif not VOCAB_DICT:
            st.warning("æ— è¯é¢‘æ•°æ®ï¼Œæ— æ³•ç­›é€‰")
        else:
            final_words, num_m, num_b = analyze_text(txt, curr_lvl, targ_lvl)
            st.session_state['words'] = final_words
            
            st.markdown(f"""
            <div class="success-box">
                <b>ğŸ¯ ç­›é€‰å‡º {len(final_words)} ä¸ªé‡ç‚¹è¯ (Learning Zone)</b><br>
                <span style='font-size:0.85em; opacity:0.8'>
                âœ… å·²æŒæ¡: {num_m} | ğŸš€ è¶…çº²: {num_b}
                </span>
            </div>
            """, unsafe_allow_html=True)

    if 'words' in st.session_state and st.session_state['words']:
        words_str = st.text_area("ç¡®è®¤å•è¯åˆ—è¡¨", ", ".join(st.session_state['words']), height=100)
        
        st.markdown("##### ğŸš€ å¤åˆ¶ Prompt å‘ç»™ AI")
        if st.button("ç”Ÿæˆ English Prompt"):
            final_list = [w.strip() for w in words_str.split(',') if w.strip()]
            prompt = get_ai_prompt(final_list)
            st.code(prompt, language="markdown")
            st.info("ğŸ’¡ å°† AI å›å¤çš„ç®¡é“ç¬¦å†…å®¹å¤åˆ¶ï¼Œå» Tab 2 ç”Ÿæˆå¡ç‰‡ã€‚")

# --- Tab 2: åˆ¶ä½œ ---
with t2:
    st.markdown("### ğŸ› ï¸ åˆ¶ä½œ iOS å®Œç¾é€‚é…åŒ…")
    st.markdown("<div class='info-text'>å°† AI å›å¤ç²˜è´´åˆ°ä¸‹æ–¹ (æ ¼å¼: Word | IPA | Def | Ex | Etym)ï¼š</div>", unsafe_allow_html=True)
    
    ai_response = st.text_area("ç²˜è´´ AI æ•°æ®", height=200, placeholder="benevolent | ... | well meaning | ... | bene(good)+vol(wish)")
    deck_title = st.text_input("ç‰Œç»„åç§°", "My English Vocab")
    
    if st.button("ğŸ“¦ ç”Ÿæˆ .apkg æ–‡ä»¶", type="primary"):
        if not ai_response.strip():
            st.error("å†…å®¹ä¸ºç©º")
        else:
            lines = ai_response.strip().split('\n')
            cards = []
            err_cnt = 0
            
            for line in lines:
                if "|" not in line: continue
                if "Word | IPA" in line: continue 
                
                parts = [p.strip() for p in line.split('|')]
                if len(parts) >= 3: 
                    cards.append({
                        'word': parts[0],
                        'ipa': parts[1] if len(parts) > 1 else '',
                        'meaning': parts[2] if len(parts) > 2 else '',
                        'examples': parts[3] if len(parts) > 3 else '',
                        'etymology': parts[4] if len(parts) > 4 else ''
                    })
                else:
                    err_cnt += 1
            
            if cards:
                try:
                    tmp_file_path = generate_anki_package(cards, deck_title)
                    with open(tmp_file_path, "rb") as f:
                        file_data = f.read()
                    
                    st.download_button(
                        label=f"ğŸ“¥ ç‚¹å‡»ä¸‹è½½ {deck_title}.apkg",
                        data=file_data,
                        file_name=f"{deck_title}.apkg",
                        mime="application/octet-stream",
                        type="primary"
                    )
                    st.success(f"æˆåŠŸæ‰“åŒ… {len(cards)} å¼ å¡ç‰‡ï¼")
                except Exception as e:
                    st.error(f"æ‰“åŒ…å‡ºé”™: {e}")
            else:
                st.error("æ•°æ®æ ¼å¼é”™è¯¯ï¼Œè¯·æ£€æŸ¥åˆ†éš”ç¬¦ |")