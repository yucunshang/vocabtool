import streamlit as st
import pandas as pd
import re
import os
import simplemma

st.set_page_config(page_title="Vibe Vocab Studio", page_icon="ğŸ›¡ï¸", layout="wide")

# ==========================================
# 1. å…¼å®¹æ€§æœ€å¼ºçš„ Lemmatizer (è¿˜åŸå¼•æ“)
# ==========================================
# ä¸å†æ£€æµ‹ç‰ˆæœ¬ï¼Œç›´æ¥å®šä¹‰ä¸€ä¸ªèƒ½å®¹é”™çš„å‡½æ•°
try:
    # å°è¯•åŠ è½½æ•°æ® (æ—§ç‰ˆé€»è¾‘)
    if hasattr(simplemma, 'load_data'):
        LANG_DATA = simplemma.load_data('en')
    else:
        LANG_DATA = None
except:
    LANG_DATA = None

def get_lemma(word):
    """æœ€ç¨³å¥çš„è¿˜åŸå‡½æ•°"""
    try:
        # ä¼˜å…ˆå°è¯•æ–°ç‰ˆ (v1.x) ç›´æ¥è°ƒç”¨
        return simplemma.lemmatize(word, lang='en')
    except TypeError:
        # å¦‚æœæŠ¥é”™ï¼Œè¯´æ˜æ˜¯æ—§ç‰ˆï¼Œéœ€è¦ä¼ æ•°æ®
        if LANG_DATA:
            return simplemma.lemmatize(word, LANG_DATA)
        return word # å®åœ¨ä¸è¡Œè¿”å›åŸè¯
    except Exception:
        return word

# ==========================================
# 2. å¸¦â€œè‡ªæ£€â€åŠŸèƒ½çš„è¯åº“åŠ è½½ (æ ¸å¿ƒä¿®å¤)
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv", "COCA20000è¯Excelç‰ˆ.xlsx - Sheet1.csv"]

@st.cache_data
def load_vocab_robust():
    # 1. æ‰¾åˆ°æ–‡ä»¶
    file_path = None
    for f in POSSIBLE_FILES:
        if os.path.exists(f):
            file_path = f
            break
    
    if not file_path:
        return None, "âŒ æœªæ‰¾åˆ° csv æ–‡ä»¶ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å·²ä¸Šä¼ åˆ° GitHubã€‚"

    # 2. å°è¯•å¤šç§æ–¹å¼è¯»å–ï¼Œç›´åˆ°é€šè¿‡â€œè‡ªæ£€â€
    success_dict = None
    debug_msg = []

    # å®šä¹‰è¯»å–ç­–ç•¥
    strategies = [
        # ç­–ç•¥A: æ ‡å‡†CSV (æœ‰è¡¨å¤´)
        {'args': {'encoding': 'utf-8-sig'}, 'desc': 'UTF-8 æ ‡å‡†è¯»å–'},
        {'args': {'encoding': 'utf-8'}, 'desc': 'UTF-8 è¯»å–'},
        {'args': {'encoding': 'gbk'}, 'desc': 'GBK è¯»å–'},
        # ç­–ç•¥B: æ— è¡¨å¤´ (å‡è®¾ç¬¬ä¸€åˆ—å•è¯ï¼Œç¬¬äºŒåˆ—æ’å)
        {'args': {'encoding': 'utf-8', 'header': None}, 'desc': 'æ— è¡¨å¤´æ¨¡å¼'},
    ]

    for strat in strategies:
        try:
            df = pd.read_csv(file_path, **strat['args'])
            
            # ç»Ÿä¸€åˆ—å (å¦‚æœæ˜¯æ— è¡¨å¤´æ¨¡å¼ï¼Œæ‰‹åŠ¨æŒ‡å®š)
            if strat.get('args', {}).get('header') is None:
                # å‡è®¾å‰ä¸¤åˆ—æœ‰æ•ˆ
                if len(df.columns) >= 2:
                    df = df.iloc[:, :2]
                    df.columns = ['word', 'rank']
            else:
                # æ ‡å‡†åŒ–åˆ—å
                df.columns = [str(c).strip().lower() for c in df.columns]

            # å¯»æ‰¾ word å’Œ rank åˆ—
            w_col = next((c for c in df.columns if any(k in c for k in ['word', 'å•è¯', 'è¯æ±‡'])), None)
            r_col = next((c for c in df.columns if any(k in c for k in ['rank', 'æ’å', 'åºå·', 'è¯é¢‘'])), None)

            # å…œåº•åˆ—å (å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œä¸”åªæœ‰2åˆ—ï¼Œå°±ç›²çŒœ)
            if not w_col and len(df.columns) == 2: w_col = df.columns[0]
            if not r_col and len(df.columns) == 2: r_col = df.columns[1]

            # ä¹Ÿæ˜¯å…œåº• (é’ˆå¯¹åŸå§‹ä¹±æ–‡ä»¶)
            if not w_col and len(df.columns) >= 4: w_col = df.columns[0]
            if not r_col and len(df.columns) >= 4: r_col = df.columns[3]

            if not w_col or not r_col:
                continue # åˆ—éƒ½æ²¡æ‰¾é½ï¼Œæ¢ä¸‹ä¸€ç§ç­–ç•¥

            # æ¸…æ´—æ•°æ®
            df['w_clean'] = df[w_col].astype(str).str.lower().str.strip()
            df['r_clean'] = pd.to_numeric(df[r_col], errors='coerce')
            
            # å»é™¤æ— æ•ˆè¡Œ
            df_valid = df.dropna(subset=['r_clean'])
            
            # ç”Ÿæˆå­—å…¸
            temp_dict = pd.Series(df_valid['r_clean'].values, index=df_valid['w_clean']).to_dict()

            # === å…³é”®æ­¥éª¤ï¼šè‡ªæˆ‘æ ¸éªŒ (Sanity Check) ===
            # æ£€æŸ¥åŸºç¡€è¯ 'the', 'of', 'and' çš„æ’åæ˜¯å¦åˆç†
            # å®ƒä»¬åº”è¯¥æ˜¯å‰ 10 å
            score = 0
            if temp_dict.get('the', 999) <= 10: score += 1
            if temp_dict.get('of', 999) <= 10: score += 1
            if temp_dict.get('and', 999) <= 10: score += 1

            if score >= 1:
                # é€šè¿‡æ ¸éªŒï¼
                success_dict = temp_dict
                debug_msg.append(f"âœ… ç­–ç•¥ [{strat['desc']}] æˆåŠŸ! 'the' rank: {temp_dict.get('the')}")
                break
            else:
                debug_msg.append(f"âš ï¸ ç­–ç•¥ [{strat['desc']}] å¤±è´¥: 'the' rank is {temp_dict.get('the')}")

        except Exception as e:
            debug_msg.append(f"âŒ ç­–ç•¥ [{strat['desc']}] æŠ¥é”™: {str(e)}")
            continue

    if success_dict:
        return success_dict, f"åŠ è½½æˆåŠŸ ({len(success_dict)}è¯)"
    else:
        return None, f"æ‰€æœ‰è¯»å–ç­–ç•¥éƒ½å¤±è´¥ã€‚\nè°ƒè¯•æ—¥å¿—:\n" + "\n".join(debug_msg)


# ==========================================
# 3. æ ¸å¿ƒé€»è¾‘
# ==========================================
vocab_dict, status_msg = load_vocab_robust()

st.title("ğŸ›¡ï¸ Vibe Vocab v7.0 (æœ€ç»ˆæ ¸éªŒç‰ˆ)")

# ä¾§è¾¹æ çŠ¶æ€
if vocab_dict:
    st.sidebar.success(status_msg)
    # åŒé‡ä¿é™©æ˜¾ç¤º
    the_rank = vocab_dict.get('the', 'Not Found')
    st.sidebar.info(f"æ£€æŸ¥ç‚¹: 'the' = {the_rank}")
else:
    st.error("ğŸ’¥ ä¸¥é‡é”™è¯¯ï¼šè¯åº“åŠ è½½å¤±è´¥")
    st.text(status_msg)
    st.stop()

st.sidebar.divider()
vocab_range = st.sidebar.slider("è®¾å®šå­¦ä¹ èŒƒå›´", 1, 20000, (6000, 8000), 500)
r_start, r_end = vocab_range

st.sidebar.markdown(f"""
- ğŸŸ¢ **ç†Ÿè¯**: 1 ~ {r_start}
- ğŸŸ¡ **é‡ç‚¹**: {r_start} ~ {r_end}
- ğŸ”´ **è¶…çº²**: {r_end}+
""")

# å¤„ç†é€»è¾‘
def process_text(text):
    text_lower = text.lower()
    words = re.findall(r'\b[a-z\']{2,}\b', text_lower)
    unique_words = sorted(list(set(words)))
    
    known, target, beyond = [], [], []
    
    for w in unique_words:
        rank = 99999
        match = w
        note = ""

        # 1. ç›´æ¥æŸ¥
        if w in vocab_dict:
            rank = vocab_dict[w]
        else:
            # 2. è¿˜åŸæŸ¥ (went -> go)
            lemma = get_lemma(w)
            if lemma in vocab_dict:
                rank = vocab_dict[lemma]
                match = lemma
                note = f"(åŸ: {w})"
            else:
                # 3. ç®€å•å»å°¾æŸ¥
                if w.endswith("s") and w[:-1] in vocab_dict:
                    rank = vocab_dict[w[:-1]]
                    match = w[:-1]
                elif w.endswith("'s") and w[:-2] in vocab_dict:
                    rank = vocab_dict[w[:-2]]
                    match = w[:-2]

        item = {'å•è¯': match, 'æ’å': int(rank), 'å¤‡æ³¨': note}
        
        if rank <= r_start: known.append(item)
        elif r_start < rank <= r_end: target.append(item)
        else: beyond.append(item)

    return pd.DataFrame(known), pd.DataFrame(target), pd.DataFrame(beyond)

# ç•Œé¢
text_input = st.text_area("åœ¨æ­¤ç²˜è´´æ–‡æœ¬:", height=150)

if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
    if not text_input.strip():
        st.warning("è¯·è¾“å…¥å†…å®¹")
    else:
        df_k, df_t, df_b = process_text(text_input)
        
        st.success("åˆ†æå®Œæˆ")
        t1, t2, t3 = st.tabs([
            f"ğŸŸ¡ é‡ç‚¹è¯ ({len(df_t)})", 
            f"ğŸ”´ è¶…çº²è¯ ({len(df_b)})", 
            f"ğŸŸ¢ ç†Ÿè¯ ({len(df_k)})"
        ])
        
        def dl_btn(df, name):
            if df.empty: return
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(f"ğŸ“¥ ä¸‹è½½ {name}.csv", csv, f"{name}.csv", "text/csv")

        with t1:
            st.dataframe(df_t, use_container_width=True)
            dl_btn(df_t, "target_words")
            
        with t2:
            st.dataframe(df_b, use_container_width=True)
            dl_btn(df_b, "beyond_words")
            
        with t3:
            st.dataframe(df_k, use_container_width=True)
            dl_btn(df_k, "known_words")