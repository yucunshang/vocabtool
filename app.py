import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import time
# æ–‡ä»¶å¤„ç†åº“
import PyPDF2
from ebooklib import epub
import ebooklib
from bs4 import BeautifulSoup
import pysrt
# API åº“
from openai import OpenAI

# ==========================================
# 1. æ ¸å¿ƒé…ç½®ä¸ NLTK ä¿®å¤
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

# ä¿®å¤ Streamlit Cloud ä¸Šçš„ NLTK æŠ¥é”™ (å¢åŠ  punkt_tab)
@st.cache_resource
def download_nltk_data():
    resources = [
        ('tokenizers/punkt', 'punkt'),
        ('tokenizers/punkt_tab', 'punkt_tab'), # å…³é”®ä¿®å¤
        ('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger'),
        ('corpora/wordnet', 'wordnet')
    ]
    for path, name in resources:
        try:
            nltk.data.find(path)
        except LookupError:
            nltk.download(name)

download_nltk_data()

# ==========================================
# 2. å·¥å…·å‡½æ•°å®šä¹‰
# ==========================================

def get_exam_tag(rank):
    """æ ¹æ® COCA æ’åæ˜ å°„è€ƒè¯•éš¾åº¦"""
    if pd.isna(rank): return "æœªçŸ¥"
    rank = int(rank)
    if rank <= 2000: return "åˆä¸­/åŸºç¡€"
    if rank <= 4000: return "é«˜ä¸­/å››çº§"
    if rank <= 6000: return "å…­çº§/è€ƒç ”"
    if rank <= 9000: return "é›…æ€/æ‰˜ç¦"
    if rank <= 13000: return "GRE/ä¸“å…«"
    if rank <= 20000: return "é«˜é˜¶åŸè‘—"
    return "è¶…çº²/ç½•è§"

def extract_text_from_file(uploaded_file):
    """å¤šæ ¼å¼æ–‡æœ¬æå–å™¨"""
    text = ""
    try:
        file_ext = uploaded_file.name.split('.')[-1].lower()
        
        if file_ext == 'txt':
            text = uploaded_file.getvalue().decode("utf-8")
            
        elif file_ext == 'pdf':
            reader = PyPDF2.PdfReader(uploaded_file)
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted: text += extracted + "\n"
                
        elif file_ext == 'epub':
            # EbookLib éœ€è¦æ–‡ä»¶è·¯å¾„ï¼Œå…ˆå­˜ä¸´æ—¶æ–‡ä»¶
            with open("temp.epub", "wb") as f:
                f.write(uploaded_file.getbuffer())
            book = epub.read_epub("temp.epub")
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_body_content(), 'html.parser')
                    text += soup.get_text() + "\n"
            os.remove("temp.epub") # æ¸…ç†
            
        elif file_ext in ['srt', 'vtt']:
            content = uploaded_file.getvalue().decode("utf-8")
            # ç®€å•æ­£åˆ™å»é™¤æ—¶é—´è½´ (00:00:01,000 --> ...)
            lines = [l for l in content.splitlines() 
                     if not re.match(r'(\d{2}:\d{2})|(\d+$)', l.strip())]
            text = "\n".join(lines)
            
    except Exception as e:
        st.error(f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")
        return ""
        
    return text

def smart_lemmatize(text):
    """æ™ºèƒ½è¯å½¢è¿˜åŸ (å¸¦è¯æ€§åˆ¤æ–­)"""
    # æ¸…æ´—éå­—æ¯å­—ç¬¦ï¼Œä½†ä¿ç•™ç©ºæ ¼
    clean_text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    words = nltk.word_tokenize(clean_text)
    pos_tags = nltk.pos_tag(words)
    
    lemmatized = []
    for word, tag in pos_tags:
        if len(word) < 2: continue # è·³è¿‡å•ä¸ªå­—æ¯
        
        # æ˜ å°„ Treebank POS åˆ° lemminflect
        if tag.startswith('J'): pos = 'ADJ'
        elif tag.startswith('V'): pos = 'VERB'
        elif tag.startswith('R'): pos = 'ADV'
        else: pos = 'NOUN'
        
        lemma = lemminflect.getLemma(word, upos=pos)
        if not lemma: 
            lemma = word.lower()
        else: 
            lemma = lemma[0].lower()
            
        lemmatized.append(lemma)
    return lemmatized

# ==========================================
# 3. ä¾§è¾¹æ è®¾ç½®
# ==========================================
with st.sidebar:
    st.title("âš™ï¸ è®¾ç½®")
    st.markdown("### DeepSeek API")
    api_key = st.text_input("API Key", type="password", placeholder="sk-...", help="å¡«å…¥ Key å¯ç›´æ¥ç”Ÿæˆè§£é‡Šï¼Œå¦åˆ™ä»…å¤åˆ¶ Prompt")
    
    st.markdown("### ç­›é€‰é…ç½®")
    rank_range = st.slider("è¯é¢‘èŒƒå›´ (Rank)", 0, 20000, (4000, 15000), help="æ•°å­—è¶Šå¤§å•è¯è¶Šç”Ÿåƒ»")
    
    st.markdown("---")
    st.info("ğŸ’¡ æç¤ºï¼šæ”¯æŒä¸Šä¼  PDF, EPUB, SRT å­—å¹•æ–‡ä»¶")

# ==========================================
# 4. ä¸»ç•Œé¢é€»è¾‘
# ==========================================
st.title("ğŸš€ Vocab Master Pro")
st.caption("ä¸Šä¼ æ–‡æ¡£ -> æ™ºèƒ½æå–ç”Ÿè¯ -> ä¸€é”®ç”Ÿæˆ Anki å¡ç‰‡")

col1, col2 = st.columns([2, 1])
with col1:
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡ä»¶", type=['txt', 'pdf', 'epub', 'srt'])
with col2:
    user_input = st.text_area("âœï¸ æˆ–ç›´æ¥ç²˜è´´æ–‡æœ¬", height=100)

# è·å–æ–‡æœ¬
raw_text = ""
if uploaded_file:
    raw_text = extract_text_from_file(uploaded_file)
elif user_input:
    raw_text = user_input

if raw_text:
    # --- å¤„ç†å¼€å§‹ ---
    with st.spinner("æ­£åœ¨è¿›è¡Œ NLP åˆ†æä¸è¯é¢‘æ¯”å¯¹..."):
        start_time = time.time()
        
        # 1. è¿˜åŸ
        words = smart_lemmatize(raw_text)
        
        # 2. ç»Ÿè®¡
        word_counts = pd.Series(words).value_counts().reset_index()
        word_counts.columns = ['word', 'count']
        
        # 3. åŠ è½½ COCA æ•°æ® (å¸¦å®¹é”™)
        try:
            # å°è¯•åŠ è½½çœŸå®æ•°æ®
            df_coca = pd.read_csv('coca20000.csv') 
            # ç®€å•çš„åˆ—åæ ‡å‡†åŒ–ï¼Œé˜²æ­¢csvè¡¨å¤´ä¸ä¸€æ ·
            if 'lemma' in df_coca.columns: 
                df_coca.rename(columns={'lemma': 'word'}, inplace=True)
        except FileNotFoundError:
            #å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é˜²æ­¢æŠ¥é”™
            st.toast("âš ï¸ æœªæ‰¾åˆ° coca20000.csvï¼Œä½¿ç”¨æµ‹è¯•æ•°æ®è¿è¡Œ", icon="ğŸ")
            import numpy as np
            mock_words = word_counts['word'].tolist()
            df_coca = pd.DataFrame({
                'word': mock_words,
                'rank': np.random.randint(1, 20000, size=len(mock_words))
            })
            
        # 4. åˆå¹¶æ•°æ®
        df_merged = pd.merge(word_counts, df_coca, on='word', how='inner') # inner join åªä¿ç•™è®¤è¯†çš„è¯
        
        # 5. å¢åŠ æ ‡ç­¾
        df_merged['Exam_Tag'] = df_merged['rank'].apply(get_exam_tag)
        
        # 6. ç­›é€‰
        mask = (df_merged['rank'] >= rank_range[0]) & (df_merged['rank'] <= rank_range[1])
        df_final = df_merged[mask].sort_values('rank').reset_index(drop=True)
        
        end_time = time.time()

    # --- ç»“æœå±•ç¤º ---
    st.divider()
    st.success(f"âœ… åˆ†æå®Œæˆï¼è€—æ—¶ {end_time - start_time:.2f}s | åŸæ–‡ {len(words)} è¯ | å‘½ä¸­ç”Ÿè¯ **{len(df_final)}** ä¸ª")

    if not df_final.empty:
        # å¯è§†åŒ–åŒºåŸŸ
        tab1, tab2 = st.tabs(["ğŸ“Š éš¾åº¦åˆ†å¸ƒ", "ğŸ“ˆ è¯é¢‘è¶‹åŠ¿"])
        with tab1:
            st.bar_chart(df_final['Exam_Tag'].value_counts())
        with tab2:
            st.line_chart(df_final['rank'])
            
        # æ•°æ®è¡¨åŒºåŸŸ
        st.subheader("ğŸ“ ç”Ÿè¯åˆ—è¡¨")
        st.dataframe(
            df_final[['word', 'rank', 'Exam_Tag', 'count']], 
            use_container_width=True,
            column_config={
                "rank": st.column_config.NumberColumn("è¯é¢‘æ’å (COCA)"),
                "count": st.column_config.ProgressColumn("å‡ºç°æ¬¡æ•°", format="%d", min_value=0, max_value=df_final['count'].max())
            }
        )
        
        # --- AI ç”ŸæˆåŒºåŸŸ ---
        st.divider()
        st.subheader("ğŸ¤– AI è§£é‡Šç”Ÿæˆ")
        
        # æ„å»º Prompt
        target_words = df_final['word'].head(50).tolist() # é™åˆ¶å‰50ä¸ªé˜²æ­¢ token æº¢å‡º
        words_str = ", ".join(target_words)
        
        default_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±è¯­è€å¸ˆã€‚è¯·åˆ†æä»¥ä¸‹å•è¯ï¼Œå¹¶è¾“å‡ºä¸º CSV æ ¼å¼ï¼ˆä½¿ç”¨ç«–çº¿ | åˆ†éš”ï¼‰ã€‚
åŒ…å«å­—æ®µï¼šå•è¯ | éŸ³æ ‡ | è¯æ€§ | ä¸­æ–‡ç®€æ˜é‡Šä¹‰ | è¯­å¢ƒä¾‹å¥ (è‹±æ–‡) | è®°å¿†æ³• (è¯æ ¹/è”æƒ³)

å•è¯åˆ—è¡¨ï¼š
{words_str}

è¦æ±‚ï¼š
1. ä¸è¦è¾“å‡ºè¡¨å¤´ã€‚
2. é‡Šä¹‰è¦ç²¾å‡†ç®€ç»ƒã€‚
3. ä¸¥æ ¼éµå®ˆæ ¼å¼ï¼Œæ–¹ä¾¿å¯¼å…¥ Ankiã€‚
"""
        user_prompt = st.text_area("Prompt é¢„è§ˆ (å¯ç¼–è¾‘)", value=default_prompt, height=200)
        
        c1, c2 = st.columns([1, 1])
        
        with c1:
            # ç®€å•çš„å¤åˆ¶æŒ‰é’®é€»è¾‘ï¼ˆStreamlitåŸç”Ÿä¸æ”¯æŒç‚¹å‡»å¤åˆ¶ï¼Œè¿™é‡Œç”¨ä»£ç å—å±•ç¤ºæ–¹ä¾¿å¤åˆ¶ï¼‰
            st.info("ğŸ‘‡ è¿™é‡Œçš„ Prompt å·²å‡†å¤‡å¥½ï¼Œå…¨é€‰å¤åˆ¶å³å¯")
            st.code(user_prompt, language="text")
            
        with c2:
            st.write("### ğŸš€ ç›´æ¥è°ƒç”¨ DeepSeek")
            if st.button("å¼€å§‹ç”Ÿæˆ (DeepSeek V3)", type="primary"):
                if not api_key:
                    st.error("è¯·å…ˆåœ¨å·¦ä¾§è¾¹æ å¡«å…¥ API Key")
                else:
                    try:
                        client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
                        
                        placeholder = st.empty()
                        full_response = ""
                        
                        # æµå¼è¾“å‡ºæ•ˆæœ
                        with st.spinner("AI æ­£åœ¨æ€è€ƒä¸­..."):
                            response = client.chat.completions.create(
                                model="deepseek-chat",
                                messages=[
                                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè¾…åŠ©ç”Ÿæˆ Anki å¡ç‰‡çš„åŠ©æ‰‹ã€‚"},
                                    {"role": "user", "content": user_prompt}
                                ],
                                stream=True
                            )
                            
                            for chunk in response:
                                if chunk.choices[0].delta.content:
                                    content = chunk.choices[0].delta.content
                                    full_response += content
                                    placeholder.markdown(full_response + "â–Œ")
                                    
                            placeholder.markdown(full_response)
                            st.success("ç”Ÿæˆå®Œæ¯•ï¼æ‚¨å¯ä»¥ç›´æ¥å¤åˆ¶ä¸Šæ–¹å†…å®¹å­˜ä¸º .csv æ–‡ä»¶å¯¼å…¥ Ankiã€‚")
                            
                    except Exception as e:
                        st.error(f"API è°ƒç”¨å¤±è´¥: {e}")

    else:
        st.warning("åœ¨æ­¤ç­›é€‰æ¡ä»¶ä¸‹æ²¡æœ‰æ‰¾åˆ°ç”Ÿè¯ï¼Œè¯·å°è¯•è°ƒæ•´å·¦ä¾§ Rank èŒƒå›´ã€‚")