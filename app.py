import streamlit as st
import pandas as pd
import re
import os
import simplemma

st.set_page_config(page_title="Vibe Vocab Studio", page_icon="ğŸ§ ", layout="wide")

# --- 1. è‡ªåŠ¨é€‚é… Simplemma (ä¿æŒä¸å˜) ---
try:
    simplemma.lemmatize("t", lang="en")
    def get_lemma(word): return simplemma.lemmatize(word, lang="en")
except TypeError:
    if hasattr(simplemma, 'load_data'):
        lang_data = simplemma.load_data('en')
        def get_lemma(word): return simplemma.lemmatize(word, lang_data)
    else:
        def get_lemma(word): return word 

# --- 2. æ™ºèƒ½åˆ†æµåŠ è½½ (æ ¸å¿ƒä¿®å¤) ---
# ä¼˜å…ˆè¯»å– coca_cleaned.csv
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv", "COCA20000è¯Excelç‰ˆ.xlsx - Sheet1.csv"]

@st.cache_data
def load_vocab():
    file_path = None
    for f in POSSIBLE_FILES:
        if os.path.exists(f):
            file_path = f
            break
            
    if not file_path: return None, "æœªæ‰¾åˆ°æ–‡ä»¶"

    try:
        # å°è¯•è¯»å– (ä¼˜å…ˆ utf-8-sig å»é™¤ BOM å¤´)
        df = None
        for enc in ['utf-8-sig', 'utf-8', 'gbk']:
            try:
                df = pd.read_csv(file_path, encoding=enc)
                if len(df) > 10: break
            except: continue
        
        if df is None: return None, "è¯»å–å¤±è´¥"

        # === æ ¸å¿ƒä¿®å¤é€»è¾‘ ===
        cols = [str(c).strip().lower() for c in df.columns]
        df.columns = cols # é‡å‘½ååˆ—åä»¥ä¾¿æŸ¥æ‰¾

        word_col = None
        rank_col = None

        # æƒ…å†µ A: æ¸…æ´—è¿‡çš„æ–‡ä»¶ (é€šå¸¸åªæœ‰ word, rank ä¸¤åˆ—)
        if 'rank' in cols and 'word' in cols:
            word_col = 'word'
            rank_col = 'rank'
        # æƒ…å†µ B: åªæœ‰ä¸¤åˆ—ï¼Œä¸”åˆ—åä¸å¯¹ (ç›²çŒœ)
        elif len(cols) == 2:
            # å‡è®¾ç¬¬1åˆ—æ˜¯è¯ï¼Œç¬¬2åˆ—æ˜¯æ’å(æ•°å­—)
            word_col = df.columns[0]
            rank_col = df.columns[1]
        # æƒ…å†µ C: åŸå§‹ä¹±æ–‡ä»¶ (å¤šåˆ—)
        elif len(cols) >= 4:
            # åŸå§‹æ–‡ä»¶ç¬¬1åˆ—æ˜¯å•è¯ï¼Œç¬¬4åˆ—(ç´¢å¼•3)æ˜¯æ’å
            word_col = df.columns[0]
            rank_col = df.columns[3]
        
        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•å…³é”®è¯æœç´¢
        if not rank_col:
            rank_col = next((c for c in df.columns if any(k in c for k in ['rank', 'æ’å', 'åºå·', 'è¯é¢‘'])), None)
        if not word_col:
            word_col = next((c for c in df.columns if any(k in c for k in ['word', 'å•è¯', 'è¯æ±‡'])), None)

        if not word_col or not rank_col:
            return None, f"æ— æ³•è¯†åˆ«åˆ—åã€‚æ£€æµ‹åˆ°çš„åˆ—: {cols}"

        # === æ•°æ®æ¸…æ´— ===
        # å¼ºåˆ¶è½¬å°å†™ï¼Œå»ç©ºæ ¼
        df['word_clean'] = df[word_col].astype(str).str.lower().str.strip()
        # å¼ºåˆ¶è½¬æ•°å­—
        df['rank_clean'] = pd.to_numeric(df[rank_col], errors='coerce').fillna(99999)
        
        # å†æ¬¡è¿‡æ»¤ï¼šç¡®ä¿ rank æ˜¯æœ‰æ•ˆæ•°å­—ä¸” > 0
        df = df[df['rank_clean'] > 0]
        df = df[df['rank_clean'] < 99999]

        vocab_dict = pd.Series(
            df['rank_clean'].values, 
            index=df['word_clean']
        ).to_dict()
        
        return vocab_dict, f"å·²åŠ è½½: {file_path} (å•è¯åˆ—:{word_col}, æ’ååˆ—:{rank_col})"

    except Exception as e:
        return None, str(e)

# --- 3. æ ¸å¿ƒå¤„ç†é€»è¾‘ ---
def process_text_smart(text, vocab_dict, range_start, range_end):
    text_lower = text.lower()
    words = re.findall(r'\b[a-z\']{2,}\b', text_lower)
    unique_words = sorted(list(set(words)))
    
    tier_known = []   
    tier_target = []  
    tier_beyond = []  
    
    for w in unique_words:
        rank = 999999
        match_word = w
        
        # 1. æŸ¥åŸå½¢
        if w in vocab_dict:
            rank = vocab_dict[w]
            match_word = w
        else:
            # 2. æŸ¥è¿˜åŸ
            lemma = get_lemma(w)
            if lemma in vocab_dict:
                rank = vocab_dict[lemma]
                match_word = lemma
            else:
                # 3. æŸ¥å˜ä½“
                if w.endswith("'s") and w[:-2] in vocab_dict:
                    rank = vocab_dict[w[:-2]]
                    match_word = w[:-2]

        item = {'å•è¯': match_word, 'åŸæ–‡': w, 'æ’å': int(rank)}
        
        if rank <= range_start:
            tier_known.append(item)
        elif range_start < rank <= range_end:
            tier_target.append(item)
        else:
            if item['å•è¯'] == item['åŸæ–‡']:
                item['åŸæ–‡'] = '-'
            tier_beyond.append(item)

    def to_df(data):
        if not data: return pd.DataFrame()
        return pd.DataFrame(data).sort_values('æ’å').drop_duplicates(subset=['å•è¯'])

    return to_df(tier_known), to_df(tier_target), to_df(tier_beyond)

# --- 4. ç•Œé¢ UI ---
st.title("ğŸ§  Vibe Vocab v5.2 (æ™ºèƒ½åŒæ ¸ç‰ˆ)")
st.caption("å®Œç¾é€‚é… coca_cleaned.csv")

vocab_dict, status_msg = load_vocab()

# ä¾§è¾¹æ æ˜¾ç¤ºåŠ è½½çŠ¶æ€ï¼Œæ–¹ä¾¿è°ƒè¯•
st.sidebar.header("âš™ï¸ ç³»ç»ŸçŠ¶æ€")
if vocab_dict:
    st.sidebar.success(f"âœ… æˆåŠŸ! {len(vocab_dict)}è¯")
    st.sidebar.caption(f"è¯¦æƒ…: {status_msg}")
else:
    st.sidebar.error("âŒ è¯åº“åŠ è½½å¤±è´¥")
    st.sidebar.code(status_msg)
    st.stop()

st.sidebar.subheader("è®¾å®šå­¦ä¹ åŒºé—´")
vocab_range = st.sidebar.slider(
    "æ‹–åŠ¨æ»‘å—ï¼š", 1, 20000, (6000, 8000), 500
)
range_start, range_end = vocab_range

st.sidebar.info(
    f"ğŸŸ¢ **ç†Ÿè¯**: 1 - {range_start}\n\n"
    f"ğŸŸ¡ **é‡ç‚¹**: {range_start} - {range_end}\n\n"
    f"ğŸ”´ **è¶…çº²**: {range_end}+"
)

with st.expander("ğŸ“ æ–‡æœ¬è¾“å…¥", expanded=True):
    tab_paste, tab_upload = st.tabs(["ç²˜è´´æ–‡æœ¬", "ä¸Šä¼  TXT"])
    with tab_paste:
        text_input = st.text_area("åœ¨æ­¤ç²˜è´´:", height=150)
    with tab_upload:
        uploaded = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type="txt")
        if uploaded:
            text_input = uploaded.read().decode("utf-8")

final_text = text_input if text_input else ""

def show_download_buttons(df, prefix):
    if df.empty: return
    col1, col2 = st.columns(2)
    csv = df.to_csv(index=False).encode('utf-8')
    col1.download_button(f"ğŸ“¥ ä¸‹è½½ Excel", csv, f"{prefix}.csv", "text/csv")
    txt = "\n".join(df['å•è¯'].tolist())
    col2.download_button(f"ğŸ“„ ä¸‹è½½ TXT", txt, f"{prefix}.txt", "text/plain")

if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
    if not final_text.strip():
        st.warning("è¯·å…ˆè¾“å…¥æ–‡æœ¬ï¼")
    else:
        df_known, df_target, df_beyond = process_text_smart(final_text, vocab_dict, range_start, range_end)
        
        st.success(f"åˆ†æå®Œæˆï¼")
        
        t1, t2, t3 = st.tabs([
            f"ğŸŸ¡ é‡ç‚¹çªç ´ ({len(df_target)})", 
            f"ğŸ”´ è¶…çº²/ç”Ÿè¯ ({len(df_beyond)})", 
            f"ğŸŸ¢ å·²æŒæ¡ ({len(df_known)})"
        ])
        
        with t1:
            st.markdown(f"### ğŸ¯ é‡ç‚¹å­¦ä¹  ({range_start}-{range_end})")
            if not df_target.empty:
                st.dataframe(df_target, use_container_width=True)
                show_download_buttons(df_target, "target_words")
            else:
                st.info("æ­¤åŒºé—´æ— ç”Ÿè¯ã€‚")

        with t2:
            st.markdown(f"### ğŸš€ è¶…çº²è¯ (>{range_end})")
            if not df_beyond.empty:
                st.dataframe(df_beyond, use_container_width=True)
                show_download_buttons(df_beyond, "beyond_words")
            else:
                st.info("æ²¡æœ‰è¶…çº²è¯ã€‚")

        with t3:
            st.markdown(f"### âœ… å·²æŒæ¡ (<{range_start})")
            if not df_known.empty:
                st.dataframe(df_known, use_container_width=True)
                show_download_buttons(df_known, "known_words")
            else:
                st.info("æ— ç†Ÿè¯ã€‚")