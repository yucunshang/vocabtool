import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import json
import zipfile

# å°è¯•å¯¼å…¥æ–‡æ¡£å¤„ç†åº“
try:
    import PyPDF2
    import docx
except ImportError:
    pass # æ‰‹æœºç«¯å¦‚æœåªæ˜¯åˆ·é¢‘æ®µï¼Œä¸éœ€è¦è¿™äº›ï¼Œå®¹é”™å¤„ç†

# ==========================================
# 1. ç§»åŠ¨ç«¯ä¼˜å…ˆé…ç½®
# ==========================================
st.set_page_config(page_title="Vocab Prompt Gen", page_icon="ğŸ“±", layout="centered", initial_sidebar_state="collapsed")

# CSS é€‚é…æ‰‹æœºç«¯ï¼šå¢å¤§é—´è·ï¼Œéšè—ä¸éœ€è¦çš„å…ƒç´ 
st.markdown("""
<style>
    /* å…¨å±€å­—ä½“ä¸é—´è·ä¼˜åŒ– */
    .block-container { padding-top: 1rem; padding-bottom: 2rem; }
    
    /* éšè— Streamlit é»˜è®¤æ±‰å ¡èœå•å’Œé¡µè„šï¼Œä¸ä»…æ¸…çˆ½ä¹Ÿé˜²è¯¯è§¦ */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    
    /* æŒ‰é’®æ ·å¼ä¼˜åŒ– - æ›´åƒ App çš„è§¦æ§åŒº */
    .stButton>button {
        width: 100%;
        border-radius: 12px;
        height: 3em;
        font-weight: bold;
    }
    
    /* è¾“å…¥æ¡†æ ·å¼ */
    .stTextArea>div>div>textarea {
        font-size: 16px; /* é˜²æ­¢ iOS è¾“å…¥ç¼©æ”¾ */
    }
    
    /* ç»Ÿè®¡æ•°æ®å¤§å­—å· */
    [data-testid="stMetricValue"] {
        font-size: 24px !important;
    }
    
    /* åˆ†å‰²çº¿é¢œè‰² */
    hr { margin: 1.5em 0; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ•°æ®åˆå§‹åŒ– (ç²¾ç®€ç‰ˆ)
# ==========================================
@st.cache_data
def load_resources():
    # è·¯å¾„æ£€æŸ¥
    if not os.path.exists('data'): return {}, {}, {}
    
    try:
        with open('data/terms.json', 'r', encoding='utf-8') as f: terms = json.load(f)
        with open('data/proper.json', 'r', encoding='utf-8') as f: proper = json.load(f)
        # åŠ è½½è¯é¢‘è¡¨
        vocab = {}
        file_path = next((f for f in ["coca_cleaned.csv", "data.csv"] if os.path.exists(f)), None)
        if file_path:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c), cols[1])
            # ç®€å•æ¸…æ´—
            df = df.sort_values(r_col).drop_duplicates(subset=[w_col])
            vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
            
        return terms, proper, vocab
    except Exception as e:
        return {}, {}, {}

BUILTIN_TERMS, PROPER_NOUNS, VOCAB_DICT = load_resources()

# è¯å½¢è¿˜åŸéœ€ NLTK
@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    try: 
        nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_dir, quiet=True)
        nltk.download('punkt', download_dir=nltk_data_dir, quiet=True)
    except: pass
setup_nltk()

def get_lemma(w):
    lemmas = lemminflect.getAllLemmas(w)
    if not lemmas: return w.lower()
    return list(lemmas.values())[0][0]

# ==========================================
# 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# ==========================================

# æå–æ–‡æœ¬
def extract_text(file_obj):
    if not file_obj: return ""
    ext = file_obj.name.split('.')[-1].lower()
    try:
        if ext == 'txt': return file_obj.getvalue().decode("utf-8", errors="ignore")
        if ext == 'pdf':
            reader = PyPDF2.PdfReader(file_obj)
            return " ".join([p.extract_text() for p in reader.pages if p.extract_text()])
        if ext == 'docx':
            doc = docx.Document(file_obj)
            return " ".join([p.text for p in doc.paragraphs])
        if ext == 'epub':
            # ç®€åŒ–å¤„ç† epub
            with zipfile.ZipFile(file_obj) as z:
                return " ".join([z.read(n).decode('utf-8', errors='ignore') for n in z.namelist() if n.endswith('.html')])
    except: return ""
    return ""

# ç”Ÿæˆ Prompt
def generate_prompt(words, start_rank, end_rank, source_type="rank"):
    word_list_str = ", ".join(words)
    count = len(words)
    
    # é’ˆå¯¹æ‰‹æœºç«¯ä¼˜åŒ–çš„ Prompt
    prompt = f"""You are an expert Anki card generator.
    
TASK:
Create vocabulary flashcards for the following {count} English words.
{'Source: Words ranked ' + str(start_rank) + '-' + str(end_rank) + ' in frequency.' if source_type == 'rank' else 'Source: Extracted from user text.'}

STRICT OUTPUT FORMAT:
Please generate a **downloadable CSV file** with the following columns. 
If you cannot generate a file, output a **Code Block** in CSV format that I can easily copy.

CSV Structure:
"Target Word (w/ POS)","Definition & Context"

Content Rules:
1. Column 1: The word + Part of Speech (e.g., "ephemeral (adj)").
2. Column 2: 
   - English Definition (brief & clear).
   - Chinese Definition (brief).
   - One high-quality example sentence with the target word **bolded** (use HTML <b>word</b>).
   - [Optional] Etymology/Root if helpful.
   - Format usage: Use HTML line breaks <br> to separate definition and example.

List of Words:
{word_list_str}
"""
    return prompt

# ==========================================
# 4. ç•Œé¢é€»è¾‘ (App UI)
# ==========================================

st.title("ğŸ“± Vocab Master")

# æ¨¡å¼åˆ‡æ¢ï¼šå¦‚åŒ App çš„åº•éƒ¨æˆ–é¡¶éƒ¨ Tab
mode = st.radio("åŠŸèƒ½æ¨¡å¼", ["ğŸ”¢ è¯é¢‘åˆ·è¯ (Rank)", "ğŸ“– æ–‡æœ¬é€è§† (Context)"], horizontal=True, label_visibility="collapsed")

# -------------------------------------------------
# æ¨¡å¼ A: è¯é¢‘åˆ·è¯ (Range Mode) - æ–°åŠŸèƒ½
# -------------------------------------------------
if "Rank" in mode:
    st.markdown("### ğŸ¯ åˆ¶å®šæ¯æ—¥åˆ·è¯è®¡åˆ’")
    
    # å°†å­—å…¸åè½¬ç”¨äºæŸ¥æ‰¾ï¼šRank -> List of Words
    # æ³¨æ„ï¼šå¯èƒ½æœ‰å¤šä¸ªè¯æ‹¥æœ‰ç›¸åŒçš„ Rankï¼Œè™½ç„¶æˆ‘ä»¬çš„æ¸…æ´—é€»è¾‘å°½é‡é¿å…äº†
    if 'rank_map' not in st.session_state:
        r_map = {}
        for w, r in VOCAB_DICT.items():
            if r not in r_map: r_map[r] = []
            r_map[r].append(w)
        st.session_state.rank_map = r_map

    col1, col2 = st.columns(2)
    with col1:
        start_r = st.number_input("èµ·å§‹æ’å", value=8000, step=100)
    with col2:
        end_r = st.number_input("ç»“æŸæ’å", value=8100, step=100)
        
    if start_r >= end_r:
        st.error("ç»“æŸæ’åå¿…é¡»å¤§äºèµ·å§‹æ’å")
    else:
        # è·å–è¯¥åŒºé—´çš„è¯
        target_words = []
        for r in range(start_r, end_r + 1):
            if r in st.session_state.rank_map:
                target_words.extend(st.session_state.rank_map[r])
        
        # æˆªæ–­ä¸€ä¸‹é˜²æ­¢è¿‡å¤š
        if len(target_words) > 100:
            st.warning(f"åŒºé—´å†…æœ‰ {len(target_words)} ä¸ªè¯ï¼Œè‡ªåŠ¨æˆªå–å‰ 100 ä¸ªã€‚")
            target_words = target_words[:100]
            
        st.info(f"âœ… é€‰ä¸­ **{len(target_words)}** ä¸ªå•è¯")
        
        with st.expander("ğŸ‘€ é¢„è§ˆå•è¯åˆ—è¡¨"):
            st.write(", ".join(target_words))
            
        if st.button("ğŸš€ ç”Ÿæˆ AI Prompt", type="primary"):
            final_prompt = generate_prompt(target_words, start_r, end_r, "rank")
            st.session_state.final_prompt = final_prompt

# -------------------------------------------------
# æ¨¡å¼ B: æ–‡æœ¬é€è§† (Context Mode) - åŸåŠŸèƒ½ç®€åŒ–
# -------------------------------------------------
else:
    st.markdown("### ğŸ“– ä»é˜…è¯»ææ–™ä¸­æå–")
    
    # éšè—çš„é«˜çº§è®¾ç½®
    with st.expander("âš™ï¸ è¿‡æ»¤è®¾ç½® (é»˜è®¤å·²ä¼˜åŒ–)"):
        user_level = st.slider("å¿½ç•¥è¿‡äºç®€å•çš„è¯ (Rank < X)", 0, 15000, 4000)
        max_level = st.slider("å¿½ç•¥è¿‡äºç”Ÿåƒ»çš„è¯ (Rank > X)", 1000, 30000, 20000)
    
    # è¾“å…¥åŒºï¼šæ‰‹æœºä¸Š Text Area ä¸å¥½ç”¨ï¼Œä¼˜å…ˆæ–‡ä»¶ï¼Œæˆ–è€…ç²˜è´´æ¿
    tab1, tab2 = st.tabs(["ğŸ“ ç²˜è´´æ–‡æœ¬", "ğŸ“‚ ä¸Šä¼ æ–‡æ¡£"])
    with tab1:
        text_input = st.text_area("åœ¨æ­¤ç²˜è´´", height=150, placeholder="æ”¯æŒé•¿æŒ‰ç²˜è´´...")
    with tab2:
        file_input = st.file_uploader("æ”¯æŒ TXT/PDF/DOCX", type=["txt", "pdf", "docx", "epub"])
    
    if st.button("ğŸ” åˆ†æå¹¶æå–ç”Ÿè¯", type="primary"):
        # å¤„ç†æ–‡æœ¬
        raw_text = text_input
        if file_input: raw_text += "\n" + extract_text(file_input)
        
        if not raw_text.strip():
            st.warning("æ²¡å†…å®¹å•Šå¤§ä½¬")
        else:
            # ç®€å•çš„ NLP å¤„ç†
            words = re.findall(r"[a-zA-Z']+", raw_text)
            lemmas = set([get_lemma(w).lower() for w in words])
            
            # è¿‡æ»¤é€»è¾‘
            valid_words = []
            for w in lemmas:
                rank = VOCAB_DICT.get(w, 99999)
                if user_level < rank <= max_level:
                    valid_words.append((w, rank))
            
            # æ’åº
            valid_words.sort(key=lambda x: x[1])
            final_list = [x[0] for x in valid_words]
            
            # æˆªå– Top 50 (æ‰‹æœºä¸Šä¸å®œä¸€æ¬¡å¤ªå¤š)
            if len(final_list) > 50:
                final_list = final_list[:50]
                st.caption("ğŸ“± ä¸ºæ–¹ä¾¿æ‰‹æœºåˆ¶å¡ï¼Œä»…ä¿ç•™ Top 50 ç”Ÿè¯")
                
            st.success(f"ç­›é€‰å‡º {len(final_list)} ä¸ªç”Ÿè¯")
            with st.expander("ğŸ‘€ é¢„è§ˆå•è¯"):
                st.write(", ".join(final_list))
                
            st.session_state.final_prompt = generate_prompt(final_list, 0, 0, "text")

# ==========================================
# 5. ç»“æœè¾“å‡ºåŒº (å…±ç”¨)
# ==========================================
if "final_prompt" in st.session_state:
    st.divider()
    st.markdown("### ğŸ“‹ å¤åˆ¶æŒ‡ä»¤ç»™ AI")
    st.info("ğŸ’¡ è¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ç§»åŠ¨ç«¯ä¼˜åŒ–çš„æŒ‡ä»¤ã€‚å¤åˆ¶åï¼Œå‘é€ç»™ ChatGPT/Claude App å³å¯ã€‚")
    
    # ä½¿ç”¨ä»£ç å—æ˜¾ç¤ºï¼Œå³ä¸Šè§’è‡ªå¸¦å¤åˆ¶æŒ‰é’®
    st.code(st.session_state.final_prompt, language="markdown")
    
    st.markdown("""
    **æ‰‹æœºç«¯ä½¿ç”¨æŠ€å·§ï¼š**
    1. ç‚¹å‡»ä¸Šæ–¹ä»£ç å—å³ä¸Šè§’çš„ **Copy**ã€‚
    2. æ‰“å¼€ ChatGPT App ç²˜è´´å‘é€ã€‚
    3. AI ç”Ÿæˆåï¼Œç‚¹å‡»ä¸‹è½½ CSV æ–‡ä»¶ã€‚
    4. ç”¨ **AnkiMobile** æ‰“å¼€è¯¥æ–‡ä»¶å³å¯ç›´æ¥å¯¼å…¥ã€‚
    """)