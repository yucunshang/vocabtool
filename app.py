import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import time
# æ–‡ä»¶å¤„ç†åº“
import PyPDF2
from ebooklib import epub
import ebooklib
from bs4 import BeautifulSoup
# API åº“
from openai import OpenAI

# ==========================================
# 0. ç”¨æˆ·é…ç½®åŒº (åœ¨è¿™é‡Œå¡«ä½ çš„ Key)
# ==========================================
# âš ï¸âš ï¸âš ï¸ è¯·å°†ä½ çš„ DeepSeek API Key å¡«åœ¨ä¸‹é¢å¼•å·å†… âš ï¸âš ï¸âš ï¸
USER_API_KEY = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" 
API_BASE_URL = "https://api.deepseek.com"

# ==========================================
# 1. åŸºç¡€ç¯å¢ƒè®¾ç½®
# ==========================================
st.set_page_config(layout="centered", page_title="Vocab Master", page_icon="âš¡")

# NLTK è‡ªåŠ¨ä¿®å¤ (åŒ…å« punkt_tab)
@st.cache_resource
def init_nltk():
    resources = ['punkt', 'punkt_tab', 'averaged_perceptron_tagger', 'wordnet']
    for res in resources:
        try:
            if 'punkt' in res: nltk.data.find(f'tokenizers/{res}')
            else: nltk.data.find(f'*/{res}')
        except LookupError:
            nltk.download(res)

init_nltk()

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘å‡½æ•°
# ==========================================
def get_exam_tag(rank):
    if pd.isna(rank): return "æœªçŸ¥"
    rank = int(rank)
    if rank <= 2000: return "åŸºç¡€"
    if rank <= 4000: return "å››çº§/é«˜ä¸­"
    if rank <= 6000: return "å…­çº§/è€ƒç ”"
    if rank <= 9000: return "é›…æ€/æ‰˜ç¦"
    if rank <= 13000: return "GRE/ä¸“å…«"
    return "é«˜é˜¶åŸè‘—"

def extract_text(file):
    """é€šç”¨æ–‡æœ¬æå–"""
    try:
        ext = file.name.split('.')[-1].lower()
        if ext == 'txt': return file.getvalue().decode("utf-8")
        if ext == 'pdf':
            pdf = PyPDF2.PdfReader(file)
            return "\n".join([p.extract_text() for p in pdf.pages if p.extract_text()])
        if ext == 'epub':
            with open("temp.epub", "wb") as f: f.write(file.getbuffer())
            book = epub.read_epub("temp.epub")
            text = ""
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    text += BeautifulSoup(item.get_body_content(), 'html.parser').get_text() + "\n"
            os.remove("temp.epub")
            return text
        if ext in ['srt', 'vtt']:
            lines = [l for l in file.getvalue().decode("utf-8").splitlines() 
                     if not re.match(r'(\d{2}:\d{2})|(\d+$)', l.strip())]
            return "\n".join(lines)
    except Exception as e:
        return f"Error: {e}"
    return ""

def smart_lemmatize(text):
    clean = re.sub(r'[^a-zA-Z\s]', ' ', text)
    words = nltk.word_tokenize(clean)
    pos_tags = nltk.pos_tag(words)
    lemmas = []
    for w, t in pos_tags:
        if len(w) < 2: continue
        tag_map = {'J': 'ADJ', 'V': 'VERB', 'R': 'ADV'}
        pos = tag_map.get(t[0], 'NOUN')
        lemma = lemminflect.getLemma(w, upos=pos)
        lemmas.append(lemma[0].lower() if lemma else w.lower())
    return lemmas

# ==========================================
# 3. UI ç•Œé¢ (ä»¿åŸç‰ˆï¼Œæ— ä¾§è¾¹æ )
# ==========================================
st.title("âš¡ Vocab Master")

# ç¬¬ä¸€è¡Œï¼šä¸Šä¼  + æ–‡æœ¬æ¡† (ç´§å‡‘å¸ƒå±€)
c1, c2 = st.columns([1, 1])
with c1:
    f = st.file_uploader("å¯¼å…¥æ–‡ä»¶ (TXT/PDF/EPUB/SRT)", type=['txt','pdf','epub','srt'])
with c2:
    t = st.text_area("ç›´æ¥ç²˜è´´æ–‡æœ¬", height=68, placeholder="åœ¨æ­¤ç²˜è´´è‹±æ–‡å†…å®¹...")

# è·å–è¾“å…¥
raw = ""
if f: raw = extract_text(f)
elif t: raw = t

# è®¾ç½®åŒº (æŠ˜å èµ·æ¥ï¼Œä¸å åœ°)
with st.expander("âš™ï¸ ç­›é€‰è®¾ç½® (Rank èŒƒå›´ / ç»Ÿè®¡å›¾è¡¨)", expanded=False):
    sc1, sc2 = st.columns(2)
    with sc1:
        # ä½¿ç”¨åŸæ¥çš„æ•°å­—è¾“å…¥æ–¹å¼ï¼Œå¯èƒ½æ¯”æ»‘å—æ›´æå®¢
        min_r = st.number_input("æœ€å° Rank", value=4000, step=1000)
    with sc2:
        max_r = st.number_input("æœ€å¤§ Rank", value=15000, step=1000)
    show_chart = st.checkbox("æ˜¾ç¤ºå¯è§†åŒ–å›¾è¡¨", value=True)

# å¤„ç†é€»è¾‘
if raw:
    st.divider()
    with st.spinner("Analyzing..."):
        # 1. è§£æ
        words = smart_lemmatize(raw)
        counts = pd.Series(words).value_counts().reset_index()
        counts.columns = ['word', 'count']
        
        # 2. è¯»å–/ç”Ÿæˆæ•°æ®
        try:
            coca = pd.read_csv('coca20000.csv')
            if 'lemma' in coca.columns: coca.rename(columns={'lemma':'word'}, inplace=True)
        except:
            # æ²¡æ–‡ä»¶æ—¶çš„ Mock æ•°æ®
            import numpy as np
            coca = pd.DataFrame({'word': counts['word'], 'rank': np.random.randint(1,20000, len(counts))})

        # 3. ç­›é€‰
        merged = pd.merge(counts, coca, on='word')
        merged['tag'] = merged['rank'].apply(get_exam_tag)
        final = merged[(merged['rank']>=min_r) & (merged['rank']<=max_r)].sort_values('rank')

    # ç»“æœå±•ç¤º
    st.markdown(f"**åˆ†æç»“æœï¼š** åŸæ–‡ {len(words)} è¯ | ğŸ¯ å‘½ä¸­ç”Ÿè¯ **{len(final)}** ä¸ª")
    
    if show_chart and not final.empty:
        st.bar_chart(final['tag'].value_counts())

    if not final.empty:
        # ç´§å‡‘çš„æ•°æ®å±•ç¤º
        st.dataframe(
            final[['word', 'rank', 'tag', 'count']], 
            use_container_width=True,
            hide_index=True
        )
        
        # AI ç”ŸæˆåŒº
        st.divider()
        st.subheader("DeepSeek è§£é‡Šç”Ÿæˆ")
        
        # è‡ªåŠ¨æå–å‰ 50 ä¸ªè¯
        target_list = final['word'].head(50).tolist()
        
        default_prompt = f"""è¯·åˆ†æä»¥ä¸‹å•è¯ï¼Œè¾“å‡º CSV æ ¼å¼ï¼ˆç«–çº¿ | åˆ†éš”ï¼‰ï¼Œä¸å¸¦è¡¨å¤´ã€‚
å­—æ®µï¼šå•è¯ | éŸ³æ ‡ | è¯æ€§ | ä¸­æ–‡ç®€æ˜é‡Šä¹‰ | è‹±æ–‡è¯­å¢ƒä¾‹å¥ | è®°å¿†æ³•
å•è¯ï¼š{', '.join(target_list)}"""

        # å…è®¸ç”¨æˆ·æœ€åä¿®æ”¹ä¸€ä¸‹ Prompt
        final_prompt = st.text_area("Prompt", value=default_prompt, height=100)
        
        if st.button("ğŸš€ å¼€å§‹ç”Ÿæˆ (ä½¿ç”¨å†…ç½® Key)", type="primary"):
            if "sk-" not in USER_API_KEY:
                st.error("è¯·å…ˆåœ¨ä»£ç ç¬¬ 19 è¡Œå¡«å…¥æ­£ç¡®çš„ API Keyï¼")
            else:
                client = OpenAI(api_key=USER_API_KEY, base_url=API_BASE_URL)
                output_box = st.empty()
                full_text = ""
                
                try:
                    stream = client.chat.completions.create(
                        model="deepseek-chat",
                        messages=[{"role": "user", "content": final_prompt}],
                        stream=True
                    )
                    for chunk in stream:
                        if chunk.choices[0].delta.content:
                            full_text += chunk.choices[0].delta.content
                            output_box.code(full_text + "â–Œ", language="csv")
                    
                    output_box.code(full_text, language="csv") # æœ€ç»ˆç»“æœå»é™¤å…‰æ ‡
                    st.success("ç”Ÿæˆå®Œæˆï¼å¯ç›´æ¥å¤åˆ¶ä¸Šæ–¹å†…å®¹ã€‚")
                except Exception as e:
                    st.error(f"API è¯·æ±‚å¤±è´¥: {e}")