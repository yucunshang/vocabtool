import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import json
import time
import requests

# å°è¯•å¯¼å…¥å¤šæ ¼å¼æ–‡æ¡£å¤„ç†åº“ï¼Œå¦‚æœæ²¡æœ‰åˆ™æç¤º
try:
    import PyPDF2
    import docx
except ImportError:
    st.error("âš ï¸ ç¼ºå°‘æ–‡ä»¶å¤„ç†ä¾èµ–ã€‚è¯·åœ¨ç»ˆç«¯è¿è¡Œ: pip install PyPDF2 python-docx")

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode { font-family: 'Consolas', 'Courier New', monospace !important; font-size: 16px !important; }
    header {visibility: hidden;} footer {visibility: hidden;}
    .block-container { padding-top: 1rem; }
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    [data-testid="stMetricValue"] { font-size: 28px !important; color: var(--primary-color) !important; }
    .param-box { background-color: var(--secondary-background-color); padding: 15px 20px 5px 20px; border-radius: 10px; border: 1px solid var(--border-color-light); margin-bottom: 20px; }
    .copy-hint { color: #888; font-size: 14px; margin-bottom: 5px; margin-top: 10px; padding-left: 5px; }
    .exam-tag { font-size: 12px; background: #e0e0e0; color: #333; padding: 2px 6px; border-radius: 4px; margin-left: 8px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ•°æ®ä¸ NLP åˆå§‹åŒ–
# ==========================================
@st.cache_data
def load_knowledge_base():
    try:
        with open('data/terms.json', 'r', encoding='utf-8') as f: terms = {k.lower(): v for k, v in json.load(f).items()}
        with open('data/proper.json', 'r', encoding='utf-8') as f: proper = {k.lower(): v for k, v in json.load(f).items()}
        with open('data/patch.json', 'r', encoding='utf-8') as f: patch = json.load(f)
        with open('data/ambiguous.json', 'r', encoding='utf-8') as f: ambiguous = set(json.load(f))
        return terms, proper, patch, ambiguous
    except FileNotFoundError:
        st.error("âš ï¸ ç¼ºå°‘ data/ æ–‡ä»¶å¤¹ä¸‹çš„ JSON çŸ¥è¯†åº“æ–‡ä»¶ï¼")
        return {}, {}, {}, set()

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    for pkg in ['averaged_perceptron_tagger', 'punkt']:
        try: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
        except: pass
setup_nltk()

def get_lemma(w):
    """æå–è¯æ ¹ (æ›´ç»†ç²’åº¦)"""
    lemmas_dict = lemminflect.getAllLemmas(w)
    if not lemmas_dict: return w.lower()
    for pos in ['ADJ', 'ADV', 'VERB', 'NOUN']:
        if pos in lemmas_dict: return lemmas_dict[pos][0]
    return list(lemmas_dict.values())[0][0]

@st.cache_data
def load_vocab():
    vocab = {}
    file_path = next((f for f in ["coca_cleaned.csv", "data.csv"] if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
            df = df.sort_values(r_col, ascending=True).drop_duplicates(subset=[w_col], keep='first')
            vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except: pass
    
    for word, rank in BUILTIN_PATCH_VOCAB.items(): vocab[word] = rank
    URGENT_OVERRIDES = {
        "china": 400, "turkey": 1500, "march": 500, "may": 100, "august": 1500, "polish": 2500,
        "monday": 300, "tuesday": 300, "wednesday": 300, "thursday": 300, "friday": 300, "saturday": 300, "sunday": 300,
        "january": 400, "february": 400, "april": 400, "june": 400, "july": 400, "september": 400, "october": 400, "november": 400, "december": 400,
        "usa": 200, "uk": 200, "google": 1000, "apple": 1000, "microsoft": 1500
    }
    for word, rank in URGENT_OVERRIDES.items(): vocab[word] = rank
    return vocab

vocab_dict = load_vocab()

# ==========================================
# 3. æ ¸å¿ƒåŠŸèƒ½æ˜ å°„ï¼šè€ƒè¯•å¤§çº² & AI
# ==========================================
def get_exam_syllabus(rank):
    """å†…ç½® COCA Rank åˆ° å›½å†…å¤–è€ƒè¯•å¤§çº² çš„æ˜ å°„å…³ç³»"""
    if rank == 99999: return "æœªæ”¶å½•/è¶…çº²"
    if rank <= 1500: return "å°å­¦/åˆä¸­"
    if rank <= 3500: return "ä¸­è€ƒæ ¸å¿ƒ"
    if rank <= 5500: return "é«˜è€ƒæ ¸å¿ƒ"
    if rank <= 7500: return "CET-4 (å››çº§)"
    if rank <= 9500: return "CET-6 (å…­çº§)"
    if rank <= 13000: return "è€ƒç ”/é›…æ€"
    if rank <= 20000: return "æ‰˜ç¦/GRE"
    return "æéš¾è¯æ±‡"

def extract_text_from_file(uploaded_file):
    """æ”¯æŒ txt, pdf, docx å¤šç§æ ¼å¼è§£æ"""
    ext = uploaded_file.name.split('.')[-1].lower()
    try:
        if ext == 'txt':
            return uploaded_file.getvalue().decode("utf-8", errors="ignore")
        elif ext == 'pdf':
            reader = PyPDF2.PdfReader(uploaded_file)
            return " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif ext == 'docx':
            doc = docx.Document(uploaded_file)
            return " ".join([p.text for p in doc.paragraphs])
    except Exception as e:
        st.error(f"æ–‡ä»¶è§£æå¤±è´¥: {e}")
        return ""
    return ""

def call_deepseek_api(api_key, prompt_template, words):
    """è°ƒç”¨ DeepSeek æ¥å£ç›´æ¥ç”Ÿæˆåˆ¶å¡ CSV"""
    if not api_key: return "âš ï¸ é”™è¯¯ï¼šæœªæä¾› API Key æˆ– ç®¡ç†å‘˜å¯†ç ã€‚"
    if not words: return "âš ï¸ é”™è¯¯ï¼šæ²¡æœ‰éœ€è¦ç”Ÿæˆçš„å•è¯ã€‚"
    
    url = "https://api.deepseek.com/chat/completions"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    full_prompt = f"{prompt_template}\n\nå¾…å¤„ç†å•è¯ï¼š\n{', '.join(words)}"
    
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": full_prompt}],
        "temperature": 0.3
    }
    
    try:
        resp = requests.post(url, json=payload, headers=headers)
        resp.raise_for_status()
        return resp.json()['choices'][0]['message']['content']
    except Exception as e:
        return f"ğŸš¨ API è°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ– Key æ˜¯å¦æ­£ç¡®ã€‚\nè¯¦ç»†é”™è¯¯: {str(e)}"

# ==========================================
# 4. åˆ†æå¼•æ“
# ==========================================
def analyze_words(unique_word_list):
    unique_items = [] 
    JUNK_WORDS = {'s', 't', 'd', 'm', 'll', 've', 're'}
    for item_lower in unique_word_list:
        if len(item_lower) < 2 and item_lower not in ['a', 'i']: continue
        if item_lower in JUNK_WORDS: continue
        actual_rank = vocab_dict.get(item_lower, 99999)
        
        syllabus = get_exam_syllabus(actual_rank if actual_rank != 99999 else 99999)
        
        if item_lower in BUILTIN_TECHNICAL_TERMS:
            domain = BUILTIN_TECHNICAL_TERMS[item_lower]
            term_rank = actual_rank if actual_rank != 99999 else 15000
            unique_items.append({"word": f"{item_lower} ({domain})", "rank": term_rank, "raw": item_lower, "syllabus": "ä¸“ä¸šæœ¯è¯­"})
            continue
        
        if item_lower in PROPER_NOUNS_DB or item_lower in AMBIGUOUS_WORDS:
            display = PROPER_NOUNS_DB.get(item_lower, item_lower.title())
            unique_items.append({"word": display, "rank": actual_rank, "raw": item_lower, "syllabus": "ä¸“æœ‰åè¯"})
            continue
            
        if actual_rank != 99999:
            unique_items.append({"word": item_lower, "rank": actual_rank, "raw": item_lower, "syllabus": syllabus})
            
    return pd.DataFrame(unique_items)

# ==========================================
# 5. UI ä¸æµæ°´çº¿
# ==========================================
st.title("ğŸš€ Vocab Master Pro - å…¨èƒ½æ™ºèƒ½æ•™ç ”å¼•æ“")
st.markdown("ğŸ’¡ æ”¯æŒç²˜è´´é•¿æ–‡æˆ–ä¸Šä¼  `TXT / PDF / DOCX`ï¼Œè‡ªåŠ¨å¤§çº²æ˜ å°„ï¼Œå¹¶å†…ç½® **DeepSeek AI** ä¸€é”®ç”Ÿæˆ Anki è®°å¿†å¡ç‰‡ã€‚")

if "raw_input_text" not in st.session_state: st.session_state.raw_input_text = ""
if "uploader_key" not in st.session_state: st.session_state.uploader_key = 0 
def clear_all_inputs():
    st.session_state.raw_input_text = ""
    st.session_state.uploader_key += 1 

# --- å‚æ•°é…ç½®åŒº ---
st.markdown("<div class='param-box'>", unsafe_allow_html=True)
c1, c2, c3, c4, c5 = st.columns(5)
with c1: current_level = st.number_input("ğŸ¯ å½“å‰æ°´å¹³ (èµ·)", 0, 30000, 7500, 500, help="ä½äºæ­¤è¯é¢‘çš„è§†ä¸ºå·²æŒæ¡")
with c2: target_level = st.number_input("ğŸ¯ ç›®æ ‡æ°´å¹³ (æ­¢)", 0, 30000, 15000, 500, help="é«˜äºæ­¤è¯é¢‘çš„è§†ä¸ºè¶…çº²")
with c3: top_n = st.number_input("ğŸ”¥ ç²¾é€‰ Top N", 10, 500, 50, 10)
with c4: min_rank_threshold = st.number_input("ğŸ“‰ å¿½ç•¥å‰ N è¯", 0, 20000, 3500, 500)
with c5: 
    st.write("") 
    st.write("") 
    show_visual = st.checkbox("ğŸ“Š æ˜¾ç¤ºå¯è§†åŒ–åé¦ˆ", value=True)
st.markdown("</div>", unsafe_allow_html=True)

# --- åŒé€šé“å¤šæ ¼å¼è¾“å…¥ ---
col_input1, col_input2 = st.columns([3, 2])
with col_input1:
    raw_text = st.text_area("ğŸ“¥ ç²˜è´´æ–‡æœ¬ (æ”¯æŒ10ä¸‡å­—ä»¥å†…)", height=150, key="raw_input_text")
with col_input2:
    st.info("ğŸ’¡ **å¤šæ ¼å¼è§£æ**ï¼šæ”¯æŒè¶…å¤§ `.txt`, `.pdf`, `.docx` åŸè‘—æ–‡ä»¶ ğŸ‘‡")
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£", type=["txt", "pdf", "docx"], key=f"uploader_{st.session_state.uploader_key}")

col_btn1, col_btn2 = st.columns([5, 1])
with col_btn1: btn_process = st.button("ğŸš€ æé€Ÿæ™ºèƒ½è§£æ", type="primary", use_container_width=True)
with col_btn2: st.button("ğŸ—‘ï¸ ä¸€é”®æ¸…ç©º", on_click=clear_all_inputs, use_container_width=True)

st.divider()

combined_text = raw_text
if uploaded_file is not None:
    combined_text += "\n" + extract_text_from_file(uploaded_file)

if btn_process and combined_text.strip() and vocab_dict:
    start_time = time.time()
    
    with st.spinner("ğŸ§  æ­£åœ¨è¿›è¡Œå¤šçº¿ç¨‹è¯æ±‡æ‹†è§£ä¸å¤§çº²æ˜ å°„..."):
        raw_words = re.findall(r"[a-zA-Z']+", combined_text)
        lemmatized_words = [get_lemma(w) for w in raw_words]
        full_lemmatized_text = " ".join(lemmatized_words)
        
        unique_lemmas = list(set([w.lower() for w in lemmatized_words]))
        df = analyze_words(unique_lemmas)
        
        process_time = time.time() - start_time
        
        col_m1, col_m2, col_m3, col_m4 = st.columns(4)
        col_m1.metric(label="ğŸ“ è§£ææ€»å­—æ•°", value=f"{len(raw_words):,}")
        col_m2.metric(label="âœ‚ï¸ å»é‡è¯æ ¹æ•°", value=f"{len(unique_lemmas):,}")
        col_m3.metric(label="ğŸ¯ çº³å…¥åˆ†çº§è¯æ±‡", value=f"{len(df):,}")
        col_m4.metric(label="âš¡ æé€Ÿè§£æè€—æ—¶", value=f"{process_time:.2f} ç§’")
        
        if not df.empty:
            # === å¯è§†åŒ–åé¦ˆåŒº (å¯é€‰) ===
            if show_visual:
                st.subheader("ğŸ“Š è¯æ±‡åˆ†å¸ƒå¤§çº²é›·è¾¾å›¾")
                chart_data = df['syllabus'].value_counts()
                st.bar_chart(chart_data, color="#ff4b4b")
                st.caption("ğŸ‘† é€šè¿‡ä¸Šå›¾å¯ç›´è§‚åˆ¤æ–­è¿™ç¯‡æ–‡ç« å¯¹åº”å›½å†…å“ªç§è€ƒè¯•éš¾åº¦ã€‚")
                st.divider()
            
            def categorize(row):
                r = row['rank']
                if r <= current_level: return "known"
                elif r <= target_level: return "target"
                else: return "beyond"
            
            df['final_cat'] = df.apply(categorize, axis=1)
            df = df.sort_values(by='rank')
            top_df = df[df['rank'] >= min_rank_threshold].sort_values(by='rank', ascending=True).head(top_n)
            
            t_top, t_target, t_beyond, t_known, t_raw = st.tabs([
                f"ğŸ”¥ Top {len(top_df)}", f"ğŸŸ¡ é‡ç‚¹ ({len(df[df['final_cat']=='target'])})", 
                f"ğŸ”´ è¶…çº² ({len(df[df['final_cat']=='beyond'])})", f"ğŸŸ¢ å·²æŒæ¡ ({len(df[df['final_cat']=='known'])})",
                "ğŸ“ åŸæ–‡é˜²å¡æ­»ä¸‹è½½"
            ])
            
            # --- AI åŠ¨æ€ Prompt å®šä¹‰ ---
            default_prompt = """è¯·æ‰®æ¼”ä¸€ä½ä¸“ä¸šçš„ Anki åˆ¶å¡ä¸“å®¶ã€‚è¯·ä¸¥æ ¼ä¸ºä»¥ä¸‹å•è¯ç”Ÿæˆ CSV å¯¼å…¥æ ¼å¼ã€‚
æ ¸å¿ƒåŸåˆ™ï¼š
1. æç®€é€Ÿè®°ï¼šä»…æä¾›1ä¸ªæœ€æ ¸å¿ƒã€æœ€ç¬¦åˆç°ä»£è¯­å¢ƒçš„é‡Šä¹‰ã€‚
2. ç»“æ„(æ¯å­—æ®µç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼Œå†…å®¹åŠ åŒå¼•å·)ï¼š"å•è¯æˆ–çŸ­è¯­", "è‹±æ–‡é‡Šä¹‰<br><br><em>æ–œä½“ä¾‹å¥</em><br><br>ä¸­æ–‡åŠ©è®°"
è¯·ç›´æ¥è¾“å‡ºæ ‡å‡† CSV ä»£ç å—ï¼Œä¸è¦åŒ…å«ä»»ä½•å¤šä½™è§£é‡Šã€‚"""

            def render_tab(tab_obj, data_df, label, expand_default=False, df_key=""):
                with tab_obj:
                    if not data_df.empty:
                        pure_words = data_df['word'].tolist()
                        
                        # å±•ç¤ºå¤§çº²æ˜ å°„æ ‡ç­¾
                        display_lines = []
                        for _, row in data_df.iterrows():
                            rank_str = str(int(row['rank'])) if row['rank'] != 99999 else "æœªæ”¶å½•"
                            display_lines.append(f"{row['word']} [Rank: {rank_str}] - ã€{row['syllabus']}ã€‘")
                        
                        with st.expander("ğŸ‘ï¸ æŸ¥çœ‹å¸¦æœ‰å¤§çº²æ˜ å°„çš„å•è¯åˆ—è¡¨", expanded=expand_default):
                            st.code("\n".join(display_lines), language='text')
                        
                        # ==========================================
                        # ğŸ¤– åŸç”Ÿå†…ç½® DeepSeek AI å¼•æ“ (å®‰å…¨é‰´æƒç‰ˆ)
                        # ==========================================
                        st.markdown(f"#### ğŸ¤– AI ä¸€é”®åˆ¶å¡å¼•æ“ ({label})")
                        
                        col_ai1, col_ai2 = st.columns([1, 1])
                        with col_ai1:
                            ai_pwd = st.text_input("ğŸ”‘ é‰´æƒå¯†ç  / API Key", type="password", placeholder="è¾“å…¥ç«™é•¿å¯†ç æˆ–æ‚¨è‡ªå·±çš„ DeepSeek Key", key=f"pwd_{df_key}")
                        with col_ai2:
                            st.write("")
                            st.write("")
                            st.caption("è®¿å®¢å¿…é¡»è‡ªå¤‡ Keyï¼›ç«™é•¿è¾“å…¥ç‰¹æƒå¯†ç å³å¯ç›´æ¥è°ƒç”¨å†…ç½®é¢åº¦ã€‚")
                        
                        custom_prompt = st.text_area("ğŸ“ è‡ªå®šä¹‰ AI Prompt (å¯åŠ¨æ€ä¿®æ”¹)", value=default_prompt, height=150, key=f"prompt_{df_key}")
                        
                        if st.button("âš¡ å¬å”¤ DeepSeek ç«‹å³ç”Ÿæˆ CSV", key=f"btn_{df_key}", type="primary"):
                            with st.spinner("AI æ­£åœ¨å…‰é€Ÿç¼–çº‚å¡ç‰‡ï¼Œè¯·ç¨å€™..."):
                                # --- æ ¸å¿ƒé‰´æƒé€»è¾‘ ---
                                actual_key = ""
                                try:
                                    # å¦‚æœè¾“å…¥çš„å¯†ç ç­‰äºåå°è®¾ç½®çš„ç«™é•¿å¯†ç ï¼Œåˆ™æå–éšè—çš„ API Key
                                    if ai_pwd == st.secrets["APP_PASSWORD"]:
                                        actual_key = st.secrets["DEEPSEEK_API_KEY"]
                                    else:
                                        # å¦åˆ™ï¼ŒæŠŠç”¨æˆ·è¾“å…¥çš„å½“æˆä»–ä»¬è‡ªå·±çš„ API Key
                                        actual_key = ai_pwd
                                except:
                                    # æœ¬åœ°æµ‹è¯•å¦‚æœæ²¡æœ‰ secrets æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨è¾“å…¥çš„å­—ç¬¦ä¸²
                                    actual_key = ai_pwd
                                
                                ai_result = call_deepseek_api(actual_key, custom_prompt, pure_words)
                                
                                st.success("ğŸ‰ ç”Ÿæˆå®Œæˆï¼")
                                st.code(ai_result, language="markdown")
                                
                                # æ”¯æŒç›´æ¥æŠŠ AI ç»“æœå­˜æˆ CSV æ–‡ä»¶ä¸‹è½½
                                st.download_button(
                                    label="ğŸ“¥ ç›´æ¥ä¸‹è½½ç”Ÿæˆçš„ Anki å¡ç‰‡ (.csv)",
                                    data=ai_result,
                                    file_name=f"anki_cards_{label}.csv",
                                    mime="text/csv"
                                )
                    else: st.info("è¯¥åŒºé—´æš‚æ— å•è¯")

            render_tab(t_top, top_df, "Topç²¾é€‰", expand_default=True, df_key="top") 
            render_tab(t_target, df[df['final_cat']=='target'], "é‡ç‚¹", expand_default=False, df_key="target")
            render_tab(t_beyond, df[df['final_cat']=='beyond'], "è¶…çº²", expand_default=False, df_key="beyond")
            render_tab(t_known, df[df['final_cat']=='known'], "ç†Ÿè¯", expand_default=False, df_key="known")
            
            with t_raw:
                st.info("ğŸ’¡ è¿™æ˜¯è‡ªåŠ¨è¯å½¢è¿˜åŸåçš„å…¨æ–‡è¾“å‡ºï¼Œå·²é’ˆå¯¹é•¿æ–‡ä¼˜åŒ–é˜²å¡æ­»ä½“éªŒã€‚")
                st.download_button(label="ğŸ’¾ ä¸€é”®ä¸‹è½½å®Œæ•´è¯å½¢è¿˜åŸåŸæ–‡ (.txt)", data=full_lemmatized_text, file_name="lemmatized_text.txt", mime="text/plain", type="primary")
                if len(full_lemmatized_text) > 50000:
                    st.warning("âš ï¸ æ–‡æœ¬è¶…é•¿ï¼Œä»…å±•ç¤ºå‰ 50,000 å­—ç¬¦ã€‚")
                    st.code(full_lemmatized_text[:50000] + "\n\n... [è¯·ä¸‹è½½æŸ¥çœ‹å®Œæ•´å†…å®¹] ...", language='text')
                else:
                    st.code(full_lemmatized_text, language='text')