import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import time

# ==========================================
# 0. åŸºç¡€é…ç½® (ç§»åŠ¨ç«¯ä¼˜åŒ–)
# ==========================================
st.set_page_config(
    page_title="Vocab Master Pro", 
    page_icon="âš¡ï¸", 
    layout="wide", #ä»¥æ­¤å®¹çº³è¡¨æ ¼
    initial_sidebar_state="expanded" # é»˜è®¤å±•å¼€ä¾§è¾¹æ ä»¥ä¾¿çœ‹åˆ°æŒ‰é’®
)

st.markdown("""
<style>
    /* ç•Œé¢ä¼˜åŒ– */
    .block-container { padding-top: 1rem; padding-bottom: 5rem; }
    #MainMenu {visibility: hidden;} footer {visibility: hidden;} header {visibility: hidden;}
    
    /* ä¾§è¾¹æ ä¼˜åŒ– */
    [data-testid="stSidebar"] { background-color: #f9f9f9; }
    
    /* æŒ‰é’®å¤§å°ºå¯¸ */
    .stButton>button {
        width: 100%; border-radius: 8px; height: 3em; font-weight: bold; font-size: 16px !important;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    
    /* è¡¨æ ¼ç¼–è¾‘å™¨ä¼˜åŒ– */
    [data-testid="stDataFrameResizable"] { border: 1px solid #ddd; border-radius: 8px; }
    
    /* æç¤ºæ¡† */
    .stAlert { border-radius: 8px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºåŠ è½½
# ==========================================
@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    try: 
        nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_dir, quiet=True)
        nltk.download('punkt', download_dir=nltk_data_dir, quiet=True)
    except: pass
setup_nltk()

@st.cache_data
def load_data():
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c), cols[1])
            
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            df = df.dropna(subset=[r_col])
            # é»˜è®¤æŒ‰ Rank å‡åº (1, 2, 3...)
            df = df.sort_values(r_col)
            
            vocab_dict = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
            return vocab_dict, df, r_col, w_col
        except Exception as e:
            st.error(f"æ•°æ®åŠ è½½å‡ºé”™: {e}")
            return {}, None, None, None
    return {}, None, None, None

VOCAB_DICT, FULL_DF, RANK_COL, WORD_COL = load_data()
def get_lemma(word): return lemminflect.getLemma(word, upos='VERB')[0] 

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘
# ==========================================
def analyze_text(text, current_lvl, target_lvl):
    """åˆ†ææ–‡æœ¬ï¼Œè¿”å› DataFrame ä»¥ä¾¿ç¼–è¾‘å™¨ä½¿ç”¨"""
    raw_words = re.findall(r"[a-z]+", text.lower())
    unique_words = set(raw_words)
    
    data_list = [] # å­˜æ”¾å­—å…¸ {'Word': w, 'Rank': r, 'Category': c}
    
    for w in unique_words:
        if len(w) < 2: continue
        lemma = get_lemma(w)
        rank = VOCAB_DICT.get(lemma, 99999) # 99999 ä»£è¡¨æœªæ”¶å½•
        
        category = "Beyond"
        if rank <= current_lvl: category = "Mastered"
        elif rank <= target_lvl: category = "Target"
        
        data_list.append({"Word": lemma, "Rank": int(rank), "Category": category})
        
    df = pd.DataFrame(data_list)
    return df

def generate_prompt(word_list, settings):
    word_str = ", ".join(word_list)
    fmt = settings.get("format", "CSV")
    ex_count = settings.get("example_count", 1)
    lang = settings.get("lang", "Chinese")
    
    prompt = f"""Role: High-Efficiency Anki Card Creator
Task: Convert the provided word list into a strict {fmt} data block.

--- OUTPUT FORMAT RULES ---
1. Structure: {'2 Columns (Front, Back)' if fmt=='CSV' else 'Custom Text Format'}.
   Format: "Front","Back"
   Header: **Do NOT output a header row.**

2. Column 1 (Front):
   - Content: A natural, short English phrase/collocation.
   - Style: **ALL LOWERCASE**.

3. Column 2 (Back):
   - Content: Definition + {ex_count} Example(s) + Etymology.
   - HTML Layout: Definition <br> <br> <em>Example</em> <br> <br> ã€æºã€‘Etymology
   - Definition Language: {lang} & English concise (Start with lowercase).
   - Example Style: **Start with UPPERCASE**. Wrapped in <em>.
   - Spacing: Double <br> tags.

4. Etymology Style:
   - Only explain roots/affixes in {lang}.
   - Format: ã€æºã€‘Root (Meaning) + Affix (Meaning)
   - Do NOT explain the final word meaning.

5. Atomicity: Separate rows for distinct meanings.

--- WORD LIST ({len(word_list)} words) ---
{word_str}
"""
    return prompt

# ==========================================
# 3. ä¾§è¾¹æ ï¼šå…¨å±€æ§åˆ¶ (éœ€æ±‚1ï¼šæŒ‰é’®ä¸€ç›´åœ¨)
# ==========================================
with st.sidebar:
    st.header("ğŸ›ï¸ æ§åˆ¶å°")
    
    # æ¨¡å¼é€‰æ‹©
    mode = st.radio("æ¨¡å¼", ["ğŸ“– æ–‡æœ¬æå–", "ğŸ”¢ è¯é¢‘åˆ·è¯", "ğŸ› ï¸ æ ¼å¼è½¬æ¢"])
    st.divider()
    
    # å…¨å±€ Prompt è®¾ç½®
    with st.expander("âš™ï¸ ç”Ÿæˆè®¾ç½®", expanded=False):
        set_format = st.selectbox("æ ¼å¼", ["CSV", "TXT"], index=0)
        set_lang = st.selectbox("è¯­è¨€", ["Chinese", "English"], index=0)
        set_ex_count = st.number_input("ä¾‹å¥æ•°", 1, 3, 1)
    
    settings = {"format": set_format, "lang": set_lang, "example_count": set_ex_count}
    
    # --- æå–æ¨¡å¼çš„è¾“å…¥ ---
    if mode == "ğŸ“– æ–‡æœ¬æå–":
        st.subheader("1. è¾“å…¥æ–‡æœ¬")
        curr_lvl = st.number_input("å½“å‰æ°´å¹³", 4000, step=500)
        targ_lvl = st.number_input("ç›®æ ‡æ°´å¹³", 8000, step=500)
        
        inp_type = st.radio("æ¥æº", ["ç²˜è´´", "æ–‡ä»¶"], horizontal=True)
        
        user_text = ""
        if inp_type == "ç²˜è´´":
            user_text = st.text_area("åœ¨æ­¤ç²˜è´´", height=150)
        else:
            up = st.file_uploader("ä¸Šä¼  (TXT/PDF)", type=["txt","pdf"])
            if up:
                try:
                    if up.name.endswith('.txt'): user_text = up.getvalue().decode("utf-8")
                    else: 
                        import PyPDF2
                        r = PyPDF2.PdfReader(up)
                        user_text = " ".join([p.extract_text() for p in r.pages])
                except: st.error("æ–‡ä»¶è¯»å–å¤±è´¥")
        
        # ğŸŸ¢ éœ€æ±‚1ï¼šåˆ†ææŒ‰é’®æ”¾åœ¨ Sidebarï¼Œæ°¸è¿œå¯è§
        analyze_clicked = st.button("ğŸ” å¼€å§‹åˆ†æ", type="primary")

# ==========================================
# 4. ä¸»ç•Œé¢ï¼šç»“æœå±•ç¤ºä¸ç¼–è¾‘
# ==========================================
st.title("âš¡ï¸ Vocab Master Pro")

if FULL_DF is None:
    st.error("âš ï¸ ç¼ºå°‘è¯é¢‘æ–‡ä»¶")
else:
    # ------------------------------------------------
    # æ¨¡å¼ A: æ–‡æœ¬æå– (æ ¸å¿ƒå‡çº§)
    # ------------------------------------------------
    if mode == "ğŸ“– æ–‡æœ¬æå–":
        # ä½¿ç”¨ Session State ä¿å­˜åˆ†æç»“æœï¼Œé˜²æ­¢åˆ·æ–°ä¸¢å¤±
        if analyze_clicked and user_text:
            with st.spinner("æ­£åœ¨æé€Ÿåˆ†æ..."):
                t0 = time.time()
                df_result = analyze_text(user_text, curr_lvl, targ_lvl)
                st.session_state['analysis_df'] = df_result
                st.session_state['analysis_time'] = time.time() - t0
        
        if 'analysis_df' in st.session_state:
            df = st.session_state['analysis_df']
            t_taken = st.session_state.get('analysis_time', 0)
            
            st.success(f"âœ… åˆ†æå®Œæˆï¼å…± {len(df)} è¯ (è€—æ—¶ {t_taken:.2f}s)")
            
            # åˆ†ç±»ç­›é€‰
            df_target = df[df['Category'] == 'Target'].copy()
            df_mastered = df[df['Category'] == 'Mastered'].copy()
            df_beyond = df[df['Category'] == 'Beyond'].copy()
            
            # ğŸŸ¢ éœ€æ±‚3ï¼šé‡ç‚¹è¯æŒ‰ Rank ä»é«˜åˆ°ä½ (éš¾ -> æ˜“)
            df_target = df_target.sort_values(by="Rank", ascending=False)
            
            tab1, tab2, tab3 = st.tabs([
                f"ğŸ¯ é‡ç‚¹ ({len(df_target)})", 
                f"âœ… å·²æŒæ¡ ({len(df_mastered)})", 
                f"ğŸš€ è¶…çº² ({len(df_beyond)})"
            ])
            
            # --- é‡ç‚¹è¯ Tab (å¯ç¼–è¾‘ + åˆ†æ‰¹) ---
            with tab1:
                st.markdown("### ğŸ“ ç¼–è¾‘é‡ç‚¹è¯åˆ—è¡¨")
                st.caption("æç¤ºï¼šä½ å¯ä»¥ç›´æ¥ä¿®æ”¹å•è¯ï¼Œæˆ–åœ¨æœ€åä¸€è¡Œæ·»åŠ æ–°è¯ã€‚å‹¾é€‰å·¦ä¾§å¤é€‰æ¡†å¹¶æŒ‰ Delete å¯åˆ é™¤è¡Œã€‚")
                
                # ğŸŸ¢ éœ€æ±‚2ï¼šå¯ç¼–è¾‘è¡¨æ ¼ (Data Editor)
                # num_rows="dynamic" å…è®¸æ·»åŠ /åˆ é™¤è¡Œ
                edited_df = st.data_editor(
                    df_target[["Word", "Rank"]],
                    num_rows="dynamic",
                    use_container_width=True,
                    key="editor_target",
                    column_config={
                        "Rank": st.column_config.NumberColumn("Rank (è¶Šå¤§è¶Šç”Ÿåƒ»)")
                    }
                )
                
                # è·å–ç¼–è¾‘åçš„æœ€ç»ˆåˆ—è¡¨
                final_words = edited_df["Word"].tolist()
                final_words = [str(w).strip() for w in final_words if str(w).strip()] # æ¸…æ´—ç©ºè¡Œ
                
                if final_words:
                    st.divider()
                    st.markdown("### ğŸš€ ç”Ÿæˆ AI æŒ‡ä»¤")
                    
                    # ğŸŸ¢ éœ€æ±‚4ï¼šåˆ†æ‰¹å¤„ç†é€»è¾‘
                    BATCH_SIZE = 30  # å»ºè®®æ‰¹æ¬¡å¤§å°
                    total_words = len(final_words)
                    
                    if total_words > BATCH_SIZE:
                        st.warning(f"âš ï¸ å•è¯æ€»æ•° ({total_words}) è¾ƒå¤šï¼Œå»ºè®®åˆ†æ‰¹ç”Ÿæˆä»¥ä¿è¯ AI è¾“å‡ºè´¨é‡ã€‚")
                        
                        # è®¡ç®—æ‰¹æ¬¡æ•°
                        num_batches = (total_words // BATCH_SIZE) + (1 if total_words % BATCH_SIZE != 0 else 0)
                        
                        # æ‰¹æ¬¡é€‰æ‹©å™¨
                        selected_batch = st.radio(
                            "é€‰æ‹©æ‰¹æ¬¡:",
                            options=range(1, num_batches + 1),
                            format_func=lambda x: f"ç¬¬ {x} æ‰¹ (å•è¯ {(x-1)*BATCH_SIZE + 1} - {min(x*BATCH_SIZE, total_words)})",
                            horizontal=True
                        )
                        
                        # åˆ‡ç‰‡
                        start_idx = (selected_batch - 1) * BATCH_SIZE
                        end_idx = start_idx + BATCH_SIZE
                        batch_words = final_words[start_idx : end_idx]
                        
                        st.info(f"å½“å‰é€‰ä¸­: **{len(batch_words)}** ä¸ªå•è¯")
                        
                        if st.button(f"ç”Ÿæˆ Prompt (ç¬¬ {selected_batch} æ‰¹)", type="primary"):
                            prompt = generate_prompt(batch_words, settings)
                            st.code(prompt, language="markdown")
                            
                    else:
                        # æ•°é‡å°‘ï¼Œç›´æ¥ç”Ÿæˆ
                        if st.button("ç”Ÿæˆ Prompt (å…¨éƒ¨)", type="primary"):
                            prompt = generate_prompt(final_words, settings)
                            st.code(prompt, language="markdown")

            # --- å…¶ä»– Tab (ä»…å±•ç¤ºå¯å¤åˆ¶) ---
            with tab2:
                st.code(", ".join(df_mastered["Word"].tolist()), language="text")
            with tab3:
                st.code(", ".join(df_beyond["Word"].tolist()), language="text")

    # ------------------------------------------------
    # æ¨¡å¼ B: åˆ·è¯
    # ------------------------------------------------
    elif mode == "ğŸ”¢ è¯é¢‘åˆ·è¯":
        c1, c2 = st.columns(2)
        with c1: s_r = st.number_input("Start", 8000, step=50)
        with c2: cnt = st.number_input("Count", 50, step=10)
        
        if st.button("æå–å•è¯"):
            res = FULL_DF[FULL_DF[RANK_COL] >= s_r].sort_values(RANK_COL).head(cnt)
            # å­˜å…¥ session ç”¨äºç¼–è¾‘
            st.session_state['range_df'] = res[[WORD_COL, RANK_COL]]
            
        if 'range_df' in st.session_state:
            st.markdown("### ğŸ“ å•è¯åˆ—è¡¨ (å¯ç¼–è¾‘)")
            
            # åŒæ ·ä½¿ç”¨ Editor
            range_edited = st.data_editor(
                st.session_state['range_df'],
                num_rows="dynamic",
                use_container_width=True,
                key="editor_range"
            )
            
            words_to_gen = range_edited[WORD_COL].tolist()
            
            if st.button("ç”Ÿæˆ Prompt"):
                prompt = generate_prompt(words_to_gen, settings)
                st.code(prompt, language="markdown")

    # ------------------------------------------------
    # æ¨¡å¼ C: è½¬æ¢
    # ------------------------------------------------
    elif mode == "ğŸ› ï¸ æ ¼å¼è½¬æ¢":
        st.markdown("### ğŸ“¥ è½¬ Anki CSV")
        txt = st.text_area("ç²˜è´´ AI å›å¤", height=200)
        if txt:
            st.download_button("ğŸ“¥ ä¸‹è½½ .csv", txt.encode("utf-8"), "anki.csv", "text/csv", type="primary")