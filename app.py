import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import json

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode {
        font-family: 'Consolas', 'Courier New', monospace !important;
        font-size: 16px !important;
    }
    header {visibility: hidden;}
    footer {visibility: hidden;}
    .block-container { padding-top: 1rem; }
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    
    /* ä¼˜åŒ–æ•°æ®çœ‹æ¿å¤–è§‚ */
    [data-testid="stMetricValue"] {
        font-size: 28px !important;
        color: var(--primary-color) !important;
    }
    
    /* å‚æ•°é¢æ¿åº•è‰²æ¡† */
    .param-box {
        background-color: var(--secondary-background-color);
        padding: 15px 20px 5px 20px;
        border-radius: 10px;
        border: 1px solid var(--border-color-light);
        margin-bottom: 20px;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ•°æ®åŠ è½½ (Data Loading)
# ==========================================
@st.cache_data
def load_knowledge_base():
    try:
        with open('data/terms.json', 'r', encoding='utf-8') as f:
            terms = json.load(f)
        with open('data/proper.json', 'r', encoding='utf-8') as f:
            proper = json.load(f)
        with open('data/patch.json', 'r', encoding='utf-8') as f:
            patch = json.load(f)
        with open('data/ambiguous.json', 'r', encoding='utf-8') as f:
            ambiguous = set(json.load(f))
            
        terms = {k.lower(): v for k, v in terms.items()}
        proper = {k.lower(): v for k, v in proper.items()}
        
        return terms, proper, patch, ambiguous
    except FileNotFoundError:
        st.error("âš ï¸ ç¼ºå°‘æ•°æ®æ–‡ä»¶ï¼è¯·ç¡®ä¿ `data/` æ–‡ä»¶å¤¹ä¸‹åŒ…å« terms.json, proper.json, patch.json, ambiguous.json")
        return {}, {}, {}, set()

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

# ==========================================
# 3. åˆå§‹åŒ– NLP (è¯å½¢è¿˜åŸå¼•æ“)
# ==========================================
@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    if not os.path.exists(nltk_data_dir):
        os.makedirs(nltk_data_dir)
    nltk.data.path.append(nltk_data_dir)
    for pkg in ['averaged_perceptron_tagger', 'punkt']:
        try: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
        except: pass

setup_nltk()

def get_lemma(w):
    """æå–å•ä¸ªå•è¯çš„åŸå‹"""
    lemmas_dict = lemminflect.getAllLemmas(w)
    if not lemmas_dict:
        return w.lower()
    if 'ADJ' in lemmas_dict: return lemmas_dict['ADJ'][0]
    elif 'ADV' in lemmas_dict: return lemmas_dict['ADV'][0]
    elif 'VERB' in lemmas_dict: return lemmas_dict['VERB'][0]
    elif 'NOUN' in lemmas_dict: return lemmas_dict['NOUN'][0]
    else: return list(lemmas_dict.values())[0][0]

# ==========================================
# 4. è¯åº“åŠ è½½ (å«ç´§æ€¥ä¿®å¤è¡¥ä¸)
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv"]

@st.cache_data
def load_vocab():
    vocab = {}
    file_path = next((f for f in POSSIBLE_FILES if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
            df = df.sort_values(r_col, ascending=True)
            df = df.drop_duplicates(subset=[w_col], keep='first')
            vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except: pass
    
    for word, rank in BUILTIN_PATCH_VOCAB.items():
        vocab[word] = rank
        
    URGENT_OVERRIDES = {
        "china": 400, "turkey": 1500, "march": 500, "may": 100, "august": 1500, "polish": 2500,
        "monday": 300, "tuesday": 300, "wednesday": 300, "thursday": 300, "friday": 300, "saturday": 300, "sunday": 300,
        "january": 400, "february": 400, "april": 400, "june": 400, "july": 400, "september": 400, "october": 400, "november": 400, "december": 400,
        "usa": 200, "uk": 200, "google": 1000, "apple": 1000, "microsoft": 1500
    }
    for word, rank in URGENT_OVERRIDES.items():
        vocab[word] = rank
        
    return vocab

vocab_dict = load_vocab()

# ==========================================
# 5. AI æŒ‡ä»¤ç”Ÿæˆå™¨
# ==========================================
def generate_ai_prompt(word_list, output_format, def_mode="single", is_term_list=False):
    words_str = ", ".join(word_list)
    core_principle_text = ""
    
    if is_term_list or def_mode == "term":
        core_principle_text = """1. æ ¸å¿ƒåŸåˆ™ï¼šé¢†åŸŸé”å®š (Domain Locked)
- **é¢†åŸŸåŒ¹é…**ï¼šå¦‚æœå•è¯å¸¦æœ‰ (Domain) æ ‡ç­¾ï¼Œ**å¿…é¡»**ä»…æä¾›ç¬¦åˆè¯¥é¢†åŸŸèƒŒæ™¯çš„ä¸“ä¸šé‡Šä¹‰ã€‚
- **åŸå­æ€§**ï¼šä¸€å¼ å¡ç‰‡åªè§£é‡Šè¯¥é¢†åŸŸçš„ä¸€ä¸ªå«ä¹‰ã€‚"""
    elif def_mode == "split":
        core_principle_text = """1. æ ¸å¿ƒåŸåˆ™ï¼šåŸå­æ€§ (Atomicity)
- **å«ä¹‰æ‹†åˆ†**ï¼šè‹¥ä¸€ä¸ªå•è¯æœ‰å¤šä¸ªä¸åŒå¸¸ç”¨é‡Šä¹‰ï¼ˆåè¯ vs åŠ¨è¯ï¼Œå­—é¢ä¹‰ vs å¼•ç”³ä¹‰ï¼‰ï¼Œ**å¿…é¡»æ‹†åˆ†ä¸ºå¤šæ¡ï¼ˆ1-3ï¼‰ç‹¬ç«‹æ•°æ®**ï¼ˆå³ä¸ºåŒä¸€ä¸ªå•è¯ç”Ÿæˆå¤šè¡Œ/å¤šå¼ å¡ç‰‡ï¼‰ã€‚
- **ä¸¥ç¦å †ç Œ**ï¼šæ¯å¼ å¡ç‰‡åªæ‰¿è½½ä¸€ä¸ªç‰¹å®šè¯­å¢ƒä¸‹çš„å«ä¹‰ï¼Œä¸å‡†å°†å¤šä¸ªé‡Šä¹‰æŒ¤åœ¨ä¸€èµ·ã€‚"""
    else: 
        core_principle_text = """1. æ ¸å¿ƒåŸåˆ™ï¼šæç®€é€Ÿè®° (Minimalist)
- **å•ä¸€é‡Šä¹‰**ï¼šè¯·**ä»…æä¾› 1 ä¸ªæœ€æ ¸å¿ƒã€æœ€å¸¸ç”¨çš„é‡Šä¹‰**ã€‚
- **ä¸¥ç¦æ‹†åˆ†**ï¼šå¯¹äºè¿™äº›ç”Ÿè¯ï¼Œä¸è¦ç”Ÿæˆå¤šå¼ å¡ç‰‡ï¼Œä¸€å¼ å¡ç‰‡å³å¯ã€‚
- **å‡è½»è´Ÿæ‹…**ï¼šç›®çš„æ˜¯å¿«é€Ÿæ··ä¸ªè„¸ç†Ÿï¼Œä¸è¦é¢é¢ä¿±åˆ°ã€‚"""

    if output_format == 'csv':
        format_req = "CSV Code Block (åç¼€å .csv)"
        format_desc = "è¯·ç›´æ¥è¾“å‡ºæ ‡å‡† CSV ä»£ç å—ã€‚"
    else:
        format_req = "TXT Code Block (åç¼€å .txt)"
        format_desc = "è¯·è¾“å‡ºçº¯æ–‡æœ¬ TXT ä»£ç å—ã€‚"

    prompt = f"""
è¯·æ‰®æ¼”ä¸€ä½ä¸“ä¸šçš„ Anki åˆ¶å¡ä¸“å®¶ã€‚è¿™æ˜¯æˆ‘æ•´ç†çš„å•è¯åˆ—è¡¨ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ã€æ ¸å¿ƒåŸåˆ™ã€‘ä¸ºæˆ‘ç”Ÿæˆå¯¼å…¥æ–‡ä»¶ã€‚

{core_principle_text}

2. å¡ç‰‡æ­£é¢ (Column 1: Front)
- å†…å®¹ï¼šæä¾›è‡ªç„¶çš„çŸ­è¯­æˆ–æ­é… (Phrase/Collocation)ã€‚
- æ ·å¼ï¼šçº¯æ–‡æœ¬ã€‚
- æ³¨æ„ï¼šå¦‚æœæ˜¯â€œå«ä¹‰æ‹†åˆ†â€æ¨¡å¼ï¼Œæ­£é¢å¯ä»¥æ˜¯ä¸€æ ·çš„å•è¯/çŸ­è¯­ï¼Œä½†èƒŒé¢è§£é‡Šä¸åŒã€‚

3. å¡ç‰‡èƒŒé¢ (Column 2: Back)
- æ ¼å¼ï¼šHTML æ’ç‰ˆï¼ŒåŒ…å«ä¸‰éƒ¨åˆ†ï¼Œå¿…é¡»ä½¿ç”¨ <br><br> åˆ†éš”ã€‚
- ç»“æ„ï¼šè‹±æ–‡é‡Šä¹‰<br><br><em>æ–œä½“ä¾‹å¥</em><br><br>ã€è¯æº/è¯æ ¹è¯ç¼€ã€‘ä¸­æ–‡åŠ©è®° (è¯æºä¼˜å…ˆ)

4. è¾“å‡ºæ ¼å¼æ ‡å‡† ({format_req})
- {format_desc}
- å…³é”®æ ¼å¼ï¼šä½¿ç”¨è‹±æ–‡é€—å· (,) åˆ†éš”ï¼Œä¸”æ¯ä¸ªå­—æ®µå†…å®¹å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å· ("...") åŒ…è£¹ã€‚

å¾…å¤„ç†å•è¯ï¼š
{words_str}
"""
    return prompt

# ==========================================
# 6. æ ¸å¿ƒåˆ†æå¼•æ“
# ==========================================
def analyze_words(unique_word_list):
    """ç›´æ¥å¯¹å»é‡ä¸”è¿˜åŸåçš„å•è¯åˆ—è¡¨è¿›è¡Œè¯é¢‘å®šçº§"""
    unique_items = [] 
    JUNK_WORDS = {'s', 't', 'd', 'm', 'll', 've', 're'}
    
    for item_lower in unique_word_list:
        if len(item_lower) < 2 and item_lower not in ['a', 'i']: continue
        if item_lower in JUNK_WORDS: continue
        
        actual_rank = vocab_dict.get(item_lower, 99999)
        
        # 1. æœ¯è¯­èº«ä»½
        if item_lower in BUILTIN_TECHNICAL_TERMS:
            domain = BUILTIN_TECHNICAL_TERMS[item_lower]
            term_rank = actual_rank if actual_rank != 99999 else 15000
            unique_items.append({
                "word": f"{item_lower} ({domain})", 
                "rank": term_rank,
                "raw": item_lower
            })
            continue
        
        # 2. ä¸“åä¸æ­§ä¹‰è¯
        if item_lower in PROPER_NOUNS_DB or item_lower in AMBIGUOUS_WORDS:
            display = PROPER_NOUNS_DB.get(item_lower, item_lower.title())
            unique_items.append({
                "word": display,
                "rank": actual_rank, 
                "raw": item_lower
            })
            continue
            
        # 3. æ™®é€šè¯
        if actual_rank != 99999:
            unique_items.append({
                "word": item_lower,
                "rank": actual_rank,
                "raw": item_lower
            })
            
    return pd.DataFrame(unique_items)

# ==========================================
# 7. ç•Œé¢å¸ƒå±€ä¸ç»Ÿä¸€æµæ°´çº¿
# ==========================================
st.title("ğŸš€ Vocab Master Pro - å…¨èƒ½é•¿æ–‡è§£æå¼•æ“")
st.markdown("ğŸ’¡ **ä¸€ç«™å¼å·¥ä½œæµ**ï¼šæ”¯æŒç²˜è´´æ•´æœ¬ä¹¦ã€é•¿ç¯‡å¤–åˆŠæˆ–è®ºæ–‡ï¼ˆ**æ•°åä¸‡å­—è¶…é•¿æ–‡æœ¬è¾“å…¥**ï¼‰ã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨è¿›è¡Œã€è¯å½¢è¿˜åŸã€‘ã€ã€å…¨é‡åˆ†çº§ã€‘å¹¶æå–ã€Top N ç²¾é€‰ã€‘ï¼Œæé€Ÿç”ŸæˆåŒç«¯è¾“å‡ºã€‚")

# --- å‚æ•°é…ç½®åŒº (ç›´è§‚å±•ç¤ºï¼Œä¸æŠ˜å ) ---
st.markdown("<div class='param-box'>", unsafe_allow_html=True)
c1, c2, c3, c4, c5 = st.columns(5)
with c1: current_level = st.number_input("ğŸ¯ å½“å‰æ°´å¹³ (èµ·)", 0, 30000, 9000, 500, help="ä½äºæ­¤è¯é¢‘çš„è§†ä¸ºå·²æŒæ¡")
with c2: target_level = st.number_input("ğŸ¯ ç›®æ ‡æ°´å¹³ (æ­¢)", 0, 30000, 15000, 500, help="é«˜äºæ­¤è¯é¢‘çš„è§†ä¸ºè¶…çº²")
with c3: top_n = st.number_input("ğŸ”¥ ç²¾é€‰ Top N", 10, 500, 50, 10, help="ä»å‰©ä½™ç”Ÿè¯ä¸­æŒ‘é€‰çš„æœ€æ ¸å¿ƒæ•°é‡")
with c4: min_rank_threshold = st.number_input("ğŸ“‰ å¿½ç•¥å‰ N è¯", 0, 20000, 3000, 500, help="ç²¾é€‰æ—¶å¿½ç•¥å¤ªç®€å•çš„åŸºç¡€è¯")
with c5: 
    st.write("") 
    st.write("") 
    show_rank = st.checkbox("ğŸ”¢ é™„åŠ æ˜¾ç¤º Rank", value=False)
st.markdown("</div>", unsafe_allow_html=True)

# --- è¶…é•¿æ–‡æœ¬è¾“å…¥åŒº ---
raw_text = st.text_area("ğŸ“¥ åœ¨æ­¤ç²˜è´´ä½ çš„è¶…é•¿è‹±æ–‡åŸæ–‡...", height=250, placeholder="Once upon a time, there was a...")
btn_process = st.button("ğŸš€ ä¸€é”®æ™ºèƒ½è§£æ (å¤„ç†é•¿æ–‡)", type="primary", use_container_width=True)

st.divider()

# --- ç»Ÿä¸€æµæ°´çº¿å¤„ç†é€»è¾‘ ---
if btn_process and raw_text and vocab_dict:
    with st.spinner("ğŸ§  æ­£åœ¨è¿›è¡Œäº¿çº§è¯å½¢è¿˜åŸä¸å…¨é‡è¯é¢‘åŒ¹é…..."):
        
        # 1. æå–æ€»è¯æ•° (Total Words)
        raw_words = re.findall(r"[a-zA-Z']+", raw_text)
        total_word_count = len(raw_words)
        
        # 2. æ™ºèƒ½è¿˜åŸ (Lemmatization)
        lemmatized_words = [get_lemma(w) for w in raw_words]
        full_lemmatized_text = " ".join(lemmatized_words)
        
        # 3. å»é‡æå–å”¯ä¸€è¯æ ¹ (Unique Lemmas)
        unique_lemmas = list(set([w.lower() for w in lemmatized_words]))
        unique_word_count = len(unique_lemmas)
        
        # 4. è¯é¢‘å®šçº§ä¸è¿‡æ»¤æ— æ•ˆå­—ç¬¦
        df = analyze_words(unique_lemmas)
        valid_word_count = len(df)
        
        # === å…¨æ™¯å­—æ•°ç»Ÿè®¡çœ‹æ¿ ===
        col_m1, col_m2, col_m3 = st.columns(3)
        col_m1.metric(label="ğŸ“ åŸæ–‡æ€»å­—æ•°", value=f"{total_word_count:,}")
        col_m2.metric(label="âœ‚ï¸ å»é‡è¯æ ¹æ•°", value=f"{unique_word_count:,}")
        col_m3.metric(label="ğŸ¯ çº³å…¥åˆ†çº§è¯æ±‡", value=f"{valid_word_count:,}")
        st.write("") # ç•™ç™½
        
        if not df.empty:
            # --- æ ¸å¿ƒåˆ†ç±»å™¨ ---
            def categorize(row):
                r = row['rank']
                if r <= current_level: return "known"
                elif r <= target_level: return "target"
                else: return "beyond"
            
            df['final_cat'] = df.apply(categorize, axis=1)
            df = df.sort_values(by='rank')
            
            # --- Top N æå– ---
            valid_candidates = df[df['rank'] >= min_rank_threshold].copy()
            top_df = valid_candidates.sort_values(by='rank', ascending=True).head(top_n)
            
            # --- æ¸²æŸ“è¾“å‡º Tab é¢æ¿ ---
            t_top, t_target, t_beyond, t_known, t_raw = st.tabs([
                f"ğŸ”¥ Top {len(top_df)} æ ¸å¿ƒç²¾é€‰",
                f"ğŸŸ¡ é‡ç‚¹ ({len(df[df['final_cat']=='target'])})", 
                f"ğŸ”´ è¶…çº² ({len(df[df['final_cat']=='beyond'])})",
                f"ğŸŸ¢ å·²æŒæ¡ ({len(df[df['final_cat']=='known'])})",
                "ğŸ“ è¯å½¢è¿˜åŸå…¨æ–‡è¾“å‡º"
            ])
            
            # æ¸²æŸ“é€šç”¨å‡½æ•° (æ‰€æœ‰åˆ—è¡¨å…¨éƒ¨åº”ç”¨æŠ˜å æ )
            def render_tab(tab_obj, data_df, label, def_mode, expand_default=False):
                with tab_obj:
                    if not data_df.empty:
                        pure_words = data_df['word'].tolist()
                        
                        display_lines = []
                        for _, row in data_df.iterrows():
                            if show_rank:
                                rank_str = str(int(row['rank'])) if row['rank'] != 99999 else "æœªæ”¶å½•"
                                display_lines.append(f"{row['word']} [Rank: {rank_str}]")
                            else:
                                display_lines.append(row['word'])
                        
                        # ç»Ÿä¸€åŠ æŠ˜å æ 
                        with st.expander("ğŸ‘ï¸ æŸ¥çœ‹å®Œæ•´å•è¯åˆ—è¡¨", expanded=expand_default):
                            st.code("\n".join(display_lines), language='text')
                        
                        st.markdown(f"**ğŸ¤– AI æŒ‡ä»¤ ({label})**")
                        has_term = any('(' in w for w in pure_words)
                        
                        p_csv = generate_ai_prompt(pure_words, 'csv', def_mode, is_term_list=has_term)
                        p_txt = generate_ai_prompt(pure_words, 'txt', def_mode, is_term_list=has_term)
                        
                        t_csv, t_txt = st.tabs(["ğŸ“‹ CSV æŒ‡ä»¤", "ğŸ“ TXT æŒ‡ä»¤"])
                        with t_csv: st.code(p_csv, language='markdown')
                        with t_txt: st.code(p_txt, language='markdown')
                    else: st.info("è¯¥åŒºé—´æš‚æ— ç¬¦åˆæ¡ä»¶çš„å•è¯")

            # æ¸²æŸ“å„æ¿å—
            # ğŸ”¥ Top N é»˜è®¤å±•å¼€æŠ˜å æ ï¼Œæ–¹ä¾¿ç¬¬ä¸€çœ¼çœ‹åˆ°ï¼›å…¶ä»–é»˜è®¤æ”¶èµ·
            render_tab(t_top, top_df, "æ ¸å¿ƒå•ä¹‰", def_mode="single", expand_default=True) 
            render_tab(t_target, df[df['final_cat']=='target'], "é‡ç‚¹", def_mode="single", expand_default=False)
            render_tab(t_beyond, df[df['final_cat']=='beyond'], "è¶…çº²", def_mode="single", expand_default=False)
            render_tab(t_known, df[df['final_cat']=='known'], "ç†Ÿè¯æ‹†åˆ†", def_mode="split", expand_default=False)
            
            # æ¸²æŸ“è¿˜åŸåŸæ–‡æ¿å—
            with t_raw:
                st.info("ğŸ’¡ è¿™æ˜¯è‡ªåŠ¨è¯å½¢è¿˜åŸï¼ˆLemmatizedï¼‰åçš„è¶…é•¿æ–‡æœ¬è¾“å‡ºï¼Œå¯ç›´æ¥å¤åˆ¶ç”¨äºå…¶ä»– NLP åˆ†æã€‚")
                st.code(full_lemmatized_text, language='text')