import streamlit as st
import pandas as pd
import re
import os

# è®¾ç½®é¡µé¢
st.set_page_config(page_title="Vibe Vocab Studio", page_icon="ğŸ§ ", layout="wide")

# ==========================================
# 0. æ ¸å¿ƒå¼•æ“ï¼šå»¶è¿ŸåŠ è½½ spaCy é˜²æ­¢å´©æºƒ
# ==========================================
@st.cache_resource
def load_nlp():
    try:
        import spacy
        # å°è¯•åŠ è½½æ¨¡å‹
        try:
            return spacy.load("en_core_web_sm")
        except:
            # å¦‚æœæ¨¡å‹æ²¡ä¸‹æˆåŠŸï¼Œå°è¯•è¿™ç§æ–¹å¼åŠ è½½
            import en_core_web_sm
            return en_core_web_sm.load()
    except ImportError:
        return None

nlp = load_nlp()

# ==========================================
# 1. è¯åº“åŠ è½½é€»è¾‘
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv"]

@st.cache_data
def load_vocab():
    file_path = next((f for f in POSSIBLE_FILES if os.path.exists(f)), None)
    if not file_path:
        return None, "âŒ æœªæ‰¾åˆ°è¯åº“æ–‡ä»¶ï¼ˆcoca_cleaned.csvï¼‰"
    try:
        df = pd.read_csv(file_path)
        # æ¸…æ´—åˆ—åï¼Œé˜²æ­¢ BOM æˆ–ç©ºæ ¼å¹²æ‰°
        df.columns = [str(c).strip().lower() for c in df.columns]
        
        # ç¡®ä¿æœ‰ word å’Œ rank åˆ—
        w_col = 'word' if 'word' in df.columns else df.columns[0]
        r_col = 'rank' if 'rank' in df.columns else df.columns[1]
        
        # ç»Ÿä¸€æ ¼å¼ï¼šè½¬å°å†™ï¼Œå»ç©ºæ ¼
        df[w_col] = df[w_col].astype(str).str.lower().str.strip()
        
        # æ„å»ºå­—å…¸
        vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        return vocab, f"âœ… è¯åº“åŠ è½½æˆåŠŸ: {file_path}"
    except Exception as e:
        return None, f"âŒ è¯»å–å¤±è´¥: {str(e)}"

vocab_dict, status_msg = load_vocab()

# ==========================================
# 2. ä¾§è¾¹æ ä¸ UI
# ==========================================
st.title("ğŸ§  Vibe Vocab v12.0 (ç»ˆæç¨³å®šç‰ˆ)")

if nlp is None:
    st.error("ğŸš¨ åŸºç¡€ç»„ä»¶ (spaCy) å°šæœªå®‰è£…æˆåŠŸã€‚è¯·ç¡®ä¿ requirements.txt å·²æ›´æ–°å¹¶ç‚¹å‡» Manage app -> Rebootã€‚")
    st.stop()

if not vocab_dict:
    st.error(status_msg)
    st.stop()

with st.sidebar:
    st.success("æ ¸å¿ƒå¼•æ“å·²å°±ç»ª")
    st.info(status_msg)
    st.divider()
    v_range = st.slider("è®¾å®šå­¦ä¹ åŒºé—´", 1, 20000, (6000, 8000), 500)
    r_start, r_end = v_range
    st.write(f"ğŸŸ¢ ç†Ÿè¯: 1-{r_start}")
    st.write(f"ğŸŸ¡ é‡ç‚¹: {r_start}-{r_end}")
    st.write(f"ğŸ”´ è¶…çº²: {r_end}+")

# ==========================================
# 3. æ ¸å¿ƒå¤„ç†é€»è¾‘
# ==========================================
def process_text_pro(text):
    # ä½¿ç”¨ spaCy è¿›è¡Œå…¨æ–‡æœ¬æ·±åº¦è§£æ
    doc = nlp(text.lower())
    
    # æå–æ‰€æœ‰ä¸é‡å¤çš„è¿˜åŸè¯ (Lemmas)
    results = []
    seen_lemmas = set()
    
    for token in doc:
        # åªå¤„ç†é•¿åº¦ > 1 çš„çº¯å­—æ¯å•è¯
        if token.is_alpha and len(token.text) > 1:
            # å…³é”®ï¼šä½¿ç”¨ lemma_ è·å–è¿˜åŸè¯ï¼ˆå¦‚ went -> goï¼‰
            lemma = token.lemma_.lower()
            original = token.text.lower()
            
            if lemma not in seen_lemmas:
                # æŸ¥è¯åº“
                rank = vocab_dict.get(lemma, 99999)
                
                # ç‰¹æ®Šé€»è¾‘ï¼šå¦‚æœè¿˜åŸè¯æŸ¥ä¸åˆ°ï¼Œå†è¯•è¯•åŸè¯ï¼ˆé˜²æ¼ï¼‰
                if rank == 99999 and original in vocab_dict:
                    rank = vocab_dict[original]
                    display_word = original
                else:
                    display_word = lemma
                
                results.append({
                    'å•è¯': display_word,
                    'åŸæ–‡': original if original != display_word else "-",
                    'æ’å': int(rank)
                })
                seen_lemmas.add(lemma)

    # æ’åºå¹¶åˆ†ç±»
    if not results:
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    df = pd.DataFrame(results).sort_values('æ’å')
    known = df[df['æ’å'] <= r_start]
    target = df[(df['æ’å'] > r_start) & (df['æ’å'] <= r_end)]
    beyond = df[df['æ’å'] > r_end]
    return known, target, beyond

# ==========================================
# 4. ä¸»ç•Œé¢äº¤äº’
# ==========================================
text_input = st.text_area("åœ¨æ­¤ç²˜è´´ä½ çš„è‹±æ–‡æ–‡ç« /å°è¯´å†…å®¹:", height=200)

if st.button("ğŸš€ å¼€å§‹ç²¾å‡†åˆ†æ", type="primary"):
    if not text_input.strip():
        st.warning("è¯·è¾“å…¥æ–‡æœ¬å†…å®¹")
    else:
        with st.spinner("æ­£åœ¨è¿›è¡Œå·¥ä¸šçº§è¯å½¢è¿˜åŸåˆ†æ..."):
            df_k, df_t, df_b = process_text_pro(text_input)
        
        st.success(f"åˆ†æå®Œæˆï¼æ‰¾åˆ°é‡ç‚¹è¯: {len(df_t)} ä¸ª")
        
        tab1, tab2, tab3 = st.tabs([
            f"ğŸŸ¡ é‡ç‚¹çªç ´ ({len(df_t)})", 
            f"ğŸ”´ ç”Ÿè¯/è¶…çº² ({len(df_b)})", 
            f"ğŸŸ¢ ç†Ÿè¯è¡¨ ({len(df_k)})"
        ])
        
        with tab1:
            st.dataframe(df_t, use_container_width=True)
            if not df_t.empty:
                csv_t = df_t.to_csv(index=False).encode('utf-8')
                st.download_button("ğŸ“¥ ä¸‹è½½é‡ç‚¹è¯ CSV", csv_t, "target.csv", "text/csv")
            
        with tab2:
            st.dataframe(df_b, use_container_width=True)
            
        with tab3:
            st.dataframe(df_k, use_container_width=True)