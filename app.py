import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import time

# ==========================================
# 0. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(
    page_title="Vocab Master", 
    page_icon="âš¡ï¸", 
    layout="centered", 
    initial_sidebar_state="collapsed"
)

st.markdown("""
<style>
    .block-container { padding-top: 1rem; padding-bottom: 5rem; }
    #MainMenu {visibility: hidden;} footer {visibility: hidden;} header {visibility: hidden;}
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    
    /* å¤§æŒ‰é’® */
    .stButton>button {
        width: 100%; border-radius: 10px; height: 3.2em; font-weight: bold; font-size: 16px !important;
        margin-top: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    
    /* æ–‡æœ¬æ¡†ä¼˜åŒ– */
    .stTextArea textarea { font-size: 15px !important; border-radius: 10px; font-family: monospace; }
    
    /* æŠ˜å æ æ ·å¼ */
    [data-testid="stExpander"] { border-radius: 10px; border: 1px solid #e0e0e0; margin-bottom: 10px; }
    
    /* å¤åˆ¶æç¤º */
    .copy-tip { font-size: 12px; color: #888; margin-bottom: 5px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºåŠ è½½
# ==========================================
@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    try: 
        nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_dir, quiet=True)
        nltk.download('punkt', download_dir=nltk_data_dir, quiet=True)
    except: pass
setup_nltk()

@st.cache_data
def load_data():
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c), cols[1])
            
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            df = df.dropna(subset=[r_col])
            
            # ã€å…³é”®ä¿®å¤ã€‘å…ˆæŒ‰æ’åæ’åº(å‡åº)ï¼Œç„¶åå»é‡ä¿ç•™ç¬¬ä¸€ä¸ª
            # è¿™æ ·ä¿è¯ say (rank 19) ä¼˜å…ˆäº say (rank 11771)
            df = df.sort_values(r_col, ascending=True)
            df_unique = df.drop_duplicates(subset=[w_col], keep='first')
            
            vocab_dict = pd.Series(df_unique[r_col].values, index=df_unique[w_col]).to_dict()
            return vocab_dict, df_unique, r_col, w_col
        except Exception as e:
            st.error(f"æ•°æ®åŠ è½½å‡ºé”™: {e}")
            return {}, None, None, None
    return {}, None, None, None

VOCAB_DICT, FULL_DF, RANK_COL, WORD_COL = load_data()
def get_lemma(word): return lemminflect.getLemma(word, upos='VERB')[0] 

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘
# ==========================================
def analyze_text(text, current_lvl, target_lvl):
    raw_words = re.findall(r"[a-z]+", text.lower())
    unique_words = set(raw_words)
    
    data_list = []
    for w in unique_words:
        if len(w) < 2: continue
        lemma = get_lemma(w)
        rank = VOCAB_DICT.get(lemma, 99999)
        
        category = "Beyond"
        if rank <= current_lvl: category = "Mastered"
        elif rank <= target_lvl: category = "Target"
        
        data_list.append({"Word": lemma, "Rank": int(rank), "Category": category})
        
    df = pd.DataFrame(data_list)
    return df

def generate_prompt(word_list):
    """
    ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ç‰¹å®šæ ¼å¼ç”Ÿæˆ Prompt
    """
    word_str = ", ".join(word_list)
    
    prompt = f"""Role: High-Efficiency Anki Card Creator
Task: Convert the provided word list into a strict CSV/TXT code block.

--- OUTPUT FORMAT RULES ---

1. Structure: 2 Columns only. Comma-separated. All fields double-quoted.
   Format: "Front","Back"

2. Column 1 (Front):
   - Content: A natural, short English phrase or collocation containing the target word.
   - Style: Plain text (NO bolding, NO extra symbols).

3. Column 2 (Back):
   - Content: Definition + Example + Etymology.
   - HTML Layout: Definition<br><em>Example Sentence</em><br>ã€æºã€‘Etymology
   - Constraints:
     - Use single <br> tag for line breaks (to save tokens).
     - Example sentence must be wrapped in <em> tags (Italics).
     - Definition: Concise English.

4. Etymology Style (Crucial):
   - Language: CHINESE (ä¸­æ–‡).
   - Style: Logic-based, extremely concise. Use arrows (â†’). No narrative filler.
   - Format: ã€æºã€‘Root/Origin (Meaning) â†’ Result/Logic.
   - Example: ã€æºã€‘Lat. 'vigere' (æ´»è·ƒ) â†’ ç²¾åŠ›/æ´»åŠ›

5. Atomicity Principle (Strict):
   - If a word has distinct meanings (e.g., Noun vs. Verb, Literal vs. Metaphorical), generate SEPARATE rows for each. Do not combine them.

6. Output Requirement:
   - Output the Code Block ONLY. No conversational text before or after.

--- EXAMPLE OUTPUT ---
"limestone quarry","deep pit for extracting stone<br><em>The company owns a granite quarry.</em><br>ã€æºã€‘å¤æ³•è¯­ quarriere (æ–¹çŸ³) â†’ åˆ‡çŸ³åœº"
"hunter's quarry","animal pursued by a hunter<br><em>The eagle spotted its quarry.</em><br>ã€æºã€‘å¤æ³•è¯­ cuir (çš®é©) â†’ æ”¾çš®ä¸Šçš„å†…è„èµèµ â†’ çŒç‰©"
"stiffen with cold","make or become rigid<br><em>His muscles began to stiffen.</em><br>ã€æºã€‘stiff (åƒµç¡¬) + -en (ä½¿åŠ¨)"

--- MY WORD LIST ---
{word_str}
"""
    return prompt

# ==========================================
# 3. ä¸»ç•Œé¢
# ==========================================
st.title("âš¡ï¸ Vocab Master")

if FULL_DF is None:
    st.error("âš ï¸ ç¼ºå°‘è¯é¢‘æ–‡ä»¶")
else:
    # --- æ¨¡å¼é€‰æ‹© ---
    mode = st.radio("æ¨¡å¼", ["ğŸ“– æ–‡æœ¬æå–", "ğŸ”¢ è¯é¢‘åˆ·è¯", "ğŸ› ï¸ æ ¼å¼è½¬æ¢"], horizontal=True, label_visibility="collapsed")
    st.divider()

    # ------------------------------------------------
    # æ¨¡å¼ A: æ–‡æœ¬æå–
    # ------------------------------------------------
    if mode == "ğŸ“– æ–‡æœ¬æå–":
        st.caption("åˆ†ææ–‡ç« ï¼Œç­›é€‰é‡ç‚¹è¯")
        
        # 1. è¾“å…¥
        c_a, c_b = st.columns(2)
        with c_a: curr_lvl = st.number_input("å½“å‰æ°´å¹³", 4000, step=500)
        with c_b: targ_lvl = st.number_input("ç›®æ ‡æ°´å¹³", 8000, step=500)
        
        inp_type = st.radio("Input", ["ç²˜è´´", "ä¸Šä¼ "], horizontal=True, label_visibility="collapsed")
        
        user_text = ""
        if inp_type == "ç²˜è´´":
            user_text = st.text_area("åœ¨æ­¤ç²˜è´´æ–‡æœ¬", height=100)
        else:
            up = st.file_uploader("ä¸Šä¼  (TXT/PDF)", type=["txt","pdf"])
            if up:
                try:
                    if up.name.endswith('.txt'): user_text = up.getvalue().decode("utf-8")
                    else: 
                        import PyPDF2
                        r = PyPDF2.PdfReader(up)
                        user_text = " ".join([p.extract_text() for p in r.pages])
                except: st.error("è¯»å–å¤±è´¥")

        # 2. åˆ†ææŒ‰é’®
        if user_text and st.button("ğŸ” å¼€å§‹åˆ†æ", type="primary"):
            with st.spinner("åˆ†æä¸­..."):
                t0 = time.time()
                df_res = analyze_text(user_text, curr_lvl, targ_lvl)
                st.session_state['analysis_df'] = df_res
                st.session_state['analysis_time'] = time.time() - t0
        
        # 3. ç»“æœå±•ç¤º
        if 'analysis_df' in st.session_state:
            df = st.session_state['analysis_df']
            
            # æ’åºé€»è¾‘ï¼šé‡ç‚¹è¯æŒ‰ Rank é™åº (éš¾->æ˜“)
            df_target = df[df['Category'] == 'Target'].sort_values(by="Rank", ascending=False)
            df_mastered = df[df['Category'] == 'Mastered'].sort_values(by="Rank")
            df_beyond = df[df['Category'] == 'Beyond'].sort_values(by="Rank")
            
            st.success(f"å…± {len(df)} è¯ (è€—æ—¶ {st.session_state['analysis_time']:.2f}s)")
            
            t1, t2, t3 = st.tabs([
                f"ğŸ¯ é‡ç‚¹ ({len(df_target)})", 
                f"âœ… å·²æŒæ¡ ({len(df_mastered)})", 
                f"ğŸš€ è¶…çº² ({len(df_beyond)})"
            ])
            
            # --- é‡ç‚¹è¯ Tab ---
            with t1:
                default_target_str = ", ".join(df_target["Word"].tolist())
                
                # ç¼–è¾‘åŒº
                with st.expander("ğŸ“ ç¼–è¾‘é‡ç‚¹è¯ (å¯æŠ˜å )", expanded=True):
                    st.caption("ğŸ‘‡ åœ¨æ­¤ä¿®æ”¹åˆ—è¡¨ï¼š")
                    edited_target_str = st.text_area("Target List", value=default_target_str, height=150, key="ta_target")
                
                # çº¯å•è¯åˆ—è¡¨ä¸€é”®å¤åˆ¶
                st.markdown("<p class='copy-tip'>ğŸ‘‡ çº¯å•è¯åˆ—è¡¨ (ç‚¹å‡»å³ä¸Šè§’å¤åˆ¶)</p>", unsafe_allow_html=True)
                st.code(edited_target_str, language="text")

                final_words = [w.strip() for w in edited_target_str.split(',') if w.strip()]
                
                if final_words:
                    # ğŸŸ¢ åˆ†æ‰¹é€»è¾‘ (100ä¸ªä¸€ç»„)
                    BATCH_SIZE = 100
                    total = len(final_words)
                    
                    if total > BATCH_SIZE:
                        st.warning(f"å•è¯è¾ƒå¤š ({total})ï¼Œè‡ªåŠ¨åˆ†æ‰¹ (æ¯æ‰¹ {BATCH_SIZE})")
                        num_batches = (total // BATCH_SIZE) + (1 if total % BATCH_SIZE != 0 else 0)
                        
                        sel_batch = st.radio(
                            "é€‰æ‹©æ‰¹æ¬¡:", 
                            range(1, num_batches + 1), 
                            format_func=lambda x: f"ç¬¬ {x} æ‰¹ ({min(x*BATCH_SIZE, total)}è¯)",
                            horizontal=True
                        )
                        
                        start = (sel_batch - 1) * BATCH_SIZE
                        batch_words = final_words[start : start + BATCH_SIZE]
                        
                        if st.button(f"ğŸš€ ç”Ÿæˆ Prompt (ç¬¬ {sel_batch} æ‰¹)", type="primary"):
                            prompt = generate_prompt(batch_words)
                            st.code(prompt, language="markdown")
                            st.success("ğŸ‘† ç‚¹å‡»ä»£ç å—å³ä¸Šè§’å¤åˆ¶ï¼Œå‘é€ç»™ AI")
                    else:
                        if st.button("ğŸš€ ç”Ÿæˆ Prompt (å…¨éƒ¨)", type="primary"):
                            prompt = generate_prompt(final_words)
                            st.code(prompt, language="markdown")
                            st.success("ğŸ‘† ç‚¹å‡»ä»£ç å—å³ä¸Šè§’å¤åˆ¶ï¼Œå‘é€ç»™ AI")

            # --- å·²æŒæ¡ Tab ---
            with t2:
                words_m = ", ".join(df_mastered["Word"].tolist())
                st.caption("ğŸ‘‡ ç‚¹å‡»å³ä¸Šè§’å¤åˆ¶")
                st.code(words_m, language="text")
            
            # --- è¶…çº² Tab ---
            with t3:
                words_b = ", ".join(df_beyond["Word"].tolist())
                st.caption("ğŸ‘‡ ç‚¹å‡»å³ä¸Šè§’å¤åˆ¶")
                st.code(words_b, language="text")

    # ------------------------------------------------
    # æ¨¡å¼ B: åˆ·è¯
    # ------------------------------------------------
    elif mode == "ğŸ”¢ è¯é¢‘åˆ·è¯":
        c1, c2 = st.columns(2)
        with c1: s_r = st.number_input("Start", 8000, step=50)
        with c2: cnt = st.number_input("Count", 50, step=10)
        
        if st.button("æå–"):
            res = FULL_DF[FULL_DF[RANK_COL] >= s_r].sort_values(RANK_COL).head(cnt)
            w_str = ", ".join(res[WORD_COL].tolist())
            st.session_state['range_str'] = w_str
            
        if 'range_str' in st.session_state:
            with st.expander("ğŸ“ ç¼–è¾‘åˆ—è¡¨", expanded=True):
                edited_range_str = st.text_area("List", value=st.session_state['range_str'], height=150)
            
            st.code(edited_range_str, language="text")
            
            words = [w.strip() for w in edited_range_str.split(',') if w.strip()]
            
            if st.button("ğŸš€ ç”Ÿæˆ Prompt", type="primary"):
                prompt = generate_prompt(words)
                st.code(prompt, language="markdown")

    # ------------------------------------------------
    # æ¨¡å¼ C: è½¬æ¢
    # ------------------------------------------------
    elif mode == "ğŸ› ï¸ æ ¼å¼è½¬æ¢":
        st.markdown("### ğŸ“¥ è½¬ Anki CSV")
        st.caption("ç²˜è´´ AI å›å¤ (æ— è¡¨å¤´)")
        txt = st.text_area("ç²˜è´´å†…å®¹", height=200)
        
        if txt:
            # ç®€å•çš„æ¸…æ´—ï¼Œç¡®ä¿ AI æ²¡å¸¦ Markdown ä»£ç å—æ ‡è®°
            clean_txt = txt.replace("```csv", "").replace("```", "").strip()
            st.download_button("ğŸ“¥ ä¸‹è½½ .csv", clean_txt.encode("utf-8"), "anki.csv", "text/csv", type="primary")