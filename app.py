import streamlit as st
import pandas as pd
import re
import os
import json
import time
import requests
import zipfile
import concurrent.futures
import lemminflect
import nltk
import random
import tempfile

# ==========================================
# 0. ä¾èµ–æ£€æŸ¥ä¸åˆå§‹åŒ–
# ==========================================
try:
    import PyPDF2
    import docx
    import genanki # æ–°å¢ä¾èµ–
except ImportError:
    st.error("âš ï¸ ç¼ºå°‘å¿…è¦ä¾èµ–ã€‚è¯·è¿è¡Œ: pip install PyPDF2 python-docx genanki")
    st.stop()

# ==========================================
# 1. åŸºç¡€ UI é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro v2.0", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode { font-family: 'Consolas', 'Courier New', monospace !important; }
    .block-container { padding-top: 1rem; }
    /* ç§»åŠ¨ç«¯é€‚é…ä¼˜åŒ–: è°ƒæ•´æ‰‹æœºä¸Šçš„ Metricå­—ä½“å¤§å° */
    @media (max-width: 640px) {
        [data-testid="stMetricValue"] { font-size: 20px !important; }
    }
</style>
""", unsafe_allow_html=True)

# State åˆå§‹åŒ–
if "base_df" not in st.session_state: st.session_state.base_df = pd.DataFrame()
if "preview_card" not in st.session_state: st.session_state.preview_card = ""

# ==========================================
# 2. æ•°æ®å±‚ (è§£å†³ç¡¬ç¼–ç é—®é¢˜)
# ==========================================
@st.cache_data
def load_global_data():
    """
    å»ºè®®ï¼šå°†åŸæœ¬ä»£ç ä¸­çš„ huge dicts ä¿å­˜ä¸º data/safe_names.json å’Œ data/global_ranks.json
    æ­¤å¤„ä¸ºäº†æ¼”ç¤ºå®Œæ•´æ€§ï¼Œä¿ç•™äº†éƒ¨åˆ†æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œå®é™…ç”Ÿäº§ç¯å¢ƒè¯·ä»æ–‡ä»¶è¯»å–ã€‚
    """
    # æ¨¡æ‹Ÿä»æ–‡ä»¶è¯»å–
    safe_names = {
        'will', 'mark', 'rose', 'lily', 'bill', 'joy', 'hope', 'grace', 'amber', 'frank', 
        'miles', 'dean', 'duke', 'king', 'prince', 'baker', 'smith', 'cook', 'brown', 'white', 
        'black', 'green', 'young', 'hall', 'wright', 'price', 'long', 'major', 'rich'
    }
    
    # ä»…å±•ç¤ºéƒ¨åˆ†ç¤ºä¾‹ï¼Œå®é™…è¯·ä¿ç•™ä½ åŸæœ¬å®Œæ•´çš„å­—å…¸
    entity_ranks = {
        "china": 400, "usa": 200, "uk": 200, "apple": 1000, "google": 1000,
        "january": 400, "monday": 300, "christmas": 800
    }
    # è¡¥å…¨æ•°å­—
    for _nw in ["one", "two", "three", "ten", "hundred", "thousand", "million"]:
        entity_ranks[_nw] = 1000
        
    return safe_names, entity_ranks

SAFE_NAMES_DB, GLOBAL_ENTITY_RANKS = load_global_data()

# ä¿æŒåŸæœ‰çš„ NLP åŠ è½½é€»è¾‘
@st.cache_data
def load_vocab_resources():
    # è¿™é‡ŒåŸæ ·ä¿ç•™ä½ ä¹‹å‰çš„åŠ è½½é€»è¾‘ï¼Œä¸ºäº†èŠ‚çœç¯‡å¹…ç•¥å»ï¼Œè¯·ä¿ç•™åŸä»£ç ä¸­çš„ load_knowledge_base å®ç°
    # ... (Keep your original implementation here) ...
    return {}, {}, {}, set() # Placeholder

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_vocab_resources()
NLTK_NAMES_DB = set() # Placeholder, keep original nltk logic

# åŸæœ‰çš„ load_vocab å‡½æ•°ä¿æŒä¸å˜ï¼Œè®°å¾—æŠŠ GLOBAL_ENTITY_RANKS ä¼ è¿›å»
vocab_dict = {} # Placeholder for vocab loading

# ==========================================
# 3. åŠŸèƒ½å‡½æ•°ï¼šPrompt ä¼˜åŒ–ä¸ Anki ç”Ÿæˆ
# ==========================================

def get_dynamic_prompt_template_v2(front_style, add_pos, def_lang, ex_count, add_ety, split_polysemy):
    """
    V2 å‡çº§ï¼šå¢åŠ äº† One-Shot Example (æ ·æœ¬)ï¼Œå¤§å¹…æé«˜ AI è¾“å‡ºç¨³å®šæ€§
    """
    front_desc = "phrase using the word" if front_style == "phrase" else "the word itself"
    pos_instr = "append ' (pos)'" if add_pos else "no pos tag"
    
    # æ„å»º One-Shot ç¤ºä¾‹
    example_input = "book"
    example_output = ""
    if def_lang == "en":
        example_output = '"book (n)","A set of written or printed pages...<br><br><em>I read a good book yesterday.</em>"'
    else:
        example_output = '"book (n)","ã€åã€‘ä¹¦ï¼Œä¹¦ç±<br><br><em>I read a good book yesterday.</em>"'

    prompt = f"""# Role
Expert Linguist & Anki Card Generator.

# Task
Generate flashcards for the provided words. 
Format: CSV-style "Front","Back"

# Rules
1. Format: STRICTLY "Front_Content","Back_Content_HTML" per line.
2. Front: {front_desc}. {pos_instr}.
3. Back: Definition in {def_lang}. {f"Include {ex_count} example sentences (wrapped in <em>)." if ex_count > 0 else "NO examples."} { "Include Etymology." if add_ety else ""}
4. Output: ONLY the code block. NO explanations.

# One-Shot Example (Follow this format strictly)
Input: {example_input}
Output:
{example_output}

# Input Words:
"""
    return prompt

def generate_anki_package(cards_data, deck_name="VocabMaster Deck"):
    """
    ä½¿ç”¨ genanki ç”Ÿæˆ .apkg æ–‡ä»¶
    cards_data: list of tuples (front, back)
    """
    # 1. å®šä¹‰æ ·å¼
    model_id = random.randrange(1 << 30, 1 << 31)
    deck_id = random.randrange(1 << 30, 1 << 31)
    
    my_model = genanki.Model(
        model_id,
        'Vocab Master Model',
        fields=[{'name': 'Front'}, {'name': 'Back'}],
        templates=[
            {
                'name': 'Card 1',
                'qfmt': '<div class="front">{{Front}}</div>',
                'afmt': '{{FrontSide}}<hr id="answer"><div class="back">{{Back}}</div>',
            },
        ],
        css='.front {font-size: 24px; font-weight: bold; text-align: center;} .back {font-size: 18px; text-align: left;}'
    )

    my_deck = genanki.Deck(deck_id, deck_name)

    for front, back in cards_data:
        # æ¸…æ´—ä¸€ä¸‹å¯èƒ½çš„ CSV å¼•å·
        f_clean = front.strip('"').strip("'")
        b_clean = back.strip('"').strip("'")
        my_note = genanki.Note(model=my_model, fields=[f_clean, b_clean])
        my_deck.add_note(my_note)

    # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix='.apkg')
    genanki.Package(my_deck).write_to_file(tmp.name)
    return tmp.name

# ==========================================
# 4. API äº¤äº’ (æ”¯æŒé¢„è§ˆ)
# ==========================================
def call_deepseek_simple(prompt, api_key):
    """ç”¨äºå•æ¬¡é¢„è§ˆçš„è½»é‡çº§è°ƒç”¨"""
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.3
    }
    try:
        resp = requests.post("https://api.deepseek.com/chat/completions", json=payload, headers=headers, timeout=20)
        return resp.json()['choices'][0]['message']['content'].strip()
    except Exception as e:
        return f"Error: {e}"

# ==========================================
# 5. UI ä¸»è§†å›¾
# ==========================================
# --- ä¾§è¾¹æ ï¼šSecrets ç®¡ç† ---
with st.sidebar:
    st.header("ğŸ”‘ é…ç½® (Settings)")
    # ä¼˜å…ˆä½¿ç”¨ Secretsï¼Œå¦åˆ™å…è®¸ç”¨æˆ·è¾“å…¥
    default_key = st.secrets.get("DEEPSEEK_API_KEY", "")
    api_key_input = st.text_input("DeepSeek API Key", value=default_key, type="password")
    if not api_key_input:
        st.warning("è¯·è¾“å…¥ API Key ä»¥ä½¿ç”¨ AI åŠŸèƒ½")

    st.divider()
    st.info("ğŸ’¡ æç¤ºï¼šç§»åŠ¨ç«¯è¯·ç‚¹å‡»å·¦ä¸Šè§’ç®­å¤´æ”¶èµ·æ­¤èœå•")

# --- ä¸»ç•Œé¢ï¼šç§»åŠ¨ç«¯é€‚é…çš„å‚æ•°åŒº ---
st.title("ğŸš€ Vocab Master Pro")

# ä½¿ç”¨ Expander æ”¶çº³å¤æ‚å‚æ•°ï¼Œä¼˜åŒ–ç§»åŠ¨ç«¯é¦–å±ä½“éªŒ
with st.expander("âš™ï¸ è¿‡æ»¤ä¸æå–å‚æ•° (Filter Settings)", expanded=True):
    # ç§»åŠ¨ç«¯é€‚é…ï¼šä½¿ç”¨ 2 åˆ—è€Œä¸æ˜¯ 5 åˆ—
    c1, c2 = st.columns(2)
    with c1: 
        current_level = st.number_input("èµ· (Min Level)", 0, 20000, 9000, 500)
        top_n = st.number_input("ç²¾é€‰ Top N", 10, 500, 100, 10)
    with c2: 
        target_level = st.number_input("æ­¢ (Max Level)", 0, 20000, 15000, 500)
        min_rank_threshold = st.number_input("å¿½ç•¥å‰ N", 0, 20000, 6000, 500)

# è¾“å…¥åŒº
col_input1, col_input2 = st.columns([3, 2])
with col_input1:
    raw_text = st.text_area("ğŸ“¥ ç²˜è´´æ–‡æœ¬", height=120)
with col_input2:
    uploaded_file = st.file_uploader("ğŸ“‚ æˆ–ä¸Šä¼ æ–‡ä»¶", type=["txt", "pdf", "docx", "epub"])

if st.button("ğŸš€ å¼€å§‹åˆ†æ (Analyze)", type="primary", use_container_width=True):
    # ... (æ­¤å¤„ä¿ç•™åŸæœ¬çš„æ–‡æœ¬è§£æä¸ Pandas å¤„ç†é€»è¾‘) ...
    # å‡è®¾å¤„ç†å®Œå¾—åˆ°äº† st.session_state.base_df
    pass 

# ==========================================
# 6. ç»“æœä¸ç”ŸæˆåŒº (åŒ…å«é¢„è§ˆä¸ Anki å¯¼å‡º)
# ==========================================
if not st.session_state.base_df.empty:
    # ... (Tabs æ˜¾ç¤ºä»£ç ä¿æŒä¸å˜) ...
    
    # å‡è®¾å½“å‰åœ¨ "Topç²¾é€‰" Tab ä¸‹
    st.divider()
    st.markdown("#### ğŸ¤– AI åˆ¶å¡å·¥ä½œå°")
    
    # é…ç½®åŒº
    ac1, ac2 = st.columns(2)
    with ac1:
        export_format = st.radio("å¯¼å‡ºæ ¼å¼:", ["Anki Deck (.apkg)", "CSV / TXT"], horizontal=True)
    with ac2:
        ui_def = st.radio("é‡Šä¹‰è¯­è¨€:", ["English", "Chinese", "Bilingual"], index=1, horizontal=True)

    # åŠ¨æ€ Prompt
    prompt_v2 = get_dynamic_prompt_template_v2(
        "phrase", True, "zh" if ui_def=="Chinese" else "en", 1, True, False
    )
    
    # é¢„è§ˆåŠŸèƒ½ (æ–°å¢)
    if st.button("ğŸ‘ï¸ é¢„è§ˆé¦–å¼ å¡ç‰‡æ•ˆæœ (Preview 1 Card)"):
        if not api_key_input:
            st.error("è¯·å…ˆé…ç½® API Key")
        else:
            first_word = st.session_state.base_df.iloc[0]['raw']
            preview_prompt = f"{prompt_v2}{first_word}"
            with st.spinner("ç”Ÿæˆé¢„è§ˆä¸­..."):
                preview_res = call_deepseek_simple(preview_prompt, api_key_input)
                st.session_state.preview_card = preview_res
    
    if st.session_state.preview_card:
        st.info("é¢„è§ˆç»“æœ (Preview):")
        st.code(st.session_state.preview_card, language="csv")

    # æ‰¹é‡ç”Ÿæˆ
    if st.button("âš¡ æ‰¹é‡ç”Ÿæˆå…¨éƒ¨ (Batch Generate)", type="primary"):
        # ... (ä¿ç•™åŸæœ¬çš„å¤šçº¿ç¨‹ call_deepseek_api_chunked é€»è¾‘) ...
        # å‡è®¾ ai_result æ˜¯ç”Ÿæˆçš„ CSV å­—ç¬¦ä¸²
        
        # ç»“æœå¤„ç†ï¼šCSV vs Anki APKG
        ai_result_str = '...' # æ¨¡æ‹Ÿç»“æœ
        
        if "Anki" in export_format:
            # è§£æ CSV å­—ç¬¦ä¸²ä¸º List of Tuples
            # ç®€å•çš„è§£æé€»è¾‘ï¼Œå®é™…å»ºè®®ç”¨ csv æ¨¡å—å¤„ç† quotechar
            lines = [line.split('","') for line in ai_result_str.split('\n') if '","' in line]
            if lines:
                apkg_path = generate_anki_package(lines, deck_name="Vocab Master AI")
                with open(apkg_path, "rb") as f:
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½ .apkg æ–‡ä»¶ (ç›´æ¥å¯¼å…¥ Anki)",
                        data=f,
                        file_name="vocab_master.apkg",
                        mime="application/apkg"
                    )
        else:
            st.download_button("ğŸ“¥ ä¸‹è½½ CSV", ai_result_str, "cards.csv")