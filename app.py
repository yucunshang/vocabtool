import streamlit as st
import pandas as pd
import re
import os
import random
import tempfile
import lemminflect
import nltk
import genanki
from bs4 import BeautifulSoup

# --- æ–‡ä»¶å¤„ç†åº“ ---
import pypdf
import docx
import ebooklib
from ebooklib import epub
from datetime import datetime, timedelta, timezone

# ==========================================
# 0. é¡µé¢é…ç½®
# ==========================================
st.set_page_config(
    page_title="Vocab Flow Ultra", 
    page_icon="âš¡ï¸", 
    layout="centered", 
    initial_sidebar_state="collapsed"
)

st.markdown("""
<style>
    .stTextArea textarea { font-family: 'Consolas', monospace; font-size: 14px; }
    .stButton>button { border-radius: 8px; font-weight: 600; width: 100%; margin-top: 5px; }
    .stat-box { padding: 15px; background-color: #f0fdf4; border: 1px solid #bbf7d0; border-radius: 8px; text-align: center; color: #166534; margin-bottom: 20px; }
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºåŠ è½½
# ==========================================
@st.cache_resource
def setup_nltk():
    try:
        root_dir = os.path.dirname(os.path.abspath(__file__))
        nltk_data_dir = os.path.join(root_dir, 'nltk_data')
        os.makedirs(nltk_data_dir, exist_ok=True)
        nltk.data.path.append(nltk_data_dir)
        for pkg in ['averaged_perceptron_tagger', 'punkt', 'punkt_tab']:
            try: nltk.data.find(f'tokenizers/{pkg}')
            except LookupError: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
    except: pass
setup_nltk()

@st.cache_data
def load_vocab_data():
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            df.columns = [c.strip().lower() for c in df.columns]
            w_col = next((c for c in df.columns if 'word' in c), df.columns[0])
            r_col = next((c for c in df.columns if 'rank' in c), df.columns[1])
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            df = df.sort_values(r_col).drop_duplicates(subset=[w_col], keep='first')
            return pd.Series(df[r_col].values, index=df[w_col]).to_dict(), df
        except: return {}, None
    return {}, None

VOCAB_DICT, FULL_DF = load_vocab_data()

def get_lemma(word):
    try: return lemminflect.getLemma(word, upos='VERB')[0]
    except: return word

def get_beijing_time_str():
    utc_now = datetime.now(timezone.utc)
    beijing_now = utc_now + timedelta(hours=8)
    return beijing_now.strftime('%m%d_%H%M')

def clear_all_state():
    st.session_state.clear()

# ==========================================
# 2. æ ¸å¿ƒè§£æé€»è¾‘ (V7 æ— æŸè§£æç‰ˆ)
# ==========================================
def extract_text_from_file(uploaded_file):
    text = ""
    file_type = uploaded_file.name.split('.')[-1].lower()
    try:
        if file_type == 'txt':
            text = uploaded_file.getvalue().decode("utf-8", errors='ignore')
        elif file_type == 'pdf':
            reader = pypdf.PdfReader(uploaded_file)
            text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif file_type == 'docx':
            doc = docx.Document(uploaded_file)
            text = "\n".join([p.text for p in doc.paragraphs])
        elif file_type == 'epub':
            with tempfile.NamedTemporaryFile(delete=False, suffix='.epub') as tmp:
                tmp.write(uploaded_file.getvalue())
                tmp_path = tmp.name
            book = epub.read_epub(tmp_path)
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    text += soup.get_text() + " "
            os.remove(tmp_path)
    except Exception as e:
        return f"Error: {e}"
    return text

def analyze_logic(text, current_lvl, target_lvl):
    raw_tokens = re.findall(r"[a-z]+", text.lower())
    total_words = len(raw_tokens)
    unique_tokens = set(raw_tokens)
    target_words = []
    for w in unique_tokens:
        if len(w) < 3: continue 
        lemma = get_lemma(w)
        rank = VOCAB_DICT.get(lemma, 99999)
        if rank > current_lvl and rank <= target_lvl:
            target_words.append((lemma, rank))
    target_words.sort(key=lambda x: x[1])
    return [x[0] for x in target_words], total_words

def parse_anki_data(raw_text):
    """
    V7 è§£æå™¨ï¼š
    ä¸ä½¿ç”¨ filterï¼Œè€Œæ˜¯ä½¿ç”¨ pop ç§»é™¤é¦–å°¾ç©ºç®¡ï¼Œç¡®ä¿ä¿ç•™æ‰€æœ‰ä¸­é—´åˆ—ã€‚
    """
    parsed_cards = []
    # ç»Ÿä¸€å…¨è§’ç¬¦å·
    raw_text = raw_text.replace('ï½œ', '|')
    lines = raw_text.strip().split('\n')
    seen_phrases = set()

    for line in lines:
        line = line.strip()
        if not line: continue
        if '---' in line: continue
        if 'Phrase' in line and 'Definition' in line: continue

        # 1. åŸå§‹åˆ†å‰²
        segments = line.split('|')
        
        # 2. æ™ºèƒ½å»é™¤ Markdown è¡¨æ ¼ä¸¤ä¾§çš„ç©ºå­—ç¬¦ä¸²
        # ä¾‹å¦‚: "| A | B | C |" -> ["", "A", "B", "C", ""]
        # æˆ‘ä»¬åªç§»é™¤é¦–å°¾çš„ç©ºä¸²ï¼Œä¿ç•™ä¸­é—´çš„ç©ºä¸²(é˜²æ­¢é”™ä½)
        if len(segments) > 1 and segments[0].strip() == "":
            segments.pop(0)
        if len(segments) > 0 and segments[-1].strip() == "":
            segments.pop(-1)
            
        # 3. æ¸…æ´—æ¯ä¸€åˆ—çš„å†…å®¹
        parts = [s.strip() for s in segments]
        
        # 4. æå–æ•°æ® (å…è®¸å®¹é”™ï¼Œæœ€å°‘3åˆ—)
        if len(parts) >= 3:
            front_text = parts[0]
            
            # --- æ¸…æ´—æ­£é¢ ---
            front_text = front_text.rstrip('.,?!:; ')
            front_text = front_text.replace('*', '')
            
            # è¿‡æ»¤è¶…é•¿å¥å­ (>7å•è¯)
            if len(front_text.split()) > 7:
                continue

            # é¦–å­—æ¯å°å†™
            if front_text:
                first_word = front_text.split()[0]
                if first_word != "I" and not first_word.isupper():
                    front_text = front_text[0].lower() + front_text[1:]

            if front_text in seen_phrases:
                continue
            seen_phrases.add(front_text)

            meaning = parts[1]
            examples = parts[2]
            
            # --- å…³é”®ä¿®å¤ï¼šè·å–è¯æº ---
            # å¦‚æœæœ‰ç¬¬4åˆ—ï¼Œå–ç¬¬4åˆ—ï¼›å¦åˆ™æç¤ºç¼ºå¤±
            etymology = parts[3] if len(parts) >= 4 else "âš ï¸ ç¼ºå¤±ï¼šAIæœªç”Ÿæˆæ­¤åˆ—"

            parsed_cards.append({
                'front_phrase': front_text,
                'meaning': meaning,
                'examples': examples,
                'etymology': etymology
            })
            
    return parsed_cards

# ==========================================
# 3. Anki ç”Ÿæˆé€»è¾‘ (æ—  IPA, åŒ…å«è¯æº)
# ==========================================
def generate_anki_package(cards_data, deck_name):
    CSS = """
    .card { font-family: 'Arial', sans-serif; font-size: 20px; text-align: center; color: #333; background-color: white; padding: 20px; }
    .nightMode .card { background-color: #2e2e2e; color: #f0f0f0; }
    
    .phrase { font-size: 28px; font-weight: 700; color: #0056b3; margin-bottom: 20px; line-height: 1.3; }
    .nightMode .phrase { color: #66b0ff; }
    
    hr { border: 0; height: 1px; background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.2), rgba(0, 0, 0, 0)); margin-bottom: 15px; }
    
    .definition { font-weight: bold; color: #222; margin-bottom: 15px; font-size: 20px; text-align: left; }
    .nightMode .definition { color: #e0e0e0; }
    
    .examples { background: #f7f9fa; padding: 12px; border-left: 4px solid #0056b3; border-radius: 4px; color: #444; font-style: italic; font-size: 18px; line-height: 1.5; margin-bottom: 15px; text-align: left; }
    .nightMode .examples { background: #383838; color: #ccc; border-left-color: #66b0ff; }
    
    .footer-info { margin-top: 20px; border-top: 1px dashed #ccc; padding-top: 10px; text-align: left; }
    
    .etymology { display: block; font-size: 16px; color: #555; background-color: #fffdf5; padding: 10px; border-radius: 6px; margin-bottom: 5px; line-height: 1.4; border: 1px solid #fef3c7; }
    .etymology b { color: #8b5cf6; } 
    .nightMode .etymology { background-color: #333; color: #aaa; border-color: #444; }
    .nightMode .etymology b { color: #a78bfa; }
    """
    
    model_id = random.randrange(1 << 30, 1 << 31)
    model = genanki.Model(
        model_id, 
        f'VocabFlow NoIPA Model {model_id}',
        fields=[
            {'name': 'FrontPhrase'}, 
            {'name': 'Meaning'}, 
            {'name': 'Examples'}, 
            {'name': 'Etymology'}
        ],
        templates=[{
            'name': 'Phrase Card',
            'qfmt': '<div class="phrase">{{FrontPhrase}}</div>', 
            'afmt': '''
            {{FrontSide}}
            <hr>
            <div class="definition">{{Meaning}}</div>
            <div class="examples">{{Examples}}</div>
            
            <div class="footer-info">
                <div class="etymology">ğŸŒ± <b>è¯æº:</b> {{Etymology}}</div>
            </div>
            ''',
        }], css=CSS
    )
    
    deck = genanki.Deck(random.randrange(1 << 30, 1 << 31), deck_name)
    for c in cards_data:
        deck.add_note(genanki.Note(
            model=model, 
            fields=[
                str(c['front_phrase']), 
                str(c['meaning']), 
                str(c['examples']).replace('\n','<br>'), 
                str(c['etymology'])
            ]
        ))
        
    with tempfile.NamedTemporaryFile(delete=False, suffix='.apkg') as tmp:
        genanki.Package(deck).write_to_file(tmp.name)
        return tmp.name

# ==========================================
# 4. Prompt ç”Ÿæˆé€»è¾‘ (V7 ç»ˆæç‰ˆ)
# ==========================================
def get_ai_prompt(words):
    w_list = ", ".join(words)
    return f"""
Task: Create Anki cards.
Words: {w_list}

**STRICT OUTPUT FORMAT (4 Columns):**
`Phrase | English Definition | Example Sentences | Chinese Etymology`

**RULES:**
1. **NO IPA.** 
2. **Column 1 (Phrase):** Short collocation (2-5 words). NO sentences. Lowercase.
3. **Column 3 (Examples):** 1-2 sentences. Use `<br>` to separate.
4. **Column 4 (Etymology):** Simplified Chinese only. **MUST NOT BE EMPTY.** If unknown, write "è¯æºæš‚ç¼º".

**Example:**
a benevolent leader | characterized by goodwill | The benevolent man helped the poor.<br>She is benevolent. | è¯æ ¹: bene (å¥½) + vol (æ„æ„¿)

**Start Output:**
"""

# ==========================================
# 5. ä¸»ç¨‹åº UI
# ==========================================
st.title("âš¡ï¸ Vocab Flow Ultra")

if not VOCAB_DICT:
    st.error("âš ï¸ ç¼ºå¤± `coca_cleaned.csv`")

tab_extract, tab_anki = st.tabs(["1ï¸âƒ£ å•è¯æå– & ç”Ÿæˆ", "2ï¸âƒ£ åˆ¶ä½œ Anki ç‰Œç»„"])

with tab_extract:
    mode_context, mode_rank = st.tabs(["ğŸ“„ è¯­å¢ƒåˆ†æ", "ğŸ”¢ è¯é¢‘åˆ—è¡¨"])
    
    with mode_context:
        c1, c2 = st.columns(2)
        curr = c1.number_input("å¿½ç•¥å¤ªç®€å•çš„ (Current Level)", 1000, 20000, 4000, step=500)
        targ = c2.number_input("å¿½ç•¥å¤ªéš¾çš„ (Target Level)", 2000, 50000, 15000, step=500)
        uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£ (æ”¯æŒå¤šç§æ ¼å¼)")
        pasted_text = st.text_area("ğŸ“„ ...æˆ–ç²˜è´´æ–‡æœ¬", height=100)
        
        if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
            raw_text = extract_text_from_file(uploaded_file) if uploaded_file else pasted_text
            if len(raw_text) > 10:
                final_words, total = analyze_logic(raw_text, curr, targ)
                st.session_state['gen_words'] = final_words
                st.session_state['total_count'] = total
            else: st.warning("å†…å®¹æ— æ•ˆ")
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®", type="secondary", on_click=clear_all_state): pass

    with mode_rank:
        gen_type = st.radio("ç”Ÿæˆæ¨¡å¼", ["ğŸ”¢ é¡ºåºæˆªå–", "ğŸ”€ èŒƒå›´éšæœº"], horizontal=True)
        if "é¡ºåº" in gen_type:
            c_a, c_b = st.columns(2)
            s_rank = c_a.number_input("èµ·å§‹æ’å", 1, 20000, 8000, step=100)
            count = c_b.number_input("æ•°é‡", 10, 500, 50, step=10)
            if st.button("ğŸš€ ç”Ÿæˆåˆ—è¡¨"):
                if FULL_DF is not None:
                    r_col = next(c for c in FULL_DF.columns if 'rank' in c)
                    w_col = next(c for c in FULL_DF.columns if 'word' in c)
                    subset = FULL_DF[FULL_DF[r_col] >= s_rank].sort_values(r_col).head(count)
                    st.session_state['gen_words'] = subset[w_col].tolist()
                    st.session_state['total_count'] = count
        else:
            c_min, c_max, c_cnt = st.columns([1,1,1])
            min_r = c_min.number_input("Min Rank", 1, 20000, 6000, step=500)
            max_r = c_max.number_input("Max Rank", 1, 25000, 8000, step=500)
            r_count = c_cnt.number_input("Count", 10, 200, 50, step=10)
            if st.button("ğŸ² éšæœºæŠ½å–"):
                if FULL_DF is not None:
                    r_col = next(c for c in FULL_DF.columns if 'rank' in c)
                    w_col = next(c for c in FULL_DF.columns if 'word' in c)
                    mask = (FULL_DF[r_col] >= min_r) & (FULL_DF[r_col] <= max_r)
                    candidates = FULL_DF[mask]
                    if len(candidates) > 0:
                        subset = candidates.sample(n=min(r_count, len(candidates))).sort_values(r_col)
                        st.session_state['gen_words'] = subset[w_col].tolist()
                        st.session_state['total_count'] = len(subset)
                        st.success(f"æŠ½å–äº† {len(subset)} ä¸ªå•è¯")

    if 'gen_words' in st.session_state and st.session_state['gen_words']:
        words = st.session_state['gen_words']
        st.divider()
        st.info(f"ğŸ¯ å¾…å¤„ç†å•è¯: {len(words)} ä¸ª")
        
        batch_size = st.number_input("AI åˆ†ç»„å¤§å°", 10, 200, 50, step=10)
        batches = [words[i:i + batch_size] for i in range(0, len(words), batch_size)]
        
        for idx, batch in enumerate(batches):
            with st.expander(f"ç¬¬ {idx+1} ç»„ (å¤åˆ¶å‘ç»™ AI)", expanded=(idx==0)):
                st.code(get_ai_prompt(batch), language="markdown")

with tab_anki:
    st.markdown("### ğŸ“¦ åˆ¶ä½œ Anki ç‰Œç»„")
    bj_time_str = get_beijing_time_str()
    if 'anki_input_text' not in st.session_state: st.session_state['anki_input_text'] = ""

    ai_resp = st.text_area("åœ¨æ­¤ç²˜è´´ AI å›å¤", height=200, key="anki_input_text")
    deck_name = st.text_input("ç‰Œç»„åç§°", f"Vocab_{bj_time_str}")
    
    if ai_resp.strip():
        parsed_data = parse_anki_data(ai_resp)
        if parsed_data:
            st.markdown(f"#### ğŸ‘ï¸ é¢„è§ˆ (æˆåŠŸè§£æ {len(parsed_data)} æ¡)")
            df_view = pd.DataFrame(parsed_data)
            df_view.rename(columns={'front_phrase': 'æ­£é¢ (çŸ­è¯­)', 'meaning': 'è‹±æ–‡é‡Šä¹‰', 'examples': 'ä¾‹å¥', 'etymology': 'ä¸­æ–‡è¯æº'}, inplace=True)
            st.dataframe(df_view, use_container_width=True, hide_index=True)
            
            f_path = generate_anki_package(parsed_data, deck_name)
            with open(f_path, "rb") as f:
                st.download_button(f"ğŸ“¥ ä¸‹è½½ {deck_name}.apkg", f, file_name=f"{deck_name}.apkg", mime="application/octet-stream", type="primary")
        else:
            st.warning("âš ï¸ æ ¼å¼è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ AI å†…å®¹ã€‚")