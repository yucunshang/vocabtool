# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import re
import os
import random
import json
import time
from datetime import datetime, timedelta, timezone

# ==========================================
# 0. é¡µé¢é…ç½® (Page Configuration)
# ==========================================
st.set_page_config(
    page_title="Vocab Flow Ultra (CN-Stable)",
    page_icon="âš¡ï¸",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# æ³¨å…¥è‡ªå®šä¹‰ CSS (ä¼˜åŒ–ä¸­æ–‡æ˜¾ç¤ºä¸æ’ç‰ˆ)
st.markdown("""
<style>
    /* å­—ä½“ä¼˜åŒ– */
    .stTextArea textarea { font-family: 'Consolas', 'Courier New', monospace; font-size: 14px; }
    .stButton>button { border-radius: 8px; font-weight: 600; width: 100%; margin-top: 5px; }
    
    /* æ»šåŠ¨æ–‡æœ¬æ¡†æ ·å¼ */
    .scrollable-text {
        max-height: 250px;
        overflow-y: auto;
        padding: 10px;
        border: 1px solid #eee;
        border-radius: 5px;
        background-color: #fafafa;
        font-family: monospace;
        white-space: pre-wrap;
        font-size: 13px;
        color: #333;
    }
    
    /* æŒ‡å—å¡ç‰‡æ ·å¼ */
    .guide-step { 
        background-color: #f8f9fa; 
        padding: 15px; 
        border-radius: 8px; 
        margin-bottom: 15px; 
        border-left: 4px solid #0056b3; 
    }
    .guide-title { 
        font-weight: bold; 
        color: #0f172a; 
        display: block; 
        margin-bottom: 5px; 
        font-size: 16px;
    }
    
    /* é’ˆå¯¹ç½‘ç»œåŠ è½½æ…¢çš„æç¤ºæ¡† */
    .network-warning {
        padding: 10px;
        background-color: #fff3cd;
        border: 1px solid #ffeeba;
        color: #856404;
        border-radius: 5px;
        margin-bottom: 10px;
        font-size: 14px;
    }
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ– Session State
if 'uploader_id' not in st.session_state:
    st.session_state['uploader_id'] = "1000"

# ==========================================
# 1. æ ¸å¿ƒèµ„æºåŠ è½½ (Network Robustness)
# ==========================================

@st.cache_resource(show_spinner="æ­£åœ¨åˆå§‹åŒ– NLP å¼•æ“...")
def load_nlp_resources():
    """
    é’ˆå¯¹å›½å†…ç½‘ç»œç¯å¢ƒä¼˜åŒ–çš„èµ„æºåŠ è½½å™¨ã€‚
    ä¼˜å…ˆæ£€æŸ¥æœ¬åœ°ç›®å½•ï¼Œä¸‹è½½å¤±è´¥æ—¶æä¾›æ˜ç¡®æŒ‡å¼•ï¼Œä¸ç›´æ¥æŠ¥é”™å´©æºƒã€‚
    """
    import nltk
    import lemminflect
    
    # 1. è®¾ç½®æœ¬åœ°æ•°æ®è·¯å¾„ (ä¼˜å…ˆä½¿ç”¨å½“å‰ç›®å½•ä¸‹çš„ nltk_data æ–‡ä»¶å¤¹)
    root_dir = os.path.dirname(os.path.abspath(__file__))
    local_nltk_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(local_nltk_dir, exist_ok=True)
    
    # å¼ºåˆ¶å°†æœ¬åœ°è·¯å¾„åŠ å…¥ NLTK æœç´¢è·¯å¾„çš„é¦–ä½
    nltk.data.path.insert(0, local_nltk_dir)
    
    # éœ€è¦çš„ NLTK æ•°æ®åŒ…åˆ—è¡¨
    required_packages = [
        'averaged_perceptron_tagger', 
        'punkt', 
        'punkt_tab', 
        'wordnet', 
        'omw-1.4'
    ]
    
    missing_packages = []
    
    # 2. æ£€æŸ¥åŒ…æ˜¯å¦å­˜åœ¨
    for pkg in required_packages:
        try:
            # å°è¯•æŸ¥æ‰¾ (æ”¯æŒ tokenizers, taggers, corpora ç­‰ä¸åŒå­ç›®å½•)
            nltk.data.find(f'{pkg}')
        except LookupError:
            # å†è¯•ä¸€æ¬¡å…·ä½“è·¯å¾„æŸ¥æ‰¾ï¼Œé˜²æ­¢ find æ²¡æ‰¾åˆ°ä½†å…¶å®åœ¨
            try:
                nltk.data.find(f'tokenizers/{pkg}')
            except LookupError:
                try: nltk.data.find(f'taggers/{pkg}')
                except LookupError:
                    try: nltk.data.find(f'corpora/{pkg}')
                    except LookupError:
                        missing_packages.append(pkg)

    # 3. å°è¯•ä¸‹è½½ç¼ºå¤±åŒ… (å¸¦å¼‚å¸¸å¤„ç†)
    if missing_packages:
        try:
            # å°è¯•é™é»˜ä¸‹è½½
            nltk.download(missing_packages, download_dir=local_nltk_dir, quiet=True)
        except Exception as e:
            # ä¸‹è½½å¤±è´¥ (å›½å†…å¸¸è§æƒ…å†µ)
            pass

    return nltk, lemminflect, missing_packages

@st.cache_data
def load_vocab_data():
    """
    åŠ è½½ COCA è¯é¢‘è¡¨ã€‚è¿”å› {word: rank} å­—å…¸ã€‚
    """
    possible_files = ["coca_cleaned.csv", "vocab.csv", "data.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    
    if file_path:
        try:
            df = pd.read_csv(file_path)
            df.columns = [c.strip().lower() for c in df.columns]
            
            w_col = next((c for c in df.columns if 'word' in c), None)
            r_col = next((c for c in df.columns if 'rank' in c), None)
            
            if not w_col or not r_col: return {}

            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            
            # æ’åºå»é‡ï¼Œä¿ç•™æ’åæœ€é å‰çš„
            df = df.sort_values(r_col).drop_duplicates(subset=[w_col], keep='first')
            
            return pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except: return {}
    return {}

# å…¨å±€åŠ è½½
VOCAB_DICT = load_vocab_data()
NLTK_LIB, LEMMA_LIB, MISSING_PKGS = load_nlp_resources()

def get_beijing_time_str():
    utc_now = datetime.now(timezone.utc)
    beijing_now = utc_now + timedelta(hours=8)
    return beijing_now.strftime('%m%d_%H%M')

def clear_all_state():
    """å®Œå…¨é‡ç½®çŠ¶æ€"""
    for k in ['gen_words_data', 'raw_count', 'process_time', 'anki_input_text']:
        if k in st.session_state: del st.session_state[k]
    st.session_state['uploader_id'] = str(random.randint(100000, 999999))
    if 'paste_key' in st.session_state: st.session_state['paste_key'] = ""

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ (çº¯ Python å®ç°ï¼Œæ— å¤–éƒ¨ API è°ƒç”¨)
# ==========================================

def extract_text_from_file(uploaded_file):
    import pypdf, docx, ebooklib
    from ebooklib import epub
    from bs4 import BeautifulSoup
    
    text = ""
    file_ext = uploaded_file.name.split('.')[-1].lower()
    
    try:
        if file_ext == 'txt':
            bytes_data = uploaded_file.getvalue()
            for enc in ['utf-8', 'gb18030', 'gbk', 'latin-1']:
                try: text = bytes_data.decode(enc); break
                except: continue
        elif file_ext == 'pdf':
            reader = pypdf.PdfReader(uploaded_file)
            text = "\n".join([p.extract_text() for p in reader.pages if p.extract_text()])
        elif file_ext == 'docx':
            doc = docx.Document(uploaded_file)
            text = "\n".join([p.text for p in doc.paragraphs])
        elif file_ext == 'epub':
            with open("temp.epub", "wb") as f: f.write(uploaded_file.getvalue())
            book = epub.read_epub("temp.epub")
            parts = []
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    parts.append(soup.get_text(separator=' ', strip=True))
            text = " ".join(parts)
            if os.path.exists("temp.epub"): os.remove("temp.epub")
    except Exception as e: return f"Error: {e}"
    return text

def is_valid_word(word):
    if len(word) < 2 or len(word) > 25: return False
    if re.search(r'(.)\1{2,}', word): return False # 3ä¸ªè¿ç»­ç›¸åŒå­—æ¯
    if not re.search(r'[aeiouy]', word): return False # æ— å…ƒéŸ³
    if re.search(r'[0-9_]', word): return False
    return True

def analyze_logic(text, min_rank, max_rank, include_unknown):
    # å¦‚æœ NLTK åŠ è½½å¤±è´¥ï¼Œæä¾›é™çº§å¤„ç†
    if MISSING_PKGS:
        return [], 0
        
    raw_tokens = re.findall(r"[a-zA-Z]+(?:[-'][a-zA-Z]+)*", text)
    total_words = len(raw_tokens)
    clean_tokens = set([t.lower() for t in raw_tokens if is_valid_word(t.lower())])
    
    final_candidates = []
    seen_lemmas = set()
    
    for w in clean_tokens:
        try: lemma = LEMMA_LIB.getLemma(w, upos='VERB')[0]
        except: lemma = w
            
        rank_lemma = VOCAB_DICT.get(lemma, 99999)
        rank_orig = VOCAB_DICT.get(w, 99999)
        best_rank = min(rank_lemma, rank_orig)
        word_to_keep = lemma if rank_lemma != 99999 else w
        
        if (min_rank <= best_rank <= max_rank) or (include_unknown and best_rank == 99999):
            if lemma not in seen_lemmas:
                final_candidates.append((word_to_keep, best_rank))
                seen_lemmas.add(lemma)
                
    final_candidates.sort(key=lambda x: x[1])
    return final_candidates, total_words

# ==========================================
# 3. Anki è§£æä¸ç”Ÿæˆ (æœ¬åœ°å¤„ç†)
# ==========================================
def parse_anki_data(raw_text):
    parsed_cards = []
    text = raw_text.replace("```json", "").replace("```", "").strip()
    matches = re.finditer(r'\{.*?\}', text, re.DOTALL)
    seen_phrases = set()

    for match in matches:
        try:
            data = json.loads(match.group(), strict=False)
            front = str(data.get("w", "")).strip().replace('**', '')
            meaning = str(data.get("m", "")).strip()
            if not front or not meaning: continue
            
            if front.lower() in seen_phrases: continue
            seen_phrases.add(front.lower())

            parsed_cards.append({
                'front': front,
                'back': meaning,
                'examples': str(data.get("e", "")).strip(),
                'etymology': str(data.get("r", "")).strip()
            })
        except: continue
    return parsed_cards

def generate_anki_package(cards_data, deck_name):
    import genanki, tempfile
    
    CSS = """
    .card { font-family: arial; font-size: 20px; text-align: center; color: #333; background-color: white; padding: 20px; }
    .nightMode .card { background-color: #2e2e2e; color: #f0f0f0; }
    .phrase { font-size: 26px; font-weight: bold; color: #0056b3; margin-bottom: 20px; }
    .definition { font-weight: bold; margin-bottom: 15px; font-size: 18px; text-align: left; }
    .examples { background: #f7f9fa; padding: 10px; border-left: 3px solid #0056b3; font-style: italic; font-size: 16px; text-align: left; }
    .nightMode .examples { background: #383838; border-color: #66b0ff; }
    .etymology { font-size: 14px; color: #666; margin-top: 15px; padding-top: 10px; border-top: 1px dashed #ccc; text-align: left; }
    """
    
    model = genanki.Model(
        random.randrange(1<<30, 1<<31), 'VocabFlow Model',
        fields=[{'name': 'Front'}, {'name': 'Meaning'}, {'name': 'Examples'}, {'name': 'Etymology'}],
        templates=[{
            'name': 'Card 1',
            'qfmt': '<div class="phrase">{{Front}}</div>',
            'afmt': '{{FrontSide}}<hr><div class="definition">{{Meaning}}</div><div class="examples">{{Examples}}</div><div class="etymology">{{Etymology}}</div>',
        }], css=CSS
    )
    
    deck = genanki.Deck(random.randrange(1<<30, 1<<31), deck_name)
    for c in cards_data:
        deck.add_note(genanki.Note(model=model, fields=[c['front'], c['back'], c['examples'].replace('\n','<br>'), c['etymology']]))
        
    with tempfile.NamedTemporaryFile(delete=False, suffix='.apkg') as tmp:
        genanki.Package(deck).write_to_file(tmp.name)
        return tmp.name

def get_ai_prompt(words, front_mode, def_mode, ex_count, need_ety):
    w_list = ", ".join(words)
    w_instr = "Key `w`: The word itself (lemma)." if "å•è¯" in front_mode else "Key `w`: A common short phrase/collocation."
    m_instr = "Key `m`: Concise Chinese definition." if def_mode == "ä¸­æ–‡" else ("Key `m`: English Definition + Chinese Definition." if def_mode == "ä¸­è‹±åŒè¯­" else "Key `m`: English definition.")
    return f"""Task: Create Anki JSON.\nWords: {w_list}\n\nFormat: NDJSON (One JSON per line).\nKeys: `w` (Front), `m` (Meaning), `e` ({ex_count} Example sentences), `r` ({'Etymology in Chinese' if need_ety else 'Empty string'}).\n\nRequirements:\n1. {w_instr}\n2. {m_instr}\n\nStart:"""

# ==========================================
# 4. ä¸»ç•Œé¢ (UI)
# ==========================================

st.title("âš¡ï¸ Vocab Flow Ultra (Stable)")

# âš ï¸ NLTK ç¼ºå¤±è­¦å‘Š (é’ˆå¯¹å›½å†…ç½‘ç»œ)
if MISSING_PKGS:
    st.error(f"""
    **âš ï¸ ç¼ºå°‘å¿…è¦çš„ NLP æ•°æ®åŒ… (ç½‘ç»œä¸‹è½½å¤±è´¥)**
    
    ç”±äºç½‘ç»œåŸå› ï¼ŒNLTK æ•°æ®æœªèƒ½è‡ªåŠ¨ä¸‹è½½ã€‚è¯·æ‰‹åŠ¨æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
    1. ç¡®ä¿å·²å®‰è£… NLTK: `pip install nltk`
    2. åœ¨ Python ä¸­è¿è¡Œ: `import nltk; nltk.download('popular')`
    3. æˆ–è€…æ‰‹åŠ¨ä¸‹è½½ç¼ºå¤±çš„åŒ…: {', '.join(MISSING_PKGS)}
    """)

if not VOCAB_DICT:
    st.warning("âš ï¸ æœªæ£€æµ‹åˆ° `coca_cleaned.csv`ï¼Œè¯é¢‘ç­›é€‰åŠŸèƒ½å°†å¤±æ•ˆã€‚è¯·å°†æ–‡ä»¶æ”¾å…¥æ ¹ç›®å½•ã€‚")

tab_guide, tab_extract, tab_anki = st.tabs(["ğŸ“– ä½¿ç”¨æŒ‡å—", "1ï¸âƒ£ å•è¯æå–", "2ï¸âƒ£ Anki åˆ¶ä½œ"])

with tab_guide:
    st.markdown("""
    <div class="guide-step">
    <span class="guide-title">æ­¥éª¤ 1: æå–</span>
    ä¸Šä¼ æ–‡ä»¶æˆ–ç²˜è´´æ–‡æœ¬ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è¿›è¡Œè¯å½¢è¿˜åŸå¹¶æŒ‰è¯é¢‘ç­›é€‰ã€‚
    </div>
    <div class="guide-step">
    <span class="guide-title">æ­¥éª¤ 2: ç”Ÿæˆ Prompt</span>
    å¤åˆ¶ç”Ÿæˆçš„ Prompt å‘é€ç»™ AIã€‚
    </div>
    <div class="guide-step">
    <span class="guide-title">æ­¥éª¤ 3: åˆ¶ä½œ Anki</span>
    ç²˜è´´ AI å›å¤çš„ JSONï¼Œç”Ÿæˆ <code>.apkg</code> å¯¼å…¥åŒ…ã€‚
    </div>
    """, unsafe_allow_html=True)

with tab_extract:
    c1, c2 = st.columns(2)
    # æŒ‰è¦æ±‚è®¾ç½®: é»˜è®¤8000/15000, æ­¥é•¿500
    min_r = c1.number_input("å¿½ç•¥æ’åå‰ N (å¤ªç®€å•çš„è¯)", 1, 20000, 8000, step=500)
    max_r = c2.number_input("å¿½ç•¥æ’åå N (å¤ªç”Ÿåƒ»çš„è¯)", 1000, 50000, 15000, step=500)
    include_unknown = st.checkbox("ğŸ”“ åŒ…å«æ— æ’åè¯æ±‡ (äººå/æ–°è¯)", value=False)
    
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡ä»¶ (æ”¯æŒ PDF/DOCX/EPUB/TXT)", key=st.session_state['uploader_id'])
    pasted_text = st.text_area("ğŸ“„ ...æˆ–åœ¨æ­¤ç²˜è´´æ–‡æœ¬", height=100, key="paste_key")
    
    col_b1, col_b2 = st.columns([1, 4])
    with col_b1: st.button("ğŸ—‘ï¸ æ¸…ç©º", on_click=clear_all_state)
    with col_b2: run_btn = st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", disabled=bool(MISSING_PKGS))

    if run_btn and not MISSING_PKGS:
        txt = extract_text_from_file(uploaded_file) if uploaded_file else pasted_text
        if len(txt.strip()) > 5:
            with st.spinner("æ­£åœ¨åˆ†æ..."):
                t0 = time.time()
                data, raw_c = analyze_logic(txt, min_r, max_r, include_unknown)
                st.session_state['gen_words_data'] = data
                st.session_state['raw_count'] = raw_c
                st.session_state['process_time'] = time.time() - t0
        else:
            st.warning("âš ï¸ å†…å®¹å¤ªçŸ­æˆ–æ— æ•ˆ")

    if st.session_state.get('gen_words_data'):
        data = st.session_state['gen_words_data']
        words = [x[0] for x in data]
        
        st.divider()
        m1, m2, m3 = st.columns(3)
        m1.metric("åŸæ–‡è¯æ•°", f"{st.session_state['raw_count']:,}")
        m2.metric("æå–ç”Ÿè¯", f"{len(words)}")
        m3.metric("è€—æ—¶", f"{st.session_state['process_time']:.2f}s")
        
        with st.expander("ğŸ“‹ ç”Ÿè¯åˆ—è¡¨é¢„è§ˆ", expanded=False):
            show_rank = st.checkbox("æ˜¾ç¤ºæ’å")
            disp = ", ".join([f"{w}({r})" if show_rank else w for w, r in data])
            st.markdown(f'<div class="scrollable-text">{disp}</div>', unsafe_allow_html=True)
        
        st.markdown("### âš™ï¸ Prompt è®¾ç½®")
        pc1, pc2, pc3 = st.columns(3)
        fm = pc1.selectbox("æ­£é¢", ["å•è¯ (Word)", "çŸ­è¯­ (Phrase)"])
        dm = pc2.selectbox("é‡Šä¹‰", ["è‹±æ–‡", "ä¸­æ–‡", "ä¸­è‹±åŒè¯­"])
        # æŒ‰è¦æ±‚è®¾ç½®: é»˜è®¤100, æœ€å¤§150, æœ€å°1, æ­¥é•¿1
        bs = pc3.number_input("æ¯ç»„æ•°é‡", min_value=1, max_value=150, value=100, step=1)
        
        batches = [words[i:i+bs] for i in range(0, len(words), bs)]
        st.info(f"å…±ç”Ÿæˆ {len(batches)} ç»„ Prompts")
        
        for i, batch in enumerate(batches):
            with st.expander(f"ğŸ“ ç¬¬ {i+1} ç»„ ({len(batch)} è¯)"):
                st.code(get_ai_prompt(batch, fm, dm, 1, True), language="text")

with tab_anki:
    st.caption("ğŸ‘‡ å°† AI è¿”å›çš„ JSON ç²˜è´´åˆ°æ­¤å¤„ (æ”¯æŒå¤šæ¬¡è¿½åŠ ):")
    ai_in = st.text_area("JSON è¾“å…¥", height=200, key="anki_input_text")
    d_name = st.text_input("ç‰Œç»„å", f"Vocab_{get_beijing_time_str()}")
    
    if st.button("ğŸ› ï¸ ç”Ÿæˆ .apkg", type="primary"):
        if ai_in.strip():
            cards = parse_anki_data(ai_in)
            if cards:
                st.success(f"æˆåŠŸè§£æ {len(cards)} å¼ å¡ç‰‡")
                st.dataframe(pd.DataFrame(cards)[['front','back','etymology']], use_container_width=True)
                apk = generate_anki_package(cards, d_name)
                with open(apk, "rb") as f:
                    st.download_button(f"ğŸ“¥ ä¸‹è½½ {d_name}.apkg", f, file_name=f"{d_name}.apkg")
            else: st.error("æœªæ‰¾åˆ°æœ‰æ•ˆ JSON æ•°æ®")