import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode {
        font-family: 'Consolas', 'Courier New', monospace !important;
        font-size: 16px !important;
    }
    header {visibility: hidden;}
    footer {visibility: hidden;}
    .block-container { padding-top: 1rem; }
    div[role="radiogroup"] > label {
        font-weight: bold;
        background-color: #f0f2f6;
        padding: 0 15px;
        border-radius: 5px;
    }
    /* ä¾§è¾¹æ æ ·å¼ä¼˜åŒ– */
    [data-testid="stSidebar"] {
        background-color: #f8f9fa;
        border-right: 1px solid #dee2e6;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. å†…ç½®æ•°æ® (Technical Terms & Proper Nouns)
# ==========================================
BUILTIN_TECHNICAL_TERMS = {
    # ç”¨æˆ·æŒ‡å®šè¡¥å……
    "metal": "Chem", "motion": "Law", "gravity": "Phys", "molecule": "Chem",
    "vacuum": "Phys", "electron": "Phys", "quantum": "Phys", "velocity": "Phys",
    "friction": "Phys", "catalyst": "Chem", "equilibrium": "Chem",
    
    # CS
    "algorithm": "CS", "recursion": "CS", "latency": "CS", "throughput": "CS", "bandwidth": "CS",
    "api": "CS", "json": "CS", "backend": "CS", "frontend": "CS", "fullstack": "CS",
    "neural": "AI", "transformer": "AI", "embedding": "AI", "inference": "AI",
    "python": "CS", "java": "CS", "docker": "CS", "kubernetes": "CS", "linux": "CS",
    "database": "CS", "cache": "CS", "compiler": "CS", "framework": "CS",
    "encryption": "CS", "hash": "CS", "authentication": "CS", "authorization": "CS",
    "kernel": "CS", "shell": "CS", "terminal": "CS", "repository": "CS", "commit": "CS",
    "deployment": "CS", "iteration": "CS", "agile": "CS", "polymorphism": "CS",
    "inheritance": "CS", "instantiation": "CS", "middleware": "CS", "scalability": "CS",

    # Math
    "derivative": "Math", "integral": "Math", "limit": "Math", "calculus": "Math",
    "matrix": "Math", "vector": "Math", "scalar": "Math", "tensor": "Math",
    "theorem": "Math", "axiom": "Math", "hypothesis": "Math", "lemma": "Math",
    "variance": "Math", "deviation": "Math", "correlation": "Math", "regression": "Math",
    "polynomial": "Math", "quadratic": "Math", "logarithm": "Math", "exponential": "Math",
    "integer": "Math", "fraction": "Math", "decimal": "Math", "coefficient": "Math",
    "probability": "Math", "statistics": "Math", "permutation": "Math", "combination": "Math",

    # Phys
    "acceleration": "Phys", "momentum": "Phys", "inertia": "Phys",
    "thermodynamics": "Phys", "entropy": "Phys", "enthalpy": "Phys", "kinetic": "Phys",
    "resonance": "Phys", "photon": "Phys", "positron": "Phys",
    "proton": "Phys", "neutron": "Phys", "nucleus": "Phys", "atom": "Phys",
    "relativity": "Phys", "magnetism": "Phys", "voltage": "Phys", "amperage": "Phys",
    "resistance": "Phys", "optics": "Phys", "refraction": "Phys", "reflection": "Phys",

    # Chem
    "compound": "Chem", "solvent": "Chem", "solute": "Chem", "concentration": "Chem",
    "alkali": "Chem", "enzyme": "Chem", "substrate": "Chem", "reagent": "Chem",
    "covalent": "Chem", "ionic": "Chem", "oxidation": "Chem", "reduction": "Chem",
    "isotope": "Chem", "anion": "Chem", "cation": "Chem", "polymer": "Chem",
    "monomer": "Chem", "organic": "Chem", "inorganic": "Chem", "distillation": "Chem",
    "titration": "Chem", "filtration": "Chem", "hydrocarbon": "Chem",

    # Bio
    "tissue": "Bio", "organ": "Bio", "organism": "Bio",
    "mitochondria": "Bio", "ribosome": "Bio", "membrane": "Bio", "cytoplasm": "Bio",
    "dna": "Bio", "rna": "Bio", "chromosome": "Bio", "genome": "Bio",
    "protein": "Bio", "lipid": "Bio", "carbohydrate": "Bio", "vitamin": "Bio",
    "photosynthesis": "Bio", "metabolism": "Bio", "evolution": "Bio", "mutation": "Bio",
    "pathogen": "Med", "antibody": "Med", "antigen": "Med", "vaccine": "Med",
    "inflammation": "Med", "diagnosis": "Med", "prognosis": "Med", "symptom": "Med",
    "anatomy": "Med", "physiology": "Med", "pathology": "Med", "pharmacology": "Med",

    # Biz
    "revenue": "Biz", "margin": "Biz", "liability": "Biz", "equity": "Biz", "dividend": "Biz",
    "audit": "Biz", "fiscal": "Biz", "budget": "Biz", "forecast": "Biz",
    "stakeholder": "Biz", "shareholder": "Biz", "acquisition": "Biz", "ipo": "Biz",
    "inflation": "Econ", "deflation": "Econ", "recession": "Econ", "gdp": "Econ",
    "collateral": "Biz", "liquidity": "Biz", "bankruptcy": "Biz", "portfolio": "Biz",

    # Law
    "plaintiff": "Law", "defendant": "Law", "verdict": "Law", "prosecutor": "Law",
    "appeal": "Law", "petition": "Law", "motion": "Law", "tort": "Law",
    "felony": "Law", "misdemeanor": "Law", "affidavit": "Law", "subpoena": "Law",
    "indictment": "Law", "litigation": "Law", "attorney": "Law", "jurisdiction": "Law",
    "arbitration": "Law", "statute": "Law", "constitution": "Law"
}
BUILTIN_TECHNICAL_TERMS = {k.lower(): v for k, v in BUILTIN_TECHNICAL_TERMS.items()}

PROPER_NOUNS_DB = {
    "usa": "USA", "uk": "UK", "uae": "UAE", "prc": "PRC",
    "america": "America", "england": "England", "scotland": "Scotland", "wales": "Wales",
    "japan": "Japan", "korea": "Korea", "france": "France", "germany": "Germany", "italy": "Italy",
    "spain": "Spain", "russia": "Russia", "india": "India", "brazil": "Brazil", "canada": "Canada",
    "australia": "Australia", "mexico": "Mexico", "egypt": "Egypt", "china": "China",
    "switzerland": "Switzerland", "sweden": "Sweden", "norway": "Norway", "denmark": "Denmark",
    "finland": "Finland", "netherlands": "Netherlands", "belgium": "Belgium", "austria": "Austria",
    "greece": "Greece", "turkey": "Turkey", "israel": "Israel", "saudi arabia": "Saudi Arabia",
    "singapore": "Singapore", "malaysia": "Malaysia", "thailand": "Thailand", "vietnam": "Vietnam",
    "indonesia": "Indonesia", "philippines": "Philippines",
    "london": "London", "paris": "Paris", "tokyo": "Tokyo", "beijing": "Beijing",
    "shanghai": "Shanghai", "hong kong": "Hong Kong", "sydney": "Sydney", 
    "melbourne": "Melbourne", "berlin": "Berlin", "rome": "Rome", "madrid": "Madrid",
    "new york": "New York", "los angeles": "Los Angeles", "san francisco": "San Francisco",
    "chicago": "Chicago", "seattle": "Seattle", "boston": "Boston", "houston": "Houston",
    "moscow": "Moscow", "cairo": "Cairo", "dubai": "Dubai", "mumbai": "Mumbai",
    "africa": "Africa", "asia": "Asia", "europe": "Europe", "antarctica": "Antarctica",
    "monday": "Monday", "tuesday": "Tuesday", "wednesday": "Wednesday", "thursday": "Thursday",
    "friday": "Friday", "saturday": "Saturday", "sunday": "Sunday",
    "january": "January", "february": "February", "march": "March", "april": "April", 
    "may": "May", "june": "June", "july": "July", "august": "August", 
    "september": "September", "october": "October", "november": "November", "december": "December",
    "christmas": "Christmas", "easter": "Easter", "thanksgiving": "Thanksgiving", "halloween": "Halloween",
    "google": "Google", "apple": "Apple", "microsoft": "Microsoft", "tesla": "Tesla",
    "amazon": "Amazon", "facebook": "Facebook", "twitter": "Twitter", "youtube": "YouTube", "instagram": "Instagram",
    "tiktok": "TikTok", "netflix": "Netflix", "spotify": "Spotify", "zoom": "Zoom",
    "nasa": "NASA", "fbi": "FBI", "cia": "CIA", "un": "UN", "eu": "EU", "nato": "NATO", "wto": "WTO", "who": "WHO",
    "iphone": "iPhone", "ipad": "iPad", "mac": "Mac", "windows": "Windows", "android": "Android",
    "wifi": "Wi-Fi", "internet": "Internet", "bluetooth": "Bluetooth",
    "mr": "Mr.", "mrs": "Mrs.", "ms": "Ms.", "dr": "Dr.", "prof": "Prof.",
    "phd": "PhD", "mba": "MBA", "ceo": "CEO", "cfo": "CFO", "cto": "CTO", "vip": "VIP"
}

BUILTIN_PATCH_VOCAB = {
    "online": 2000, "website": 2500, "app": 3000, "user": 1500, "data": 1000,
    "software": 3000, "hardware": 4000, "network": 2500, "server": 3500,
    "cloud": 3000, "algorithm": 6000, "database": 5000, "interface": 5000,
    "digital": 3000, "virtual": 4000, "smart": 2000, "mobile": 2500,
    "email": 2000, "text": 1000, "chat": 2000, "video": 1500, "audio": 3000,
    "link": 2000, "click": 2000, "search": 1500, "share": 1500, "post": 1500,
    "analysis": 2500, "strategy": 2500, "method": 2000, "theory": 2500,
    "research": 1500, "evidence": 2000, "significant": 2000, "factor": 1500,
    "process": 1000, "system": 1000, "available": 1500, "similar": 1500,
    "specific": 2000, "issue": 1000, "policy": 1500, "community": 1500,
    "development": 1500, "economic": 2000, "global": 2500, "environment": 2000,
    "challenge": 2500, "opportunity": 2000, "solution": 2500, "management": 2500,
    "okay": 500, "hey": 500, "yeah": 500, "wow": 1000, "cool": 1500,
    "super": 2000, "extra": 2500, "plus": 2000
}

AMBIGUOUS_WORDS = {
    "china", "turkey", "march", "may", "august", "polish"
}

# ==========================================
# 3. åˆå§‹åŒ– NLP
# ==========================================
@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    if not os.path.exists(nltk_data_dir):
        os.makedirs(nltk_data_dir)
    nltk.data.path.append(nltk_data_dir)
    for pkg in ['averaged_perceptron_tagger', 'punkt']:
        try: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
        except: pass

setup_nltk()

def smart_lemmatize(text):
    words = re.findall(r"[a-zA-Z']+", text)
    results = []
    for w in words:
        lemmas_dict = lemminflect.getAllLemmas(w)
        if not lemmas_dict:
            results.append(w.lower())
            continue
        if 'ADJ' in lemmas_dict: lemma = lemmas_dict['ADJ'][0]
        elif 'ADV' in lemmas_dict: lemma = lemmas_dict['ADV'][0]
        elif 'VERB' in lemmas_dict: lemma = lemmas_dict['VERB'][0]
        elif 'NOUN' in lemmas_dict: lemma = lemmas_dict['NOUN'][0]
        else: lemma = list(lemmas_dict.values())[0][0]
        results.append(lemma)
    return " ".join(results)

# ==========================================
# 4. è¯åº“åŠ è½½
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv"]

@st.cache_data
def load_vocab():
    vocab = {}
    file_path = next((f for f in POSSIBLE_FILES if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
            df = df.sort_values(r_col, ascending=True)
            df = df.drop_duplicates(subset=[w_col], keep='first')
            vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except: pass
    
    # å†…ç½®è¡¥ä¸
    BUILTIN_PATCH_VOCAB = {
        "online": 2000, "website": 2500, "app": 3000, "user": 1500, "data": 1000,
        "software": 3000, "hardware": 4000, "network": 2500, "server": 3500,
        "cloud": 3000, "algorithm": 6000, "database": 5000, "interface": 5000,
        "digital": 3000, "virtual": 4000, "smart": 2000, "mobile": 2500,
        "email": 2000, "text": 1000, "chat": 2000, "video": 1500, "audio": 3000,
        "link": 2000, "click": 2000, "search": 1500, "share": 1500, "post": 1500,
        "analysis": 2500, "strategy": 2500, "method": 2000, "theory": 2500,
        "research": 1500, "evidence": 2000, "significant": 2000, "factor": 1500,
        "process": 1000, "system": 1000, "available": 1500, "similar": 1500,
        "specific": 2000, "issue": 1000, "policy": 1500, "community": 1500,
        "development": 1500, "economic": 2000, "global": 2500, "environment": 2000,
        "challenge": 2500, "opportunity": 2000, "solution": 2500, "management": 2500,
        "okay": 500, "hey": 500, "yeah": 500, "wow": 1000, "cool": 1500,
        "super": 2000, "extra": 2500, "plus": 2000
    }
    
    for word, rank in BUILTIN_PATCH_VOCAB.items():
        if word not in vocab:
            vocab[word] = rank
        else:
            if vocab[word] > 20000: vocab[word] = rank
    return vocab

vocab_dict = load_vocab()

# ==========================================
# 5. AI æŒ‡ä»¤ç”Ÿæˆå™¨
# ==========================================
def generate_ai_prompt(word_list, output_format, is_term_list=False):
    words_str = ", ".join(word_list)
    
    context_instruction = ""
    if is_term_list:
        context_instruction = "\n- æ³¨æ„ï¼šè¿™äº›å•è¯æ˜¯ã€å¸¦é¢†åŸŸæ ‡ç­¾çš„ä¸“ä¸šæœ¯è¯­ (e.g. word (Domain))ã€‘ã€‚**è‹±æ–‡é‡Šä¹‰**è¯·åŠ¡å¿…æ ¹æ®æ‹¬å·å†…çš„é¢†åŸŸï¼ˆå¦‚ Math, CSï¼‰æä¾›è¯¥é¢†åŸŸçš„ç²¾ç¡®é‡Šä¹‰ã€‚**ä¸­æ–‡è§£æ**éƒ¨åˆ†è¯·ä¼˜å…ˆæ‹†è§£ã€è¯æºã€è¯æ ¹ã€è¯ç¼€ã€‘ä»¥è¾…åŠ©è®°å¿†ï¼›åªæœ‰å½“è‹±æ–‡é‡Šä¹‰éå¸¸æ™¦æ¶©éš¾æ‡‚æ—¶ï¼Œæ‰è¡¥å……ä¸­æ–‡é¢†åŸŸè§£é‡Šï¼Œå¦åˆ™è¯·èšç„¦äºè¯æºåˆ†æã€‚"

    if output_format == 'csv':
        format_req = "CSV Code Block (åç¼€å .csv)"
        format_desc = "è¯·ç›´æ¥è¾“å‡ºæ ‡å‡† CSV ä»£ç å—ã€‚"
    else:
        format_req = "TXT Code Block (åç¼€å .txt)"
        format_desc = "è¯·è¾“å‡ºçº¯æ–‡æœ¬ TXT ä»£ç å—ã€‚"

    prompt = f"""
è¯·æ‰®æ¼”ä¸€ä½ä¸“ä¸šçš„ Anki åˆ¶å¡ä¸“å®¶ã€‚è¿™æ˜¯æˆ‘æ•´ç†çš„å•è¯åˆ—è¡¨{context_instruction}ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ã€ç»ˆæåˆ¶å¡æ ‡å‡†ã€‘ä¸ºæˆ‘ç”Ÿæˆå¯¼å…¥æ–‡ä»¶ã€‚

1. æ ¸å¿ƒåŸåˆ™ï¼šåŸå­æ€§ (Atomicity)
- å«ä¹‰æ‹†åˆ†ï¼šè‹¥å•è¯æœ‰å¤šä¸ªä¸åŒå«ä¹‰ï¼Œæ‹†åˆ†ä¸ºå¤šæ¡æ•°æ®ã€‚
- ä¸¥ç¦å †ç Œï¼šæ¯å¼ å¡ç‰‡åªæ‰¿è½½ä¸€ä¸ªç‰¹å®šè¯­å¢ƒä¸‹çš„å«ä¹‰ã€‚
- **é¢†åŸŸåŒ¹é…**ï¼šå¦‚æœå•è¯å¸¦æœ‰ (Domain) æ ‡ç­¾ï¼Œè§£é‡Šå¿…é¡»ç¬¦åˆè¯¥é¢†åŸŸèƒŒæ™¯ã€‚

2. å¡ç‰‡æ­£é¢ (Column 1: Front)
- å†…å®¹ï¼šæä¾›è‡ªç„¶çš„çŸ­è¯­æˆ–æ­é… (Phrase/Collocation)ã€‚
- æ ·å¼ï¼šçº¯æ–‡æœ¬ã€‚

3. å¡ç‰‡èƒŒé¢ (Column 2: Back)
- æ ¼å¼ï¼šHTML æ’ç‰ˆï¼ŒåŒ…å«ä¸‰éƒ¨åˆ†ï¼Œå¿…é¡»ä½¿ç”¨ <br><br> åˆ†éš”ã€‚
- ç»“æ„ï¼šè‹±æ–‡é‡Šä¹‰<br><br><em>æ–œä½“ä¾‹å¥</em><br><br>ã€è¯æº/è¯æ ¹è¯ç¼€ã€‘ä¸­æ–‡åŠ©è®°

4. è¾“å‡ºæ ¼å¼æ ‡å‡† ({format_req})
- {format_desc}
- å…³é”®æ ¼å¼ï¼šä½¿ç”¨è‹±æ–‡é€—å· (,) åˆ†éš”ï¼Œä¸”æ¯ä¸ªå­—æ®µå†…å®¹å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å· ("...") åŒ…è£¹ã€‚

å¾…å¤„ç†å•è¯ï¼š
{words_str}
"""
    return prompt

# ==========================================
# 6. è¾…åŠ©å‡½æ•°ï¼šæ™ºèƒ½ç²¾é€‰ (Top N Selector)
# ==========================================
def get_top_n_words(df, n_count, current_level):
    """
    ç­›é€‰é€»è¾‘ï¼š
    1. æ’é™¤ Rank < 2000 çš„è¯ (å¤ªç®€å•)
    2. æ’é™¤ Rank > 20000 çš„è¯ (å¤ªç”Ÿåƒ», é™¤éæ˜¯æœ¯è¯­)
    3. ä¼˜å…ˆé€‰æ‹© Rank åœ¨ current_level é™„è¿‘çš„è¯ (å­¦ä¹ åŒº)
    4. æŒ‰ Rank ç”±æ˜“åˆ°éš¾æ’åº
    """
    # è¿‡æ»¤æ‰éæ™®é€šè¯ (æœ¯è¯­å’Œä¸“æœ‰åè¯å•ç®—ï¼Œè¿™é‡Œåªç­›é€‰æ™®é€šè¯)
    candidates = df[df['cat'].isin(['target', 'beyond', 'known'])].copy()
    
    # å¼ºåˆ¶ç±»å‹è½¬æ¢ï¼Œé˜²æ­¢ rank æ˜¯å­—ç¬¦ä¸²
    candidates['rank'] = pd.to_numeric(candidates['rank'], errors='coerce').fillna(99999)
    
    # æ ¸å¿ƒè¿‡æ»¤ï¼šåªçœ‹ 2000 ~ 20000 ä¹‹é—´çš„è¯ (é»„é‡‘åŒºé—´)
    mask = (candidates['rank'] >= 2000) & (candidates['rank'] <= 22000)
    golden_candidates = candidates[mask]
    
    # å¦‚æœé»„é‡‘åŒºé—´ä¸å¤Ÿæ•°ï¼Œå°±æ”¾å®½é™åˆ¶
    if len(golden_candidates) < n_count:
        final_list = candidates.sort_values(by='rank').head(n_count)
    else:
        # åœ¨é»„é‡‘åŒºé—´é‡Œï¼ŒæŒ‰ rank æ’åº
        final_list = golden_candidates.sort_values(by='rank').head(n_count)
        
    return final_list['word'].tolist()

# ==========================================
# 7. ç•Œé¢å¸ƒå±€
# ==========================================
st.title("ğŸš€ Vocab Master Pro (Smart Select)")

# === ä¾§è¾¹æ ï¼šæ™ºèƒ½ç²¾é€‰å…¥å£ ===
with st.sidebar:
    st.header("ğŸ¯ æ™ºèƒ½ç²¾é€‰ (Top N)")
    st.info("å½“æ–‡ç« å¤ªé•¿ã€ç”Ÿè¯å¤ªå¤šæ—¶ï¼Œç”¨è¿™ä¸ªåŠŸèƒ½ç­›é€‰å‡ºâ€œæ€§ä»·æ¯”æœ€é«˜â€çš„è¯æ±‡ã€‚")
    top_n_num = st.number_input("ç­›é€‰æ•°é‡", 10, 500, 50, 10)
    # ä½¿ç”¨ session_state æ¥è§¦å‘ç­›é€‰
    if st.button("ğŸ² ç”Ÿæˆç²¾é€‰è¯å•", type="primary"):
        st.session_state['trigger_top_n'] = True
    else:
        # ä¿æŒçŠ¶æ€ï¼Œé™¤éé‡æ–°åˆ†æ
        if 'trigger_top_n' not in st.session_state:
            st.session_state['trigger_top_n'] = False

    st.divider()
    if vocab_dict:
        st.caption(f"ğŸ“š æœ¬åœ°è¯åº“: {len(vocab_dict):,} è¯")

# === é¡¶éƒ¨ Tab ===
st.divider()
app_mode = st.radio("é€‰æ‹©åŠŸèƒ½æ¨¡å¼:", ["ğŸ› ï¸ æ™ºèƒ½è¿˜åŸ", "ğŸ“Š å•è¯åˆ†çº§ (AI åˆ¶å¡)"], horizontal=True)

if "æ™ºèƒ½è¿˜åŸ" in app_mode:
    c1, c2 = st.columns(2)
    with c1:
        raw_text = st.text_area("è¾“å…¥åŸå§‹æ–‡ç« ", height=400, placeholder="He was excited.")
        btn_restore = st.button("å¼€å§‹è¿˜åŸ", type="primary")
    with c2:
        if btn_restore and raw_text:
            res = smart_lemmatize(raw_text)
            st.code(res, language='text')
            st.caption("ğŸ‘† ä¸€é”®å¤åˆ¶")
        elif not raw_text: st.info("ğŸ‘ˆ è¯·è¾“å…¥æ–‡æœ¬")

else:
    # åˆ†çº§æ¨¡å¼
    col_level1, col_level2, col_space = st.columns([1, 1, 2])
    with col_level1:
        current_level = st.number_input("å½“å‰æ°´å¹³ (è¯é¢‘)", 0, 30000, 9000, 500)
    with col_level2:
        target_level = st.number_input("ç›®æ ‡æ°´å¹³ (è¯é¢‘)", 0, 30000, 15000, 500)
    
    g_col1, g_col2 = st.columns(2)
    with g_col1:
        input_mode = st.radio("è¯†åˆ«æ¨¡å¼:", ("è‡ªåŠ¨åˆ†è¯", "æŒ‰è¡Œå¤„ç†"), horizontal=True)
        grade_input = st.text_area("input_box", height=400, placeholder="motion\nmetal\nenergy\nrevenue\nabacus\nabandon", label_visibility="collapsed")
        
        # å½“ç‚¹å‡»â€œå¼€å§‹åˆ†çº§â€æ—¶ï¼Œé‡ç½® Top N çŠ¶æ€
        if st.button("å¼€å§‹åˆ†çº§", type="primary", use_container_width=True):
            st.session_state['run_analysis'] = True
            st.session_state['trigger_top_n'] = False # é‡ç½®ç­›é€‰
        
    with g_col2:
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿è¡Œåˆ†æ
        if st.session_state.get('run_analysis', False) and grade_input and vocab_dict:
            
            # --- æ•°æ®å¤„ç†é€»è¾‘ ---
            raw_items = []
            if "æŒ‰è¡Œ" in input_mode:
                lines = grade_input.split('\n')
                for line in lines:
                    if line.strip(): raw_items.append(line.strip())
            else:
                raw_items = grade_input.split()
            
            seen = set()
            unique_items = [] 
            JUNK_WORDS = {'s', 't', 'd', 'm', 'll', 've', 're'}
            
            with st.spinner("æ­£åœ¨æ™ºèƒ½åˆ†æ..."):
                for item in raw_items:
                    item_cleaned = item.strip()
                    item_lower = item_cleaned.lower()
                    
                    if item_lower in seen: continue
                    if len(item_lower) < 2 and item_lower not in ['a', 'i']: continue
                    if item_lower in JUNK_WORDS: continue
                    
                    # 1. æœ¯è¯­
                    if item_lower in BUILTIN_TECHNICAL_TERMS:
                        domain = BUILTIN_TECHNICAL_TERMS[item_lower]
                        unique_items.append({"word": f"{item_cleaned} ({domain})", "rank": 0, "cat": "term"})
                    
                    # 2. ä¸“å
                    if item_lower in PROPER_NOUNS_DB:
                        unique_items.append({"word": PROPER_NOUNS_DB[item_lower], "rank": 0, "cat": "proper"})
                        
                    # 3. æ™®é€šè¯
                    rank = vocab_dict.get(item_lower, 99999)
                    if rank != 99999:
                        if rank <= current_level: cat = "known"
                        elif rank <= target_level: cat = "target"
                        else: cat = "beyond"
                        unique_items.append({"word": item_cleaned, "rank": rank, "cat": cat})
                    
                    seen.add(item_lower)
            
            # ä¿å­˜åˆ° session_state ä»¥ä¾¿å¤ç”¨
            df = pd.DataFrame(unique_items)
            st.session_state['df_result'] = df

        # --- å±•ç¤ºç»“æœé€»è¾‘ ---
        if 'df_result' in st.session_state and not st.session_state['df_result'].empty:
            df = st.session_state['df_result']
            
            # å¦‚æœç”¨æˆ·ç‚¹å‡»äº†ä¾§è¾¹æ çš„â€œç”Ÿæˆç²¾é€‰è¯å•â€
            if st.session_state.get('trigger_top_n', False):
                st.success(f"ğŸ¯ å·²ä¸ºæ‚¨ç²¾é€‰ Top {top_n_num} ä¸ªæœ€å€¼å¾—å­¦ä¹ çš„å•è¯ (Rank 2000+)")
                top_words = get_top_n_words(df, top_n_num, current_level)
                
                if top_words:
                    with st.expander(f"ğŸ”¥ ç²¾é€‰è¯å• ({len(top_words)} ä¸ª)", expanded=True):
                        st.code("\n".join(top_words), language='text')
                        st.markdown("**ğŸ¤– AI åˆ¶å¡æŒ‡ä»¤ (ç²¾é€‰ç‰ˆ)**")
                        p_csv = generate_ai_prompt(top_words, 'csv')
                        p_txt = generate_ai_prompt(top_words, 'txt')
                        c1, c2 = st.columns(2)
                        with c1: st.code(p_csv, language='markdown')
                        with c2: st.code(p_txt, language='markdown')
                else:
                    st.warning("è¯æ±‡å¤ªç®€å•æˆ–å¤ªå°‘ï¼Œæ— æ³•ç­›é€‰ã€‚")
                st.divider()

            # å¸¸è§„å±•ç¤º (Tabs)
            df = df.sort_values(by='rank', ascending=True)
            t_term, t_target, t_proper, t_beyond, t_known = st.tabs([
                f"ğŸŸ£ ä¸“ä¸šæœ¯è¯­ ({len(df[df['cat']=='term'])})",
                f"ğŸŸ¡ é‡ç‚¹ ({len(df[df['cat']=='target'])})", 
                f"ğŸ”µ ä¸“æœ‰åè¯ ({len(df[df['cat']=='proper'])})", 
                f"ğŸ”´ è¶…çº² ({len(df[df['cat']=='beyond'])})", 
                f"ğŸŸ¢ å·²æŒæ¡ ({len(df[df['cat']=='known'])})"
            ])
            
            def show(cat_name, label, is_term=False):
                sub = df[df['cat'] == cat_name]
                if sub.empty: 
                    st.info("æ— ")
                else:
                    words = sub['word'].tolist()
                    with st.expander(f"ğŸ‘ï¸ æŸ¥çœ‹ {label} ({len(words)})", expanded=False):
                        st.code("\n".join(words), language='text')
                    
                    st.markdown(f"**ğŸ¤– AI æŒ‡ä»¤ ({label})**")
                    prompt_csv = generate_ai_prompt(words, 'csv', is_term_list=is_term)
                    prompt_txt = generate_ai_prompt(words, 'txt', is_term_list=is_term)
                    c1, c2 = st.columns(2)
                    with c1: st.code(prompt_csv, language='markdown')
                    with c2: st.code(prompt_txt, language='markdown')

            with t_term: show("term", "ä¸“ä¸šæœ¯è¯­", is_term=True)
            with t_target: show("target", "é‡ç‚¹è¯")
            with t_proper: show("proper", "ä¸“æœ‰åè¯")
            with t_beyond: show("beyond", "è¶…çº²è¯")
            with t_known: show("known", "ç†Ÿè¯")