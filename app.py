import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode {
        font-family: 'Consolas', 'Courier New', monospace !important;
        font-size: 16px !important;
    }
    header {visibility: hidden;}
    footer {visibility: hidden;}
    .block-container { padding-top: 1rem; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. å†…ç½®å·¨å‹ä¸“æœ‰åè¯åº“ (å¤§å°å†™æ˜ å°„è¡¨)
# ==========================================
# key = å…¨å°å†™, value = æ­£ç¡®æ˜¾ç¤ºæ ¼å¼
PROPER_NOUNS_DB = {
    # --- å›½å®¶ & åœ°åŒº ---
    "china": "China", "usa": "USA", "uk": "UK", "america": "America", "england": "England",
    "japan": "Japan", "korea": "Korea", "france": "France", "germany": "Germany", "italy": "Italy",
    "spain": "Spain", "russia": "Russia", "india": "India", "brazil": "Brazil", "canada": "Canada",
    "australia": "Australia", "new zealand": "New Zealand", "mexico": "Mexico", "egypt": "Egypt",
    "singapore": "Singapore", "malaysia": "Malaysia", "thailand": "Thailand", "vietnam": "Vietnam",
    "switzerland": "Switzerland", "sweden": "Sweden", "norway": "Norway", "denmark": "Denmark",
    "finland": "Finland", "netherlands": "Netherlands", "belgium": "Belgium", "austria": "Austria",
    "greece": "Greece", "turkey": "Turkey", "israel": "Israel", "saudi arabia": "Saudi Arabia",
    "dubai": "Dubai", "africa": "Africa", "asia": "Asia", "europe": "Europe", "antarctica": "Antarctica",
    
    # --- è‘—ååŸå¸‚ ---
    "london": "London", "new york": "New York", "paris": "Paris", "tokyo": "Tokyo", "beijing": "Beijing",
    "shanghai": "Shanghai", "hong kong": "Hong Kong", "sydney": "Sydney", "melbourne": "Melbourne",
    "berlin": "Berlin", "rome": "Rome", "madrid": "Madrid", "moscow": "Moscow", "cairo": "Cairo",
    "los angeles": "Los Angeles", "san francisco": "San Francisco", "chicago": "Chicago", "seattle": "Seattle",
    "boston": "Boston", "washington": "Washington", "toronto": "Toronto", "vancouver": "Vancouver",
    
    # --- æ—¶é—´ (æ˜ŸæœŸ/æœˆä»½) ---
    "monday": "Monday", "tuesday": "Tuesday", "wednesday": "Wednesday", "thursday": "Thursday",
    "friday": "Friday", "saturday": "Saturday", "sunday": "Sunday",
    "january": "January", "february": "February", "march": "March", "april": "April",
    "may": "May", "june": "June", "july": "July", "august": "August",
    "september": "September", "october": "October", "november": "November", "december": "December",
    
    # --- å¸¸è§è‹±æ–‡å (Top 50+) ---
    "james": "James", "john": "John", "robert": "Robert", "michael": "Michael", "william": "William",
    "david": "David", "richard": "Richard", "joseph": "Joseph", "thomas": "Thomas", "charles": "Charles",
    "mary": "Mary", "patricia": "Patricia", "jennifer": "Jennifer", "linda": "Linda", "elizabeth": "Elizabeth",
    "barbara": "Barbara", "susan": "Susan", "jessica": "Jessica", "sarah": "Sarah", "karen": "Karen",
    "trump": "Trump", "biden": "Biden", "obama": "Obama", "musk": "Musk", "jobs": "Jobs", "gates": "Gates",
    
    # --- ç§‘æŠ€ & å“ç‰Œ ---
    "google": "Google", "apple": "Apple", "microsoft": "Microsoft", "amazon": "Amazon", "facebook": "Facebook",
    "tesla": "Tesla", "twitter": "Twitter", "instagram": "Instagram", "youtube": "YouTube", "tiktok": "TikTok",
    "iphone": "iPhone", "ipad": "iPad", "mac": "Mac", "windows": "Windows", "android": "Android",
    "nike": "Nike", "adidas": "Adidas", "coca-cola": "Coca-Cola", "pepsi": "Pepsi", "mcdonald's": "McDonald's",
    
    # --- ç¼©å†™ & ç»„ç»‡ ---
    "nasa": "NASA", "fbi": "FBI", "cia": "CIA", "un": "UN", "eu": "EU", "nato": "NATO",
    "ceo": "CEO", "cfo": "CFO", "cto": "CTO", "phd": "PhD", "mba": "MBA", "covid": "COVID"
}

# ==========================================
# 3. åˆå§‹åŒ– NLP (æœ¬åœ°ä¸‹è½½ä¿®å¤)
# ==========================================
@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    if not os.path.exists(nltk_data_dir):
        os.makedirs(nltk_data_dir)
    nltk.data.path.append(nltk_data_dir)
    for pkg in ['averaged_perceptron_tagger', 'punkt']:
        try: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
        except: pass

setup_nltk()

def get_word_info(word):
    """
    æ ¸å¿ƒåˆ¤æ–­é€»è¾‘ï¼š
    è¿”å› (display_word, is_proper_noun)
    """
    word_lower = word.lower()
    
    # 1. ä¼˜å…ˆæŸ¥å†…ç½®å¤§è¯åº“ (æœ€å‡†)
    if word_lower in PROPER_NOUNS_DB:
        return PROPER_NOUNS_DB[word_lower], True
        
    # 2. å¦‚æœè¯åº“æ²¡æŸ¥åˆ°ï¼Œç”¨ NLTK è¾…åŠ©åˆ¤æ–­ (é’ˆå¯¹ç”Ÿåƒ»äººå)
    try:
        test_word = word.title()
        tags = nltk.pos_tag([test_word])
        pos_tag = tags[0][1]
        if pos_tag.startswith('NNP'): # ä¸“æœ‰åè¯
            return test_word, True
    except:
        pass
        
    # 3. æ™®é€šå•è¯ï¼Œå¼ºåˆ¶å°å†™
    return word_lower, False

# è¿˜åŸå¼•æ“
def smart_lemmatize(text):
    words = re.findall(r"[a-zA-Z']+", text)
    results = []
    for w in words:
        lemmas_dict = lemminflect.getAllLemmas(w)
        if not lemmas_dict:
            results.append(w.lower())
            continue
        if 'ADJ' in lemmas_dict: lemma = lemmas_dict['ADJ'][0]
        elif 'ADV' in lemmas_dict: lemma = lemmas_dict['ADV'][0]
        elif 'VERB' in lemmas_dict: lemma = lemmas_dict['VERB'][0]
        elif 'NOUN' in lemmas_dict: lemma = lemmas_dict['NOUN'][0]
        else: lemma = list(lemmas_dict.values())[0][0]
        results.append(lemma)
    return " ".join(results)

# ==========================================
# 4. è¯åº“åŠ è½½
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv"]

@st.cache_data
def load_vocab():
    file_path = next((f for f in POSSIBLE_FILES if os.path.exists(f)), None)
    if not file_path: return None
    try:
        df = pd.read_csv(file_path)
        cols = [str(c).strip().lower() for c in df.columns]
        df.columns = cols
        w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
        r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
        df[w_col] = df[w_col].astype(str).str.lower().str.strip()
        df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
        df = df.sort_values(r_col, ascending=True)
        df = df.drop_duplicates(subset=[w_col], keep='first')
        return pd.Series(df[r_col].values, index=df[w_col]).to_dict()
    except: return None

vocab_dict = load_vocab()

# ==========================================
# 5. ç•Œé¢å¸ƒå±€
# ==========================================
st.title("ğŸš€ Vocab Master Pro (Proper Nouns)")

tab_lemma, tab_grade = st.tabs(["ğŸ› ï¸ 1. æ™ºèƒ½è¿˜åŸ", "ğŸ“Š 2. å•è¯åˆ†çº§"])

# --- Tab 1 ---
with tab_lemma:
    c1, c2 = st.columns(2)
    with c1:
        raw_text = st.text_area("è¾“å…¥åŸå§‹æ–‡ç« ", height=400, placeholder="He was excited.")
        btn_restore = st.button("å¼€å§‹è¿˜åŸ", type="primary")
    with c2:
        if btn_restore and raw_text:
            res = smart_lemmatize(raw_text)
            st.code(res, language='text')
            st.caption("ğŸ‘† ä¸€é”®å¤åˆ¶")
        elif not raw_text: st.info("ğŸ‘ˆ è¯·è¾“å…¥æ–‡æœ¬")

# --- Tab 2 (åŒ…å«ä¸“æœ‰åè¯åˆ†ç±») ---
with tab_grade:
    col_a, col_b, col_c = st.columns([1, 1, 2])
    with col_a: current_level = st.number_input("å½“å‰æ°´å¹³", 0, 20000, 9000, 500)
    with col_b: target_level = st.number_input("ç›®æ ‡æ°´å¹³", 0, 20000, 15000, 500)
    st.divider()
    
    g_col1, g_col2 = st.columns(2)
    with g_col1:
        input_mode = st.radio("è¯†åˆ«æ¨¡å¼:", ("è‡ªåŠ¨åˆ†è¯", "æŒ‰è¡Œå¤„ç†"), horizontal=True)
        grade_input = st.text_area("input_box", height=400, placeholder="China\nanti\nJohn", label_visibility="collapsed")
        btn_grade = st.button("å¼€å§‹åˆ†çº§", type="primary", use_container_width=True)

    with g_col2:
        if not vocab_dict:
            st.error("âŒ è¯åº“æœªåŠ è½½")
        elif btn_grade and grade_input:
            
            # è·å–è¾“å…¥
            raw_items = []
            if "æŒ‰è¡Œ" in input_mode:
                lines = grade_input.split('\n')
                for line in lines:
                    if line.strip(): raw_items.append(line.strip())
            else:
                raw_items = grade_input.split()
            
            seen = set()
            unique_items = []
            JUNK_WORDS = {'s', 't', 'd', 'm', 'll', 've', 're'}
            
            # æ•°æ®ç»“æ„ï¼š(æ˜¾ç¤ºå•è¯, rank, ç±»åˆ«)
            data = []
            
            with st.spinner("æ­£åœ¨æ™ºèƒ½åˆ†æ..."):
                for item in raw_items:
                    item_cleaned = item.strip()
                    item_lower = item_cleaned.lower()
                    
                    if item_lower in seen: continue
                    if len(item_lower) < 2 and item_lower not in ['a', 'i']: continue
                    if item_lower in JUNK_WORDS: continue
                    
                    # === æ ¸å¿ƒé€»è¾‘ï¼šè·å–æ˜¾ç¤ºæ ¼å¼ & æ˜¯å¦ä¸ºä¸“æœ‰åè¯ ===
                    display_word, is_proper = get_word_info(item_cleaned)
                    
                    # æŸ¥è¯é¢‘
                    rank = vocab_dict.get(item_lower, 99999)
                    
                    # åˆ†ç±»é€»è¾‘
                    if is_proper:
                        cat = "proper" # æ–°å¢ï¼šä¸“æœ‰åè¯
                    else:
                        if rank <= current_level: cat = "known"
                        elif rank <= target_level: cat = "target"
                        else: cat = "beyond"
                    
                    seen.add(item_lower)
                    data.append({"word": display_word, "rank": rank, "cat": cat})
            
            # ç”Ÿæˆ Tab
            df = pd.DataFrame(data)
            if not df.empty:
                df = df.sort_values(by='rank', ascending=True)
                
                # å®šä¹‰ 4 ä¸ª Tabs
                t1, t2, t3, t4 = st.tabs([
                    f"ğŸŸ¡ é‡ç‚¹ ({len(df[df['cat']=='target'])})", 
                    f"ğŸ”µ ä¸“æœ‰åè¯ ({len(df[df['cat']=='proper'])})", 
                    f"ğŸ”´ è¶…çº² ({len(df[df['cat']=='beyond'])})", 
                    f"ğŸŸ¢ å·²æŒæ¡ ({len(df[df['cat']=='known'])})"
                ])
                
                def show(cat_name):
                    sub = df[df['cat'] == cat_name]
                    if sub.empty: 
                        st.info("æ— ")
                    else:
                        txt = "\n".join(sub['word'].tolist())
                        st.code(txt, language='text')
                        st.caption("ğŸ‘† ä¸€é”®å¤åˆ¶")

                with t1: show("target")
                with t2: show("proper") # æ–°å¢çš„ä¸“æœ‰åè¯ Tab
                with t3: show("beyond")
                with t4: show("known")
            else:
                st.warning("æ— æœ‰æ•ˆå•è¯")