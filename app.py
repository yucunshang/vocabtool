import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import json
import time
import requests
import zipfile
import concurrent.futures
from collections import Counter
from typing import Dict, List, Set, Tuple

# ==========================================
# 1. åŸºç¡€é…ç½®ä¸å¢å¼ºå‹ CSS
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode { font-family: 'Fira Code', 'Consolas', monospace !important; font-size: 15px !important; }
    .main .block-container { padding-top: 2rem; }
    .stMetric { background: #f0f2f6; padding: 10px; border-radius: 10px; border: 1px solid #d1d5db; }
    .param-box { background-color: #ffffff; padding: 20px; border-radius: 12px; box-shadow: 0 4px 6px -1px rgba(0,0,0,0.1); margin-bottom: 25px; border-left: 5px solid #ff4b4b; }
    .copy-hint { color: #6b7280; font-size: 0.85rem; margin-top: 5px; }
    /* è‡ªå®šä¹‰æ ‡ç­¾é¡µæ ·å¼ */
    .stTabs [data-baseweb="tab-list"] { gap: 10px; }
    .stTabs [data-baseweb="tab"] { background-color: #f9fafb; border-radius: 5px 5px 0 0; padding: 10px 20px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. å¥å£®çš„æ•°æ®ä¸ NLP åŠ è½½å™¨
# ==========================================
@st.cache_data(show_spinner=False)
def load_knowledge_base():
    """å¸¦å®¹é”™çš„çŸ¥è¯†åº“åŠ è½½"""
    base_path = "data"
    default_res = ({}, {}, {}, set())
    if not os.path.exists(base_path):
        return default_res
    
    try:
        def load_json(name):
            p = os.path.join(base_path, name)
            if os.path.exists(p):
                with open(p, 'r', encoding='utf-8') as f: return json.load(f)
            return {}

        terms = {k.lower(): v for k, v in load_json('terms.json').items()}
        proper = {k.lower(): v for k, v in load_json('proper.json').items()}
        patch = load_json('patch.json')
        ambiguous = set(load_json('ambiguous.json'))
        return terms, proper, patch, ambiguous
    except Exception as e:
        st.warning(f"ğŸ’¡ éƒ¨åˆ†çŸ¥è¯†åº“æ–‡ä»¶åŠ è½½å¼‚å¸¸: {e}")
        return default_res

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

@st.cache_resource
def setup_nltk():
    """NLTK åˆå§‹åŒ–ä¼˜åŒ–"""
    nltk_data_dir = os.path.join(os.getcwd(), 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    for pkg in ['averaged_perceptron_tagger', 'punkt']:
        try:
            nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
        except Exception:
            pass

setup_nltk()

# è¯å½¢è¿˜åŸæœ¬åœ°ç¼“å­˜ï¼Œå¤§å¹…æå‡é•¿æ–‡å¤„ç†é€Ÿåº¦
LEMMA_CACHE = {}

def get_lemma_optimized(w: str) -> str:
    w_lower = w.lower()
    if w_lower in LEMMA_CACHE:
        return LEMMA_CACHE[w_lower]
    
    lemmas_dict = lemminflect.getAllLemmas(w_lower)
    if not lemmas_dict:
        res = w_lower
    else:
        # ä¼˜å…ˆçº§æ’åºï¼šåŠ¨è¯ > å½¢å®¹è¯ > åè¯
        res = w_lower
        for pos in ['VERB', 'ADJ', 'NOUN', 'ADV']:
            if pos in lemmas_dict:
                res = lemmas_dict[pos][0]
                break
        if res == w_lower:
            res = list(lemmas_dict.values())[0][0]
    
    LEMMA_CACHE[w_lower] = res
    return res

@st.cache_data
def load_vocab():
    """å¥å£®çš„è¯é¢‘è¡¨åŠ è½½é€»è¾‘"""
    vocab = {}
    # å°è¯•è¯»å–å¸¸è§è¯é¢‘æ–‡ä»¶
    for f_name in ["coca_cleaned.csv", "data.csv", "data/coca.csv"]:
        if os.path.exists(f_name):
            try:
                df = pd.read_csv(f_name)
                df.columns = [str(c).strip().lower() for c in df.columns]
                w_col = next((c for c in df.columns if 'word' in c or 'å•è¯' in c), df.columns[0])
                r_col = next((c for c in df.columns if 'rank' in c or 'æ’åº' in c), df.columns[1])
                
                df[w_col] = df[w_col].astype(str).str.lower().str.strip()
                df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
                
                # å¿«é€Ÿå»é‡
                df = df.sort_values(r_col).drop_duplicates(subset=[w_col])
                vocab = dict(zip(df[w_col], df[r_col]))
                break
            except Exception:
                continue
    
    # æ³¨å…¥å†…ç½®è¡¥ä¸
    vocab.update(BUILTIN_PATCH_VOCAB)
    # æ³¨å…¥é«˜é¢‘ä¸“æœ‰åè¯/æœˆä»½ç­‰
    OVERRIDES = {
        "china": 400, "google": 800, "apple": 800, "monday": 300, "january": 400, "usa": 200
    }
    vocab.update(OVERRIDES)
    return vocab

VOCAB_DICT = load_vocab()

# ==========================================
# 3. æ–‡æ¡£è§£æå¼•æ“ (å¢å¼ºå‹)
# ==========================================
def extract_text_from_file(uploaded_file):
    """å®‰å…¨è§£æå¤šæ ¼å¼æ–‡æ¡£"""
    ext = uploaded_file.name.split('.')[-1].lower()
    try:
        content = uploaded_file.read()
        if ext == 'txt':
            return content.decode("utf-8", errors="ignore")
        elif ext == 'pdf':
            import PyPDF2
            from io import BytesIO
            reader = PyPDF2.PdfReader(BytesIO(content))
            return " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif ext == 'docx':
            import docx
            from io import BytesIO
            doc = docx.Document(BytesIO(content))
            return " ".join([p.text for p in doc.paragraphs])
        elif ext == 'epub':
            with zipfile.ZipFile(BytesIO(content)) as z:
                texts = []
                for f in z.namelist():
                    if f.endswith(('.html', '.xhtml', '.xml')):
                        raw = z.read(f).decode('utf-8', errors='ignore')
                        texts.append(re.sub(r'<[^>]+>', ' ', raw))
                return " ".join(texts)
    except ImportError as e:
        st.error(f"âŒ ç¼ºå°‘å¿…è¦ä¾èµ–åº“: {str(e).split()[-1]}ã€‚è¯·è”ç³»ç®¡ç†å‘˜å®‰è£…ã€‚")
    except Exception as e:
        st.error(f"âŒ è§£ææ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {e}")
    return ""

# ==========================================
# 4. AI è°ƒåº¦å¼•æ“ (å¸¦æŒ‡æ•°é€€é¿)
# ==========================================
def _fetch_deepseek_chunk(batch_words: List[str], prompt_template: str, api_key: str):
    url = "https://api.deepseek.com/chat/completions"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    
    # å¼ºåˆ¶æ€§è¾“å‡ºæŒ‡ä»¤ä¼˜åŒ–ï¼šå‡å°‘AIåºŸè¯
    instruction = "\n\n[System Instruction: Output RAW text for CSV only. No markdown, no intros, no conversational fillers.]\n\nWords:\n"
    full_prompt = f"{prompt_template}{instruction}{', '.join(batch_words)}"
    
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": full_prompt}],
        "temperature": 0.2, # é™ä½éšæœºæ€§
        "max_tokens": 4000
    }
    
    for attempt in range(3):
        try:
            resp = requests.post(url, json=payload, headers=headers, timeout=120)
            if resp.status_code == 429:
                time.sleep(5 * (attempt + 1)) # æŒ‡æ•°é€€é¿
                continue
            if resp.status_code == 402: return "âŒ è´¦æˆ·ä½™é¢ä¸è¶³"
            if resp.status_code == 401: return "âŒ API KEY æ— æ•ˆ"
            
            resp.raise_for_status()
            res_json = resp.json()
            content = res_json['choices'][0]['message']['content'].strip()
            
            # æ¸…æ´— Markdown è¯­æ³•å—
            content = re.sub(r'^```[a-zA-Z]*\n', '', content)
            content = re.sub(r'\n```$', '', content)
            return content
        except Exception as e:
            if attempt == 2: return f"âŒ æ‰¹æ¬¡è¯·æ±‚å¤±è´¥: {str(e)}"
            time.sleep(2)
    return "âŒ æœªçŸ¥é”™è¯¯"

def call_deepseek_api_chunked(prompt_template, words, progress_bar, status_text):
    """å¹¶å‘æ§åˆ¶å™¨"""
    api_key = st.secrets.get("DEEPSEEK_API_KEY")
    if not api_key:
        return "âš ï¸ è¯·åœ¨ Streamlit Secrets ä¸­é…ç½® DEEPSEEK_API_KEY"
    
    if not words: return ""

    # åŠ¨æ€æ‰¹æ¬¡å¤§å°ï¼šæ¯æ‰¹ 25-30 è¯æ˜¯ API ç¨³å®šæ€§çš„é»„é‡‘å¹³è¡¡ç‚¹
    CHUNK_SIZE = 25
    chunks = [words[i:i + CHUNK_SIZE] for i in range(0, len(words), CHUNK_SIZE)]
    results = [None] * len(chunks)
    
    # ä½¿ç”¨ st.status æ›¿æ¢æ™®é€š textï¼ŒVibe æ›´é«˜çº§
    with st.status("ğŸš€ AI å¹¶å‘å¼•æ“æ­£åœ¨å…¨é€Ÿå·¥ä½œ...", expanded=True) as status:
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            future_to_idx = {executor.submit(_fetch_deepseek_chunk, chunks[i], prompt_template, api_key): i for i in range(len(chunks))}
            
            completed = 0
            for future in concurrent.futures.as_completed(future_to_idx):
                idx = future_to_idx[future]
                res = future.result()
                results[idx] = res
                completed += 1
                progress_bar.progress(completed / len(chunks))
                status.write(f"âœ… æ‰¹æ¬¡ {idx+1}/{len(chunks)} å®Œæˆ ({len(chunks[idx])} è¯)")
        
        status.update(label="âœ¨ ç¼–çº‚ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼", state="complete", expanded=False)

    return "\n".join(filter(None, results))

# ==========================================
# 5. åˆ†ææµæ°´çº¿
# ==========================================
def process_pipeline(text: str):
    """é«˜åº¦ä¼˜åŒ–çš„åˆ†ææµæ°´çº¿"""
    if not text.strip(): return None, None
    
    # 1. æé€Ÿæ¸…æ´—ä¸åˆ†è¯
    raw_words = re.findall(r"\b[a-zA-Z']{2,}\b", text) # å¿½ç•¥å•å­—æ¯
    
    # 2. ç»Ÿè®¡åŸå§‹é¢‘ç‡ä»¥ä¼˜åŒ–è¯å½¢è¿˜åŸæ€§èƒ½
    word_counts = Counter(raw_words)
    
    # 3. æ‰¹é‡è¯å½¢è¿˜åŸ (ä½¿ç”¨æœ¬åœ°ç¼“å­˜)
    unique_lemmas_map = {}
    for word in word_counts:
        lemma = get_lemma_optimized(word)
        unique_lemmas_map[lemma] = unique_lemmas_map.get(lemma, 0) + word_counts[word]
    
    # 4. ç»„è£…ç»“æœ
    data = []
    for lemma, count in unique_lemmas_map.items():
        rank = VOCAB_DICT.get(lemma, 99999)
        
        display_name = lemma
        if lemma in BUILTIN_TECHNICAL_TERMS:
            display_name = f"{lemma} ({BUILTIN_TECHNICAL_TERMS[lemma]})"
        elif lemma in PROPER_NOUNS_DB:
            display_name = PROPER_NOUNS_DB[lemma]
            
        data.append({
            "word": display_name,
            "rank": rank,
            "count": count,
            "raw": lemma
        })
    
    df = pd.DataFrame(data)
    if not df.empty:
        df = df.sort_values('rank', ascending=True)
        
    return df, raw_words

# ==========================================
# 6. Streamlit UI äº¤äº’
# ==========================================
st.title("ğŸš€ Vocab Master Pro")
st.caption("å¤§å¸ˆçº§è‹±è¯­å­¦ä¹ åˆ©å™¨ï¼šæ™ºèƒ½åˆ†çº§ã€è¯å½¢è¿˜åŸã€å¤šå¹¶å‘ AI åˆ¶å¡")

# åˆå§‹åŒ–çŠ¶æ€
if "is_processed" not in st.session_state: st.session_state.is_processed = False

# å‚æ•°é…ç½®åŒº
with st.container():
    st.markdown("<div class='param-box'>", unsafe_allow_html=True)
    c1, c2, c3, c4 = st.columns(4)
    with c1: cur_lv = st.number_input("ğŸ¯ å½“å‰è¯é¢‘ (èµ·)", 0, 20000, 3500)
    with c2: tgt_lv = st.number_input("ğŸ¯ ç›®æ ‡è¯é¢‘ (æ­¢)", 0, 30000, 12000)
    with c3: top_n = st.number_input("ğŸ”¥ ç²¾é€‰ Top N", 5, 300, 50)
    with c4: 
        st.write("")
        show_rank = st.checkbox("æ˜¾ç¤ºè¯é¢‘ Rank", True)
    st.markdown("</div>", unsafe_allow_html=True)

# è¾“å…¥åŒº
col_in1, col_in2 = st.columns([2, 1])
with col_in1:
    raw_input = st.text_area("ğŸ“¥ è¾“å…¥æ–‡æœ¬", height=200, placeholder="åœ¨æ­¤ç²˜è´´é•¿ç¯‡è‹±æ–‡æ–‡ç« ã€è®ºæ–‡æˆ–å°è¯´å†…å®¹...")
with col_in2:
    uploaded_file = st.file_uploader("ğŸ“‚ æ–‡æ¡£è§£æ", type=["txt", "pdf", "docx", "epub"])
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰è¾“å…¥", use_container_width=True):
        st.rerun()

if st.button("ğŸš€ å¼€å§‹æ™ºèƒ½è§£æ", type="primary", use_container_width=True):
    full_text = raw_input
    if uploaded_file:
        full_text += "\n" + extract_text_from_file(uploaded_file)
    
    if full_text.strip():
        with st.spinner("ğŸ§  æ·±åº¦åˆ†æå¼•æ“è¿è¡Œä¸­..."):
            start = time.time()
            df, raw_words = process_pipeline(full_text)
            if df is not None:
                st.session_state.df = df
                st.session_state.raw_count = len(raw_words)
                st.session_state.duration = time.time() - start
                st.session_state.is_processed = True
    else:
        st.warning("è¯·å…ˆè¾“å…¥æ–‡æœ¬æˆ–ä¸Šä¼ æ–‡ä»¶ã€‚")

# ç»“æœæ¸²æŸ“
if st.session_state.get("is_processed"):
    df = st.session_state.df
    
    # æŒ‡æ ‡å±•ç¤º
    m1, m2, m3, m4 = st.columns(4)
    m1.metric("è¯æ±‡æ€»é‡", f"{st.session_state.raw_count}")
    m2.metric("ç‹¬ç«‹è¯æ ¹", f"{len(df)}")
    m3.metric("éœ€é‡ç‚¹å­¦ä¹ ", f"{len(df[(df['rank'] > cur_lv) & (df['rank'] <= tgt_lv)])}")
    m4.metric("è§£æè€—æ—¶", f"{st.session_state.duration:.2f}s")
    
    # åˆ†ç±»é€»è¾‘
    target_df = df[(df['rank'] > cur_lv) & (df['rank'] <= tgt_lv)].copy()
    beyond_df = df[df['rank'] > tgt_lv].copy()
    known_df = df[df['rank'] <= cur_lv].copy()
    top_n_df = target_df.head(top_n)

    tabs = st.tabs([f"ğŸ”¥ Top {len(top_n_df)}", "ğŸŸ¡ é‡ç‚¹è¯", "ğŸ”´ è¶…çº²è¯", "ğŸŸ¢ å·²æŒæ¡", "ğŸ“‹ å¯¼å‡ºåŸæ–‡"])
    
    def render_vocab_tab(tab, data_df, key_prefix):
        with tab:
            if data_df.empty:
                st.info("è¯¥èŒƒå›´å†…æ²¡æœ‰å•è¯ã€‚")
                return
            
            # å•è¯åˆ—è¡¨é¢„è§ˆ
            words_to_show = []
            for _, row in data_df.iterrows():
                label = f"{row['word']} [{int(row['rank'])}]" if show_rank and row['rank'] < 99999 else row['word']
                words_to_show.append(label)
            
            with st.expander("ğŸ‘ï¸ æŸ¥çœ‹å¾…å¤„ç†å•è¯åˆ—è¡¨"):
                st.code("\n".join(words_to_show))
            
            st.divider()
            
            # AI ç”ŸæˆåŒº
            st.subheader("ğŸ¤– AI å¡ç‰‡è‡ªåŠ¨æ„å»º")
            exp_fmt = st.radio("å¯¼å‡ºæ ¼å¼", ["TXT (Anki)", "CSV"], horizontal=True, key=f"fmt_{key_prefix}")
            
            at1, at2 = st.tabs(["âš¡ å†…ç½®å¹¶å‘ç”Ÿæˆ", "ğŸ”— å¤åˆ¶ Prompt æ‰‹åŠ¨ç”Ÿæˆ"])
            
            with at1:
                if st.button(f"âœ¨ å¬å”¤ AI ç¼–çº‚ {len(data_df)} ä¸ªå•è¯", key=f"btn_{key_prefix}"):
                    p_bar = st.progress(0)
                    from app import get_base_prompt_template # å‡è®¾åŸ template å‡½æ•°ä¿ç•™
                    prompt = get_base_prompt_template(exp_fmt)
                    
                    raw_words_list = data_df['raw'].tolist()
                    result = call_deepseek_api_chunked(prompt, raw_words_list, p_bar, st.empty())
                    
                    if result and "âŒ" not in result:
                        st.download_button(
                            "ğŸ“¥ ä¸‹è½½ Anki å¯¼å…¥æ–‡ä»¶", 
                            result.encode('utf-8-sig'), 
                            file_name=f"anki_{key_prefix}.{exp_fmt.lower()}",
                            mime="text/plain",
                            use_container_width=True,
                            type="primary"
                        )
                        st.code(result, language="text")
                    else:
                        st.error(result)
            
            with at2:
                from app import get_base_prompt_template
                full_p = f"{get_base_prompt_template(exp_fmt)}\n\nWords:\n{', '.join(data_df['raw'].tolist())}"
                st.code(full_p)

    render_vocab_tab(tabs[0], top_n_df, "top")
    render_vocab_tab(tabs[1], target_df, "target")
    render_vocab_tab(tabs[2], beyond_df, "beyond")
    render_vocab_tab(tabs[3], known_df, "known")
    
    with tabs[4]:
        st.write("æ­¤å¤„å¯æ ¹æ®éœ€è¦å¢åŠ å¯¼å‡ºé€»è¾‘...")