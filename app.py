import streamlit as st
import pandas as pd
import re
import os
import random
import json
import time
from datetime import datetime, timedelta, timezone

# ==========================================
# 0. Vibe Config & Constants
# ==========================================
st.set_page_config(
    page_title="Vocab Flow Ultra", 
    page_icon="âš¡ï¸", 
    layout="centered", 
    initial_sidebar_state="collapsed"
)

CUSTOM_CSS = """
<style>
    .stTextArea textarea { font-family: 'Consolas', monospace; font-size: 14px; }
    .stButton>button { border-radius: 8px; font-weight: 600; width: 100%; margin-top: 5px; }
    .scrollable-text {
        max-height: 200px; overflow-y: auto; padding: 10px;
        border: 1px solid #eee; border-radius: 5px; background-color: #fafafa;
        font-family: monospace; white-space: pre-wrap;
    }
    .guide-step { background-color: #f8f9fa; padding: 20px; border-radius: 10px; margin-bottom: 20px; border-left: 5px solid #0056b3; }
    .guide-title { font-size: 18px; font-weight: bold; color: #0f172a; display: block; margin-bottom: 8px;}
    .stat-box { text-align: center; padding: 10px; background: #f1f5f9; border-radius: 8px; }
</style>
"""

ANKI_CSS = """
.card { font-family: 'Arial', sans-serif; font-size: 20px; text-align: center; color: #333; background-color: white; padding: 20px; }
.nightMode .card { background-color: #2e2e2e; color: #f0f0f0; }
.phrase { font-size: 28px; font-weight: 700; color: #0056b3; margin-bottom: 20px; }
.definition { font-weight: bold; color: #222; margin-bottom: 15px; text-align: left; }
.examples { background: #f7f9fa; padding: 12px; border-left: 4px solid #0056b3; font-style: italic; text-align: left; margin-bottom: 15px;}
.etymology { font-size: 16px; color: #555; background-color: #fffdf5; padding: 10px; border: 1px solid #fef3c7; border-radius: 6px; }
"""

# ==========================================
# 1. Core Services (Logic Only)
# ==========================================

@st.cache_resource(show_spinner="æ­£åœ¨åˆå§‹åŒ– NLP æ™ºèƒ½å¼•æ“...")
def load_nlp_resources():
    """
    æŒ‰éœ€åŠ è½½ NLP åº“ & è¯æ€§æ ‡æ³¨æ¨¡å‹
    """
    import nltk
    import lemminflect
    
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    
    # æ ¸å¿ƒåŒ…ï¼šTokenizer, Tagger(è¯æ€§), Wordnet(è¿˜åŸ)
    required_packages = ['averaged_perceptron_tagger', 'punkt', 'punkt_tab', 'wordnet']
    
    for pkg in required_packages:
        try: 
            # å°è¯•æŸ¥æ‰¾ä¸åŒè·¯å¾„ä¸‹çš„èµ„æº
            nltk.data.find(f'tokenizers/{pkg}')
        except LookupError: 
            try: nltk.data.find(f'taggers/{pkg}')
            except LookupError: 
                try: nltk.data.find(f'corpora/{pkg}')
                except LookupError: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
            
    return nltk, lemminflect

@st.cache_data
def load_vocab_db():
    """
    åŠ è½½ COCA è¯é¢‘è¡¨
    """
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    if not file_path: return {}, None

    try:
        df = pd.read_csv(file_path)
        df.columns = [c.strip().lower() for c in df.columns]
        w_col = next((c for c in df.columns if 'word' in c), df.columns[0])
        r_col = next((c for c in df.columns if 'rank' in c), df.columns[1])
        
        df = df.dropna(subset=[w_col])
        df[w_col] = df[w_col].astype(str).str.lower().str.strip()
        df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
        
        # å»é‡ï¼Œä¿ç•™æ’åæœ€é å‰çš„ (e.g. åŠ¨è¯ wind å’Œåè¯ windï¼Œä¿ç•™é«˜é¢‘çš„)
        df = df.sort_values(r_col).drop_duplicates(subset=[w_col], keep='first')
        return pd.Series(df[r_col].values, index=df[w_col]).to_dict(), df
    except Exception as e:
        st.error(f"è¯åº“åŠ è½½å¤±è´¥: {e}")
        return {}, None

VOCAB_DICT, FULL_DF = load_vocab_db()

def read_file_content(uploaded_file):
    """
    é²æ£’çš„æ–‡ä»¶è¯»å–å™¨ (PDF/Docx/Epub/Txt)
    """
    import pypdf, docx, ebooklib
    from ebooklib import epub
    from bs4 import BeautifulSoup

    ftype = uploaded_file.name.split('.')[-1].lower()
    try:
        if ftype == 'txt':
            return uploaded_file.getvalue().decode("utf-8", errors="ignore")
        elif ftype == 'pdf':
            reader = pypdf.PdfReader(uploaded_file)
            return "\n".join([p.extract_text() or "" for p in reader.pages])
        elif ftype == 'docx':
            doc = docx.Document(uploaded_file)
            return "\n".join([p.text for p in doc.paragraphs])
        elif ftype == 'epub':
            import tempfile
            with tempfile.NamedTemporaryFile(delete=False, suffix='.epub') as tmp:
                tmp.write(uploaded_file.getvalue())
                tmp_path = tmp.name
            
            text = ""
            book = epub.read_epub(tmp_path)
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    text += soup.get_text(separator=' ', strip=True) + " "
            os.remove(tmp_path)
            return text
    except Exception as e:
        return f"Error reading file: {str(e)}"
    return ""

def process_text_logic(text, cfg):
    """
    V29 Plus ç®—æ³•ï¼šNLP POS è¿‡æ»¤ + æ™ºèƒ½è¿˜åŸ + è¯é¢‘æ¯”å¯¹
    """
    nltk, lemminflect = load_nlp_resources()
    
    # 1. é¢„æ¸…æ´—ï¼šå»é™¤ URLã€é‚®ç®±ã€æ•°å­—ä¹±ç 
    text = re.sub(r'http\S+|www\.\S+', '', text)
    text = re.sub(r'\b\w*\d\w*\b', '', text)
    
    # 2. NLTK æ™ºèƒ½åˆ†è¯
    raw_tokens = nltk.word_tokenize(text)
    total_words = len(raw_tokens)
    
    # 3. è¯æ€§æ ‡æ³¨ (POS Tagging) - æ ¸å¿ƒç²¾åº¦æ¥æº
    tagged_tokens = nltk.pos_tag(raw_tokens)
    
    # å…è®¸çš„è¯æ€§: åè¯(N), åŠ¨è¯(V), å½¢å®¹è¯(J), å‰¯è¯(R)
    # æ‹’ç»: ä»£è¯, ä»‹è¯, è¿è¯, å† è¯ç­‰
    VALID_PREFIXES = ('N', 'V', 'J', 'R') 
    
    candidates = []
    seen_lemmas = set()
    
    for word, tag in tagged_tokens:
        # A. åŸºç¡€è¿‡æ»¤ï¼šå…¨å­—æ¯ï¼Œé•¿åº¦>1
        if len(word) < 2 or not word.isalpha():
            continue
            
        # B. è¯æ€§è¿‡æ»¤
        if not tag.startswith(VALID_PREFIXES):
            continue
            
        # C. æ™ºèƒ½è¿˜åŸ (Lemma)
        # å°† Treebank Tag æ˜ å°„åˆ° Lemminflect UPOS
        if tag.startswith('V'): upos = 'VERB'
        elif tag.startswith('J'): upos = 'ADJ'
        elif tag.startswith('R'): upos = 'ADV'
        else: upos = 'NOUN'
        
        lemma = lemminflect.getLemma(word.lower(), upos=upos)[0]
        
        # D. æŸ¥è¯é¢‘ (ä¼˜å…ˆæŸ¥ Lemma)
        rank_l = VOCAB_DICT.get(lemma, 99999)
        rank_w = VOCAB_DICT.get(word.lower(), 99999)
        
        # ç¡®å®šæœ€ä½³ Rank (å¦‚æœä¸¤è€…éƒ½æœ‰æ’åï¼Œå–æ›´é å‰çš„)
        if rank_l != 99999 and rank_w != 99999:
            best_rank = min(rank_l, rank_w)
        elif rank_l != 99999:
            best_rank = rank_l
        else:
            best_rank = rank_w
            
        # E. èŒƒå›´åˆ¤å®š
        in_range = cfg['curr'] <= best_rank <= cfg['targ']
        is_unknown = (best_rank == 99999 and cfg['include_unknown'])
        
        if in_range or is_unknown:
            # è¿‡æ»¤å…¨å¤§å†™ç¼©å†™ (å¦‚ API, HTML)ï¼Œé™¤éå®ƒæ˜¯å·²çŸ¥é«˜é¢‘è¯
            if word.isupper() and best_rank > 5000:
                continue

            # æœ€ç»ˆå±•ç¤ºè¯ (ä¼˜å…ˆå±•ç¤º Lemma)
            display_word = lemma if rank_l != 99999 else word.lower()
            
            if display_word not in seen_lemmas:
                candidates.append((display_word, best_rank))
                seen_lemmas.add(display_word)

    # æ’åºï¼šæŒ‰è¯é¢‘ (å¸¸è§ -> ç”Ÿåƒ» -> æœªçŸ¥)
    return sorted(candidates, key=lambda x: x[1]), total_words

def create_anki_pkg(cards, deck_name):
    """
    ç”Ÿæˆ Anki .apkg æ–‡ä»¶
    """
    import genanki, tempfile
    
    model = genanki.Model(
        random.randrange(1 << 30, 1 << 31),
        'VocabFlow Model',
        fields=[{'name': 'Front'}, {'name': 'Meaning'}, {'name': 'Examples'}, {'name': 'Etymology'}],
        templates=[{
            'name': 'Card 1',
            'qfmt': '<div class="phrase">{{Front}}</div>',
            'afmt': '{{FrontSide}}<hr><div class="definition">{{Meaning}}</div><div class="examples">{{Examples}}</div>{{#Etymology}}<div class="etymology">ğŸŒ± {{Etymology}}</div>{{/Etymology}}',
        }],
        css=ANKI_CSS
    )
    
    deck = genanki.Deck(random.randrange(1 << 30, 1 << 31), deck_name)
    for c in cards:
        deck.add_note(genanki.Note(model=model, fields=[c['w'], c['m'], c['e'].replace('\n','<br>'), c['r']]))
        
    with tempfile.NamedTemporaryFile(delete=False, suffix='.apkg') as tmp:
        genanki.Package(deck).write_to_file(tmp.name)
        return tmp.name

def get_ai_prompt(words, settings):
    """
    ç”Ÿæˆ AI æç¤ºè¯ (NDJSON æ ¼å¼)
    """
    w_str = ", ".join(words)
    context_desc = "short phrase/collocation (2-5 words)" if settings['front'] == "çŸ­è¯­" else "word itself"
    
    return f"""
Task: Create Anki cards for learning English.
Target Words: {w_str}

**OUTPUT FORMAT: NDJSON (One JSON object per line).**
**Strictly NO markdown code blocks (```json ... ```). Just raw NDJSON.**

**Fields:**
1. `w` (Front): The {context_desc} containing the target word.
2. `m` (Meaning): {settings['def_lang']} definition.
3. `e` (Examples): {settings['ex_count']} example sentence(s).
4. `r` (Etymology): {"Root/Etymology (Simple Chinese)" if settings['ety'] else "Empty string"}.

**Example:**
{{"w": "serendipity", "m": "æ„å¤–å‘ç°çå®çš„è¿æ°”", "e": "It was pure serendipity.", "r": "from Horace Walpole"}}

**Start:**
"""

# ==========================================
# 2. Main App Logic
# ==========================================

st.markdown(CUSTOM_CSS, unsafe_allow_html=True)
st.title("âš¡ï¸ Vocab Flow Ultra")

if not VOCAB_DICT:
    st.warning("âš ï¸ æœªæ£€æµ‹åˆ° `coca_cleaned.csv`ï¼Œä»…å¯ä½¿ç”¨æ— è¯é¢‘è¿‡æ»¤æ¨¡å¼ã€‚")

# åˆå§‹åŒ– Session State
if 'uploader_id' not in st.session_state: st.session_state['uploader_id'] = "1000"
if 'gen_data' not in st.session_state: st.session_state['gen_data'] = []
if 'raw_count' not in st.session_state: st.session_state['raw_count'] = 0

tab_guide, tab_extract, tab_anki = st.tabs(["ğŸ“– æŒ‡å—", "1ï¸âƒ£ æå–", "2ï¸âƒ£ Anki"])

with tab_guide:
    st.markdown("""
    <div class="guide-step">
        <span class="guide-title">Step 1: æå–ç”Ÿè¯ (Extract)</span>
        ä¸Šä¼  PDF/EPUB/TXTã€‚ç³»ç»Ÿåˆ©ç”¨ NLP ç®—æ³•è‡ªåŠ¨å‰”é™¤ <code>the, of, is</code> ç­‰è™šè¯ï¼Œå¹¶æ™ºèƒ½è¿˜åŸæ—¶æ€ã€‚
    </div>
    <div class="guide-step">
        <span class="guide-title">Step 2: è·å– Prompt (AI Generation)</span>
        å¤åˆ¶ç”Ÿæˆçš„ Prompt å‘é€ç»™ AI (ChatGPT/Claude)ï¼Œè·å– JSON æ•°æ®ã€‚
    </div>
    <div class="guide-step">
        <span class="guide-title">Step 3: åˆ¶ä½œ Anki (Create Deck)</span>
        å°† AI è¿”å›çš„ JSON ç²˜è´´å›æ¥ï¼Œä¸€é”®æ‰“åŒ…ä¸‹è½½ .apkg æ–‡ä»¶ã€‚
    </div>
    """, unsafe_allow_html=True)

with tab_extract:
    st.info("ğŸ’¡ **Vibe Update**: å·²å¯ç”¨ NLP è¯æ€§åˆ†æã€‚ç³»ç»Ÿå°†è‡ªåŠ¨å¿½ç•¥ä»£è¯ã€ä»‹è¯ã€è¿è¯ï¼Œä»…ä¿ç•™æ ¸å¿ƒå®è¯ã€‚")
    
    c1, c2 = st.columns(2)
    curr = c1.number_input("å¿½ç•¥å‰ N é«˜é¢‘è¯", 0, 20000, 2000, step=100)
    targ = c2.number_input("å¿½ç•¥å N ç”Ÿåƒ»è¯", 2000, 60000, 20000, step=500)
    include_unknown = st.checkbox("ğŸ”“ åŒ…å«æœªæ”¶å½•è¯ (äººå/æ–°è¯)", False)

    uploaded = st.file_uploader("ğŸ“„ æ‹–å…¥æ–‡æ¡£ (PDF/EPUB/TXT)", key=st.session_state['uploader_id'])
    text_input = st.text_area("...æˆ–ç›´æ¥ç²˜è´´æ–‡æœ¬", height=100)

    if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
        raw_text = read_file_content(uploaded) if uploaded else text_input
        if len(raw_text) < 5:
            st.error("âš ï¸ å†…å®¹å¤ªçŸ­ï¼Œè¯·é‡æ–°è¾“å…¥")
        else:
            with st.status("æ­£åœ¨è¿›è¡Œ NLP åˆ†æ...", expanded=True) as status:
                status.write("ğŸ§  æ­£åœ¨åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œåˆ†è¯...")
                data, total = process_text_logic(raw_text, {'curr': curr, 'targ': targ, 'include_unknown': include_unknown})
                st.session_state['gen_data'] = data
                st.session_state['raw_count'] = total
                status.update(label=f"âœ… åˆ†æå®Œæˆï¼ä» {total} è¯ä¸­æå–å‡º {len(data)} ä¸ªç”Ÿè¯", state="complete", expanded=False)

    # ç»“æœå±•ç¤ºåŒº
    if st.session_state['gen_data']:
        data = st.session_state['gen_data']
        words = [x[0] for x in data]
        
        st.divider()
        c_k1, c_k2 = st.columns(2)
        c_k1.metric("æ–‡æ¡£æ€»è¯æ•°", st.session_state.get('raw_count', 0))
        c_k2.metric("ç”Ÿè¯æå–æ•°", len(data))

        with st.expander("âš™ï¸ Prompt è®¾ç½® (ç‚¹å‡»å±•å¼€)", expanded=True):
            cols = st.columns(4)
            s_front = cols[0].selectbox("æ­£é¢å†…å®¹", ["å•è¯", "çŸ­è¯­"], index=0)
            s_def = cols[1].selectbox("é‡Šä¹‰è¯­è¨€", ["ä¸­æ–‡", "è‹±æ–‡", "ä¸­è‹±"], index=0)
            s_ex = cols[2].slider("ä¾‹å¥æ•°é‡", 1, 3, 1)
            s_ety = cols[3].checkbox("åŒ…å«è¯æº", True)
            
            settings = {'front': s_front, 'def_lang': s_def, 'ex_count': s_ex, 'ety': s_ety}

        # é¢„è§ˆä¸å¤åˆ¶åŒº
        st.markdown("### ğŸ“‹ ç”Ÿæˆ Prompt")
        batch_size = st.number_input("æ¯ç»„å•è¯æ•°é‡ (Batch Size)", 10, 500, 50)
        batches = [words[i:i + batch_size] for i in range(0, len(words), batch_size)]
        
        for i, batch in enumerate(batches):
            prompt = get_ai_prompt(batch, settings)
            st.text_area(f"ç¬¬ {i+1} ç»„ (å…± {len(batch)} è¯)", value=prompt, height=100, key=f"p_{i}")

        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®", type="secondary"):
            st.session_state.clear()
            st.rerun()

with tab_anki:
    st.markdown("### ğŸ“¦ JSON è½¬ Anki")
    json_input = st.text_area("åœ¨æ­¤ç²˜è´´ AI å›å¤çš„ JSON (æ”¯æŒå¤šæ¬¡è¿½åŠ )", height=200)
    deck_name = st.text_input("ç‰Œç»„åç§°", f"Vocab_{datetime.now().strftime('%m%d')}")

    if json_input:
        try:
            # å®½æ¾è§£æ: æå–æ‰€æœ‰ {} åŒ…è£¹çš„å†…å®¹
            matches = re.findall(r'\{.*?\}', json_input, re.DOTALL)
            parsed = []
            for m in matches:
                try: parsed.append(json.loads(m))
                except: pass
            
            clean_cards = []
            for p in parsed:
                if 'w' in p and 'm' in p:
                    clean_cards.append({
                        'w': p.get('w'), 'm': p.get('m'), 
                        'e': p.get('e', ''), 'r': p.get('r', '')
                    })
            
            if clean_cards:
                st.success(f"âœ… è§£ææˆåŠŸ: {len(clean_cards)} å¼ å¡ç‰‡")
                # é¢„è§ˆå‰5æ¡
                st.dataframe(pd.DataFrame(clean_cards).head(5)[['w', 'm', 'r']], hide_index=True, use_container_width=True)
                
                pkg_path = create_anki_pkg(clean_cards, deck_name)
                with open(pkg_path, "rb") as f:
                    st.download_button(f"ğŸ“¥ ä¸‹è½½ {deck_name}.apkg", f, file_name=f"{deck_name}.apkg", type="primary")
            else:
                st.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ JSON å¯¹è±¡ï¼Œè¯·æ£€æŸ¥ç²˜è´´å†…å®¹ã€‚")
        except Exception as e:
            st.error(f"è§£æé”™è¯¯: {e}")