import streamlit as st
import pandas as pd
import os
import sys

# ==========================================
# 0. é¡µé¢é…ç½® (å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤)
# ==========================================
st.set_page_config(
    page_title="Vocab Master (Debug Mode)", 
    page_icon="ğŸ› ï¸", 
    layout="centered"
)

# ==========================================
# 1. ä¾èµ–åº“æ£€æŸ¥ä¸å¯¼å…¥
# ==========================================
# æ£€æŸ¥ NLTK
try:
    import nltk
    from nltk.stem import WordNetLemmatizer
except ImportError:
    st.error("âŒ ç¼ºå°‘ nltk åº“ã€‚è¯·è¿è¡Œ: pip install nltk")
    st.stop()

# æ£€æŸ¥ Lemminflect (å¯é€‰ï¼Œæ²¡æœ‰å°±é™çº§)
try:
    import lemminflect
    HAS_LEMMINFLECT = True
except ImportError:
    HAS_LEMMINFLECT = False
    st.warning("âš ï¸ æœªæ£€æµ‹åˆ° lemminflect åº“ï¼Œå°†ä½¿ç”¨åŸºç¡€è¿˜åŸæ¨¡å¼ã€‚å»ºè®®è¿è¡Œ: pip install lemminflect")

# ==========================================
# 2. èµ„æºåˆå§‹åŒ– (å¸¦é”™è¯¯æ•è·)
# ==========================================
@st.cache_resource
def init_nlp_resources():
    status_text = st.empty()
    status_text.text("æ­£åœ¨åˆå§‹åŒ– NLP èµ„æº...")
    
    # 1. ä¸‹è½½ NLTK æ•°æ®
    nltk_packages = ['punkt', 'averaged_perceptron_tagger', 'wordnet', 'omw-1.4']
    nltk_path = os.path.join(os.getcwd(), 'nltk_data')
    os.makedirs(nltk_path, exist_ok=True)
    nltk.data.path.append(nltk_path)
    
    for pkg in nltk_packages:
        try:
            # å…ˆå°è¯•æŸ¥æ‰¾
            nltk.data.find(f'tokenizers/{pkg}') if pkg == 'punkt' else \
            nltk.data.find(f'taggers/{pkg}') if pkg == 'averaged_perceptron_tagger' else \
            nltk.data.find(f'corpora/{pkg}')
        except LookupError:
            try:
                # æ‰¾ä¸åˆ°åˆ™ä¸‹è½½
                nltk.download(pkg, download_dir=nltk_path, quiet=True)
            except Exception as e:
                st.error(f"âŒ NLTK æ•°æ® '{pkg}' ä¸‹è½½å¤±è´¥: {e}")
                st.info("ğŸ’¡ æç¤ºï¼šå¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œè¯·å°è¯•æŒ‚æ¢¯å­æˆ–æ‰‹åŠ¨ä¸‹è½½ NLTK dataã€‚")
                
    status_text.empty()
    return True

init_nlp_resources()

# ==========================================
# 3. æ•°æ®åŠ è½½ (å¸¦è·¯å¾„è°ƒè¯•)
# ==========================================
@st.cache_data
def load_data():
    # æ‰“å°å½“å‰è·¯å¾„ï¼Œå¸®åŠ©è°ƒè¯•
    current_dir = os.getcwd()
    files_in_dir = os.listdir(current_dir) if os.path.exists(current_dir) else []
    
    # è‡ªåŠ¨å¯»æ‰¾ csv
    target_file = "coca_cleaned.csv"
    if target_file not in files_in_dir:
        st.error(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {target_file}")
        st.code(f"å½“å‰è¿è¡Œç›®å½•: {current_dir}\nç›®å½•ä¸‹çš„æ–‡ä»¶: {files_in_dir}")
        return None, None
        
    try:
        df = pd.read_csv(target_file)
        # æ¸…æ´—åˆ—å
        df.columns = [c.strip().lower() for c in df.columns]
        
        # å¯»æ‰¾ word å’Œ rank åˆ—
        w_col = next((c for c in df.columns if 'word' in c), None)
        r_col = next((c for c in df.columns if 'rank' in c), None)
        
        if not w_col or not r_col:
            st.error(f"âŒ CSV æ ¼å¼é”™è¯¯ã€‚æ‰¾ä¸åˆ° 'word' æˆ– 'rank' åˆ—ã€‚\næ£€æµ‹åˆ°çš„åˆ—å: {df.columns.tolist()}")
            return None, None
            
        df = df.dropna(subset=[w_col, r_col])
        df[w_col] = df[w_col].astype(str).str.lower().str.strip()
        df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
        
        # å­—å…¸åŒ–
        df = df.sort_values(r_col).drop_duplicates(subset=[w_col])
        vocab_dict = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        
        return vocab_dict, df
        
    except Exception as e:
        st.error(f"âŒ è¯»å– CSV å‡ºé”™: {e}")
        return None, None

VOCAB_DICT, FULL_DF = load_data()

# ==========================================
# 4. æ ¸å¿ƒé€»è¾‘ (æ··åˆæ¨¡å¼)
# ==========================================
def get_lemma(word, tag):
    """å…¼å®¹ä¸¤ç§åº“çš„è¿˜åŸé€»è¾‘"""
    if not word.isalpha(): return word
    
    # è½¬æ¢ tag
    pos = 'n'
    if tag.startswith('V'): pos = 'v'
    elif tag.startswith('J'): pos = 'a'
    elif tag.startswith('R'): pos = 'r'

    # ä¼˜å…ˆä½¿ç”¨ Lemminflect
    if HAS_LEMMINFLECT:
        try:
            upos = 'VERB' if pos == 'v' else 'ADJ' if pos == 'a' else 'ADV' if pos == 'r' else 'NOUN'
            return lemminflect.getLemma(word, upos=upos)[0]
        except:
            pass
            
    # é™çº§ä½¿ç”¨ WordNet
    lemmatizer = WordNetLemmatizer()
    return lemmatizer.lemmatize(word, pos)

def analyze_text(text, current_lvl, target_lvl):
    if not VOCAB_DICT: return pd.DataFrame()
    
    # ç®€å•åˆ†è¯ (ä¸ä¾èµ– punkt é˜²æ­¢æŠ¥é”™)
    try:
        tokens = nltk.word_tokenize(text.lower())
    except:
        import re
        tokens = re.findall(r"[a-z]+", text.lower())
        
    # è¯æ€§æ ‡æ³¨
    try:
        tagged = nltk.pos_tag(tokens)
    except:
        tagged = [(t, 'n') for t in tokens] # å¤±è´¥åˆ™å…¨é»˜è®¤ä¸ºåè¯
        
    res = []
    seen = set()
    
    for word, tag in tagged:
        if len(word) < 2: continue
        lemma = get_lemma(word, tag)
        
        if lemma in seen: continue
        seen.add(lemma)
        
        rank = VOCAB_DICT.get(lemma, 99999)
        
        cat = "Beyond"
        if rank <= current_lvl: cat = "Mastered"
        elif rank <= target_lvl: cat = "Target"
        
        res.append({"Word": lemma, "Rank": rank, "Category": cat})
        
    return pd.DataFrame(res)

# ==========================================
# 5. ç•Œé¢
# ==========================================
st.title("âš¡ï¸ Vocab Master (ä¿®å¤ç‰ˆ)")

if FULL_DF is None:
    st.warning("âš ï¸ è¯·å…ˆè§£å†³ä¸Šè¿°æŠ¥é”™ (ç¼ºå°‘æ–‡ä»¶æˆ–CSVæ ¼å¼ä¸å¯¹)")
else:
    txt = st.text_area("è¾“å…¥è‹±æ–‡æ–‡æœ¬", height=150)
    
    if st.button("åˆ†æ"):
        if not txt.strip():
            st.warning("è¯·è¾“å…¥å†…å®¹")
        else:
            with st.spinner("åˆ†æä¸­..."):
                df = analyze_text(txt, 4000, 8000)
                
            if df.empty:
                st.info("æœªæå–åˆ°å•è¯ (æˆ–æ‰€æœ‰å•è¯å‡ä¸åœ¨è¯åº“ä¸­)")
            else:
                target_words = df[df['Category'] == 'Target'].sort_values('Rank')
                st.success(f"åˆ†æå®Œæˆ! å‘ç° {len(target_words)} ä¸ªé‡ç‚¹ç”Ÿè¯")
                st.dataframe(target_words)