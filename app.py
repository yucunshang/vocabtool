import streamlit as st
import pandas as pd
import re
import os
import json
import time
import requests
import zipfile
import concurrent.futures
import lemminflect
import nltk
from collections import Counter  # <--- [æ–°å¢] å¼•å…¥è®¡æ•°å™¨

# ==========================================
# 0. å°è¯•å¯¼å…¥å¤šæ ¼å¼æ–‡æ¡£å¤„ç†åº“
# ==========================================
try:
    import PyPDF2
    import docx
except ImportError:
    st.error("âš ï¸ ç¼ºå°‘æ–‡ä»¶å¤„ç†ä¾èµ–ã€‚è¯·åœ¨ç»ˆç«¯è¿è¡Œ: pip install PyPDF2 python-docx")

# ==========================================
# 1. åŸºç¡€ UI é…ç½®ä¸ State åˆå§‹åŒ–
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode { font-family: 'Consolas', 'Courier New', monospace !important; font-size: 16px !important; }
    header {visibility: hidden;} footer {visibility: hidden;}\
    .block-container { padding-top: 1rem; }
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    [data-testid="stMetricValue"] { font-size: 28px !important; color: var(--primary-color) !important; }
    .param-box { background-color: var(--secondary-background-color); padding: 15px 20px 5px 20px; border-radius: 10px; border: 1px solid var(--border-color-light); margin-bottom: 20px; }
    .copy-hint { color: #888; font-size: 14px; margin-bottom: 5px; margin-top: 10px; padding-left: 5px; }
</style>
""", unsafe_allow_html=True)

# ç»Ÿä¸€åˆå§‹åŒ– Session State (æå‡å¥å£®æ€§)
if "raw_input_text" not in st.session_state: st.session_state.raw_input_text = ""
if "uploader_key" not in st.session_state: st.session_state.uploader_key = 0 
if "is_processed" not in st.session_state: st.session_state.is_processed = False
if "base_df" not in st.session_state: st.session_state.base_df = pd.DataFrame()
if "stats" not in st.session_state: st.session_state.stats = {}

# ==========================================
# 2. å…¨å±€æ ¸å¿ƒé…ç½®å­—å…¸ (é›†ä¸­ç®¡ç†ï¼Œæå‡æ‹“å±•æ€§)
# ==========================================
# æ—¢æ˜¯äººååˆæ˜¯æ ¸å¿ƒå•è¯çš„â€œå…æ­»é‡‘ç‰Œâ€ç™½åå•
SAFE_NAMES_DB = {
    'will', 'mark', 'rose', 'lily', 'bill', 'pat', 'joy', 'hope', 'penny', 'faith', 
    'grace', 'amber', 'crystal', 'dawn', 'eve', 'holly', 'ivy', 'robin', 'summer', 
    'autumn', 'winter', 'brook', 'stone', 'cliff', 'ash', 'art', 'frank', 'grant', 
    'miles', 'ward', 'dean', 'earl', 'duke', 'king', 'prince', 'baker', 'smith', 
    'foster', 'clark', 'cook', 'bell', 'hill', 'wood', 'ray', 'guy', 'max', 
    'page', 'rusty', 'cash', 'chance', 'clay', 'fox', 'lane', 'reed', 'roman', 'tanner', 
    'paris', 'london', 'chase', 'hunter', 'drake', 'drew', 'buck', 'buddy', 'chuck', 
    'colt', 'daisy', 'dash', 'destiny', 'diamond', 'dusty', 'echo', 'ember', 'fern', 
    'flint', 'flora', 'gale', 'gene', 'harmony', 'hazel', 'heather', 'iris', 'jade', 
    'jasmine', 'jewel', 'justice', 'laurel', 'marina', 'melody', 'olive', 'opal', 
    'pierce', 'piper', 'poppy', 'rex', 'ruby', 'sage', 'savannah', 'scarlett', 'scout', 
    'sienna', 'sierra', 'skip', 'sky', 'starr', 'trinity', 'victor', 'violet', 'wade', 
    'willow', 'woody', 'wren', 'brown', 'white', 'black', 'green', 'young', 'hall', 
    'wright', 'scott', 'price', 'long', 'major', 'rich', 'dick', 'christian', 'kelly', 'parker'
}

# å¼ºè¡Œè¦†ç›–çš„è¯æ±‡ç­‰çº§çŸ©é˜µ (åœ°å/èŠ‚æ—¥/æœˆä»½/å¤§å‚/æ•°å­—)
GLOBAL_ENTITY_RANKS = {
    "africa": 1000, "asia": 1000, "europe": 800, "america": 500, "australia": 1500, "antarctica": 4000,
    "china": 400, "usa": 200, "uk": 200, "britain": 800, "england": 800, "france": 800, "germany": 900, "japan": 900, "russia": 900, "india": 1000, "italy": 1000, "canada": 1000, "spain": 1200, "mexico": 1200, "brazil": 1500, "korea": 1500, "egypt": 2000, "greece": 2000, "ireland": 2000, "scotland": 2000, "wales": 2500, "sweden": 2500, "switzerland": 2500, "norway": 3000, "denmark": 3000, "finland": 3000, "poland": 2500, "netherlands": 2500, "portugal": 3000, "vietnam": 3000, "thailand": 3000, "singapore": 3000, "malaysia": 3000, "indonesia": 3000, "philippines": 3000, "turkey": 1500, "israel": 1500, "iran": 2000, "iraq": 2000,
    "american": 300, "british": 500, "english": 300, "french": 600, "german": 700, "chinese": 800, "japanese": 800, "russian": 900, "indian": 900, "italian": 1000, "spanish": 1000, "canadian": 1200, "korean": 1500, "arabic": 2000, "latin": 2000, "greek": 2000,
    "london": 800, "paris": 1000, "tokyo": 1500, "rome": 1500, "berlin": 2000, "moscow": 2000, "beijing": 2500, "shanghai": 2500, "washington": 500, "york": 500, "chicago": 1500, "boston": 1500, "sydney": 2000,
    "christmas": 800, "easter": 2000, "halloween": 2500, "thanksgiving": 1500, "valentine": 3000, "hanukkah": 5000, "ramadan": 5000, "diwali": 6000, "carnival": 4000, "festival": 1500, "holiday": 1000,
    "jewish": 1500, "muslim": 1500, "christian": 1500, "catholic": 1500, "protestant": 2500, "hindu": 3000, "buddhist": 3000, "islam": 2000, "buddhism": 3500, "christianity": 2000,
    "google": 1000, "apple": 1000, "microsoft": 1500, "facebook": 1500, "twitter": 2000, "amazon": 1500,
    "monday": 300, "tuesday": 300, "wednesday": 300, "thursday": 300, "friday": 300, "saturday": 300, "sunday": 300,
    "january": 400, "february": 400, "march": 400, "april": 400, "may": 100, "june": 400, "july": 400, "august": 1500, "september": 400, "october": 400, "november": 400, "december": 400
}

# åŸºç¡€æ•°å­—è¯å†™å…¥å…¨å±€çŸ©é˜µ
for _nw in ["zero", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen", "sixteen", "seventeen", "eighteen", "nineteen", "twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty", "ninety", "hundred", "thousand", "million", "billion", "first", "second", "third", "fourth", "fifth", "sixth", "seventh", "eighth", "ninth", "tenth", "eleventh", "twelfth", "thirteenth", "fourteenth", "fifteenth", "sixteenth", "seventeenth", "eighteenth", "nineteenth", "twentieth", "thirtieth", "fortieth", "fiftieth", "sixtieth", "seventieth", "eightieth", "ninetieth", "hundredth", "thousandth"]:
    GLOBAL_ENTITY_RANKS[_nw] = 1000

# ==========================================
# 3. æ•°æ®ä¸ NLP åˆå§‹åŒ– (å¸¦å®¹é”™æœºåˆ¶)
# ==========================================
@st.cache_data
def load_knowledge_base():
    try:
        if not os.path.exists('data'):
            return {}, {}, {}, set()
        with open('data/terms.json', 'r', encoding='utf-8') as f: terms = {k.lower(): v for k, v in json.load(f).items()}
        with open('data/proper.json', 'r', encoding='utf-8') as f: proper = {k.lower(): v for k, v in json.load(f).items()}
        with open('data/patch.json', 'r', encoding='utf-8') as f: patch = json.load(f)
        with open('data/ambiguous.json', 'r', encoding='utf-8') as f: ambiguous = set(json.load(f))
        return terms, proper, patch, ambiguous
    except Exception as e:
        print(f"Knowledge base load error: {e}")
        return {}, {}, {}, set()

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.append(nltk_data_dir)
    # é˜²å¾¡æ€§ä¸‹è½½ï¼Œå³ä½¿å¤±è´¥ä¹Ÿä¸æŠ›å‡ºå¼‚å¸¸
    for pkg in ['averaged_perceptron_tagger', 'punkt', 'names']:
        try: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
        except Exception: pass
setup_nltk()

@st.cache_data
def load_names_db():
    try:
        from nltk.corpus import names
        return set([n.lower() for n in names.words()])
    except Exception:
        # å¦‚æœ nltk ç¼ºå¤±æˆ–åŠ è½½å¤±è´¥ï¼Œè¿”å›ç©ºé›†ä»¥ä¿è¯ä¸»ä½“ç¨‹åºç»§ç»­è¿è¡Œ
        return set()
NLTK_NAMES_DB = load_names_db()

def get_lemma(w):
    try:
        lemmas_dict = lemminflect.getAllLemmas(w)
        if not lemmas_dict: return w.lower()
        for pos in ['ADJ', 'ADV', 'VERB', 'NOUN']:
            if pos in lemmas_dict: return lemmas_dict[pos][0]
        return list(lemmas_dict.values())[0][0]
    except:
        return w.lower()

@st.cache_data
def load_vocab():
    vocab = {}
    file_path = next((f for f in ["coca_cleaned.csv", "data.csv"] if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
            df = df.sort_values(r_col, ascending=True).drop_duplicates(subset=[w_col], keep='first')
            vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except Exception as e: 
            print(f"Vocab CSV load error: {e}")
    
    # æŒ‰ç…§ä¼˜å…ˆçº§åˆå¹¶è¯åº“: åŸºç¡€ CSV < è¡¥ä¸æ•°æ® < å¼ºåˆ¶å¸¸é‡æ˜ å°„
    for word, rank in BUILTIN_PATCH_VOCAB.items(): vocab[word] = rank
    for word, rank in GLOBAL_ENTITY_RANKS.items(): vocab[word] = rank
    return vocab

vocab_dict = load_vocab()

# ==========================================
# 4. æ–‡æ¡£è§£æ & AI æç¤ºè¯å¼•æ“
# ==========================================
def extract_text_from_file(uploaded_file):
    ext = uploaded_file.name.split('.')[-1].lower()
    uploaded_file.seek(0)
    try:
        if ext == 'txt':
            return uploaded_file.getvalue().decode("utf-8", errors="ignore")
        elif ext == 'pdf':
            reader = PyPDF2.PdfReader(uploaded_file)
            return " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif ext == 'docx':
            doc = docx.Document(uploaded_file)
            return " ".join([p.text for p in doc.paragraphs])
        elif ext == 'epub':
            text_blocks = []
            with zipfile.ZipFile(uploaded_file) as z:
                for filename in z.namelist():
                    if filename.endswith(('.html', '.xhtml', '.htm', '.xml')):
                        try:
                            content = z.read(filename).decode('utf-8', errors='ignore')
                            clean_text = re.sub(r'<[^>]+>', ' ', content)
                            text_blocks.append(clean_text)
                        except: pass
            return " ".join(text_blocks)
    except Exception as e:
        st.error(f"âš ï¸ æ–‡ä»¶è§£æå¤±è´¥: {e}")
        return ""
    return ""

def get_dynamic_prompt_template(export_format, front_style, add_pos, def_lang, ex_count, add_ety, split_polysemy):
    front_desc = "A natural phrase or collocation using the specific meaning." if front_style == "phrase" else "The target word itself."
    if add_pos:
        front_desc += " MUST append the precise part of speech tag at the end, e.g., ' (v)', ' (n)', ' (adj)'."
    else:
        front_desc += " Do NOT add part of speech tags."

    def_map = {
        "en": "English definition of the specific meaning",
        "zh": "Chinese definition of the specific meaning",
        "en_zh": "English definition followed by Chinese definition separated by a slash (/)"
    }
    def_desc = def_map.get(def_lang, "English definition")

    if ex_count == 0:
        ex_desc = ""
        ex_rule = "Generate ZERO example sentences. Do NOT include any examples."
    elif ex_count == 1:
        ex_desc = "<br><br><em>Italicized example sentence</em>"
        ex_rule = "Generate EXACTLY ONE example sentence. NEVER generate two or more examples."
    else:
        examples = [f"{i+1}. <em>Italicized example sentence {i+1}</em>" for i in range(ex_count)]
        ex_desc = "<br><br>" + " <br><br> ".join(examples)
        ex_rule = f"Generate EXACTLY {ex_count} example sentences, numbered as shown."

    ety_desc = "<br><br>ã€è¯æ ¹è¯ç¼€/è¯æºã€‘Chinese etymology or affix explanation." if add_ety else ""
    
    if split_polysemy:
        poly_rule = "Atomicity: ONE meaning per row. Polysemous words MUST be split into multiple separate rows. NEVER stack multiple definitions in one card."
    else:
        poly_rule = "One Card Per Word: Generate EXACTLY ONE row per input word. Extract ONLY the single most common/primary meaning. NEVER split a word into multiple cards."

    prompt = f"""# Role
You are an expert English linguist and a highly precise Anki flashcard generator.

# Task
Process the user's input words, auto-correct any spelling errors/abbreviations, and generate Anki flashcards strictly following the rules below.

# Strict Rules
1. Format: Pure {export_format} format in a single code block. NO conversational filler, NO markdown formatting outside the code block.
2. Structure: STRICTLY TWO COLUMNS per row. Format: "Column 1","Column 2"
3. Quotes: Both columns MUST be wrapped in double quotes. Use single quotes (' ') inside the text if needed.
4. {poly_rule}
5. Strict Alignment (CRITICAL): The generated phrase, part of speech (if requested), definition, example sentence(s), and etymology MUST strictly logically align with the EXACT SAME specific meaning of the target word. Do not mix definitions or examples of different meanings in a single card.
6. Example Count Constraint (CRITICAL): {ex_rule}

# Content Formatting
- Column 1 (Front): {front_desc} Do NOT bold or highlight the target word.
- Column 2 (Back): Must be exactly formatted as follows (using HTML tags):
  {def_desc}{ex_desc}{ety_desc}

# Action
Process the following list of words immediately and output ONLY the final code block:"""

    return prompt

# ==========================================
# 5. å¤šæ ¸å¹¶å‘ API å¼•æ“ (å¥å£®æ€§å‡çº§ç‰ˆ)
# ==========================================
def _fetch_deepseek_chunk(batch_words, prompt_template, api_key):
    url = "https://api.deepseek.com/chat/completions".strip()
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    system_enforcement = "\n\nã€ç³»ç»Ÿç»å¯¹å¼ºåˆ¶æŒ‡ä»¤ã€‘ç°åœ¨æˆ‘å·²ç»å‘é€äº†å•è¯åˆ—è¡¨ï¼Œè¯·ç«‹å³ä¸”ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„æ•°æ®ä»£ç ï¼Œç»å¯¹ä¸å‡†å›å¤â€œå¥½çš„â€ã€â€œæ²¡é—®é¢˜â€ç­‰ä»»ä½•å®¢å¥—è¯ï¼Œç»å¯¹ä¸å‡†ä½¿ç”¨ ```csv ç­‰ Markdown è¯­æ³•åŒ…è£¹ä»£ç ï¼"
    full_prompt = f"{prompt_template}{system_enforcement}\n\nå¾…å¤„ç†å•è¯åˆ—è¡¨ï¼š\n{', '.join(batch_words)}"
    
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": full_prompt}],
        "temperature": 0.3,
        "max_tokens": 4096
    }
    
    for attempt in range(4): # å¢åŠ é‡è¯•æ¬¡æ•°å¢å¼ºç½‘ç»œç¨³å®šæ€§
        try:
            resp = requests.post(url, json=payload, headers=headers, timeout=120)
            if resp.status_code == 429: 
                time.sleep(3 * (attempt + 1)) # æŒ‡æ•°é€€é¿ï¼Œé˜²æ­¢è¢«å°IP
                continue
            if resp.status_code == 402: return "âŒ ERROR_402_NO_BALANCE"
            elif resp.status_code == 401: return "âŒ ERROR_401_INVALID_KEY"
            resp.raise_for_status()
            
            result = resp.json()['choices'][0]['message']['content'].strip()
            
            # ä½¿ç”¨ Regex æ­£åˆ™å¼ºåŠ›æ¸…æ´— Markdown æ ‡ç­¾ (æé«˜ç¨³å®šæ€§ä¿éšœ)
            result = re.sub(r"^```(?:csv|txt|text)?\n", "", result, flags=re.IGNORECASE)
            result = re.sub(r"\n```$", "", result)
            return result.strip()
            
        except Exception as e:
            if attempt == 3:
                return f"\nğŸš¨ æ‰¹æ¬¡è¯·æ±‚å¼‚å¸¸: {str(e)}"
            time.sleep(2)
            
    return f"\nğŸš¨ æ‰¹æ¬¡è¢«é™æµï¼Œæ­¤æ‰¹æ¬¡ ({len(batch_words)}è¯) ç”Ÿæˆå¤±è´¥ã€‚"

def call_deepseek_api_chunked(prompt_template, words, progress_bar, status_text):
    try: api_key = st.secrets["DEEPSEEK_API_KEY"]
    except KeyError: return "âš ï¸ ç«™é•¿é…ç½®é”™è¯¯ï¼šæœªåœ¨ Streamlit åå° Secrets ä¸­é…ç½® DEEPSEEK_API_KEYã€‚"
    
    if not words: return "âš ï¸ é”™è¯¯ï¼šæ²¡æœ‰éœ€è¦ç”Ÿæˆçš„å•è¯ã€‚"
    
    MAX_WORDS = 250
    if len(words) > MAX_WORDS:
        st.warning(f"âš ï¸ ä¸ºä¿è¯å¹¶å‘ç¨³å®šï¼Œæœ¬æ¬¡ä»…æˆªå–å‰ **{MAX_WORDS}** ä¸ªå•è¯ã€‚")
        words = words[:MAX_WORDS]

    CHUNK_SIZE = 30  
    chunks = [words[i:i + CHUNK_SIZE] for i in range(0, len(words), CHUNK_SIZE)]
    total_words = len(words)
    processed_count = 0
    results_ordered = [None] * len(chunks)
    
    status_text.markdown("ğŸš€ **å¹¶å‘ä»»åŠ¡å·²å‘å°„ï¼** æ­£åœ¨å…¨é€Ÿç”Ÿæˆé¦–æ‰¹å¡ç‰‡ï¼ˆé¦–æ¬¡è¿”å›çº¦éœ€ 8~12 ç§’ï¼Œè¯·ç¨å€™ï¼‰...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_index = {
            executor.submit(_fetch_deepseek_chunk, chunk, prompt_template, api_key): i 
            for i, chunk in enumerate(chunks)
        }
        
        for future in concurrent.futures.as_completed(future_to_index):
            idx = future_to_index[future]
            chunk_size = len(chunks[idx])
            res = future.result()
            
            if "ERROR_402_NO_BALANCE" in res: return "âŒ é”™è¯¯ï¼šDeepSeek è´¦æˆ·ä½™é¢ä¸è¶³ï¼Œè¯·å……å€¼ã€‚"
            if "ERROR_401_INVALID_KEY" in res: return "âŒ é”™è¯¯ï¼šAPI Key æ— æ•ˆã€‚"
            
            results_ordered[idx] = res 
            
            processed_count += chunk_size
            current_progress = min(processed_count / total_words, 1.0)
            progress_bar.progress(current_progress)
            status_text.markdown(f"**âš¡ AI å¤šæ ¸å¹¶å‘å…¨é€Ÿç¼–çº‚ä¸­ï¼š** `{processed_count} / {total_words}` è¯")

    return "\n".join(filter(None, results_ordered))

# ==========================================
# 6. åˆ†æå¼•æ“ (å†…ç½®æ— æ„ŸçŸ¥äººåè¿‡æ»¤æ‹¦æˆªå™¨) - [å·²ä¿®æ”¹æ”¯æŒè¯é¢‘ç»Ÿè®¡]
# ==========================================
def analyze_words(unique_word_list, freq_dict): # <--- [ä¿®æ”¹] å¢åŠ  freq_dict å‚æ•°
    unique_items = [] 
    JUNK_WORDS = {'s', 't', 'd', 'm', 'll', 've', 're', 'don', 'doesn', 'didn', 'won', 'isn', 'aren', 'ain'}
    
    for item_lower in unique_word_list:
        if len(item_lower) < 2 and item_lower not in ['a', 'i']: continue
        if item_lower in JUNK_WORDS: continue
        
        # è·å–è¯¥è¯åœ¨æœ¬æ–‡ä¸­çš„é¢‘ç‡
        doc_freq = freq_dict.get(item_lower, 1) # <--- [æ–°å¢] è·å–è¯é¢‘

        # ğŸ›¡ï¸ æ ¸å¿ƒéšå½¢æ‹¦æˆªï¼šå¼ºåˆ¶äººåè¿‡æ»¤
        if item_lower in NLTK_NAMES_DB and item_lower not in SAFE_NAMES_DB:
            continue

        actual_rank = vocab_dict.get(item_lower, 99999)
        
        if item_lower in BUILTIN_TECHNICAL_TERMS:
            domain = BUILTIN_TECHNICAL_TERMS[item_lower]
            term_rank = actual_rank if actual_rank != 99999 else 15000
            # [ä¿®æ”¹] å¢åŠ  freq å­—æ®µ
            unique_items.append({"word": f"{item_lower} ({domain})", "rank": term_rank, "raw": item_lower, "freq": doc_freq})
            continue
            
        if item_lower in PROPER_NOUNS_DB or item_lower in AMBIGUOUS_WORDS:
            display = PROPER_NOUNS_DB.get(item_lower, item_lower.title())
            # [ä¿®æ”¹] å¢åŠ  freq å­—æ®µ
            unique_items.append({"word": display, "rank": actual_rank, "raw": item_lower, "freq": doc_freq})
            continue
            
        if actual_rank != 99999:
            # [ä¿®æ”¹] å¢åŠ  freq å­—æ®µ
            unique_items.append({"word": item_lower, "rank": actual_rank, "raw": item_lower, "freq": doc_freq})
            
    return pd.DataFrame(unique_items)

# ==========================================
# 7. UI è§†å›¾å±‚
# ==========================================
st.title("ğŸš€ Vocab Master Pro - Stable Release")
st.markdown("ğŸ’¡ æ”¯æŒç²˜è´´é•¿æ–‡æˆ–ç›´æ¥ä¸Šä¼  `TXT / PDF / DOCX / EPUB` æ–‡ä»¶ï¼Œå¹¶**å†…ç½®å…è´¹ AI** ä¸€é”®ç”Ÿæˆ Anki è®°å¿†å¡ç‰‡ã€‚ *(è¯é¢‘åˆ†çº§æ•°æ®åŸºäº COCA 20000 æƒå¨æ ¸å¿ƒè¯åº“)*")

def clear_all_inputs():
    st.session_state.raw_input_text = ""
    st.session_state.uploader_key += 1 
    st.session_state.is_processed = False
    st.session_state.base_df = pd.DataFrame()

st.markdown("<div class='param-box'>", unsafe_allow_html=True)
c1, c2, c3, c4, c5 = st.columns(5)
with c1: current_level = st.number_input("ğŸ¯ å½“å‰è¯æ±‡é‡ (èµ·)", 0, 20000, 9000, 500)     
with c2: target_level = st.number_input("ğŸ¯ ç›®æ ‡è¯æ±‡é‡ (æ­¢)", 0, 20000, 15000, 500)    
with c3: top_n = st.number_input("ğŸ”¥ ç²¾é€‰ Top N", 10, 500, 100, 10)                 
with c4: min_rank_threshold = st.number_input("ğŸ“‰ å¿½ç•¥å‰ N è¯", 0, 20000, 6000, 500) 
with c5: 
    # [ä¿®æ”¹] å¢åŠ äº†æ’åºé€»è¾‘çš„é€‰æ‹©
    sort_mode = st.radio("ğŸ“Š æ’åºä¼˜å…ˆ", ["COCA è¯é¢‘ (é»˜è®¤)", "æœ¬æ–‡å‡ºç°é¢‘ç‡"], index=0)
    show_rank = st.checkbox("ğŸ”¢ æ˜¾ç¤ºè¯¦ç»†æ•°æ®", value=True)
st.markdown("</div>", unsafe_allow_html=True)

# --- UI è°ƒæ•´ï¼šæ–‡æœ¬æ¡†ä¸ä¸Šä¼ æ–‡ä»¶å¹¶æ’ ---
col_input1, col_input2 = st.columns([3, 2])
with col_input1:
    raw_text = st.text_area("ğŸ“¥ ç²˜è´´æ–‡æœ¬ (æ”¯æŒ10ä¸‡å­—ä»¥å†…)", height=150, key="raw_input_text")
with col_input2:
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£", type=["txt", "pdf", "docx", "epub"], key=f"uploader_{st.session_state.uploader_key}")

st.button("ğŸ—‘ï¸ ä¸€é”®æ¸…ç©º", on_click=clear_all_inputs, use_container_width=True)
btn_process = st.button("ğŸš€ æé€Ÿæ™ºèƒ½è§£æ", type="primary", use_container_width=True)

st.divider()

# ==========================================
# 8. æµæ°´çº¿æ‰§è¡Œ - [å·²ä¿®æ”¹ï¼šå…ˆè¿˜åŸè¯å½¢å†ç»Ÿè®¡é¢‘ç‡]
# ==========================================
if btn_process:
    with st.spinner("ğŸ§  æ­£åœ¨æ€¥é€Ÿè¯»å–æ–‡ä»¶å¹¶è¿›è¡Œæ™ºèƒ½è§£æï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰..."):
        start_time = time.time()
        combined_text = raw_text
        if uploaded_file is not None: combined_text += "\n" + extract_text_from_file(uploaded_file)
            
        if not combined_text.strip():
            st.warning("âš ï¸ æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æœ¬ï¼")
            st.session_state.is_processed = False
        elif vocab_dict:
            # 1. æå–æ‰€æœ‰åŸå§‹å•è¯
            raw_words = re.findall(r"[a-zA-Z']+", combined_text)
            
            # 2. å…¨é‡è¯å½¢è¿˜åŸ (ä¸ºäº†ç»Ÿè®¡ accuratelyï¼Œå¿…é¡»å…ˆè¿˜åŸå† count)
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸å¯¹ raw_words å»é‡ï¼Œè€Œæ˜¯å¯¹æ‰€æœ‰è¯è¿›è¡Œè¿˜åŸ
            all_lemmas_with_dups = [get_lemma(w).lower() for w in raw_words]
            
            # 3. ç»Ÿè®¡æœ¬æ–‡è¯é¢‘
            lemma_counts = Counter(all_lemmas_with_dups)
            unique_lemmas = list(lemma_counts.keys())
            
            # 4. æ ¸å¿ƒä¸šåŠ¡è°ƒç”¨ (ä¼ å…¥ freq_dict å³ lemma_counts)
            st.session_state.base_df = analyze_words(unique_lemmas, lemma_counts)
            
            st.session_state.stats = {
                "raw_count": len(raw_words),
                "unique_count": len(unique_lemmas),
                "valid_count": len(st.session_state.base_df),
                "time": time.time() - start_time
            }
            st.session_state.is_processed = True

# ==========================================
# 9. åŠ¨æ€ç»“æœæ¸²æŸ“
# ==========================================
if st.session_state.get("is_processed", False):
    
    stats = st.session_state.stats
    col_m1, col_m2, col_m3, col_m4 = st.columns(4)
    col_m1.metric(label="ğŸ“ è§£ææ€»å­—æ•°", value=f"{stats['raw_count']:,}")
    col_m2.metric(label="âœ‚ï¸ å»é‡è¯æ ¹æ•°", value=f"{stats['unique_count']:,}")
    col_m3.metric(label="ğŸ¯ çº³å…¥åˆ†çº§è¯æ±‡", value=f"{stats['valid_count']:,}")
    col_m4.metric(label="âš¡ æé€Ÿè§£æè€—æ—¶", value=f"{stats['time']:.2f} ç§’")
    
    df = st.session_state.base_df.copy()
    
    if not df.empty:
        def categorize(row):
            r = row['rank']
            if r <= current_level: return "known"
            elif r <= target_level: return "target"
            else: return "beyond"
        
        df['final_cat'] = df.apply(categorize, axis=1)
        
        # --- [ä¿®æ”¹] æ–°å¢æ’åºé€»è¾‘ ---
        if "æœ¬æ–‡å‡ºç°é¢‘ç‡" in sort_mode:
            # æŒ‰é¢‘ç‡å€’åº (å‡ºç°æ¬¡æ•°è¶Šå¤šè¶Šé å‰)ï¼Œæ¬¡è¦å…³é”®è¯æŒ‰ Rank
            df = df.sort_values(by=['freq', 'rank'], ascending=[False, True])
        else:
            # æŒ‰ COCA æ’åæ­£åº (é»˜è®¤)
            df = df.sort_values(by='rank', ascending=True)
        # -------------------
        
        top_df = df[(df['rank'] >= min_rank_threshold) & (df['rank'] < 99999)].head(top_n)
        
        t_top, t_target, t_beyond, t_known = st.tabs([
            f"ğŸ”¥ Top {len(top_df)}", 
            f"ğŸŸ¡ é‡ç‚¹ ({len(df[df['final_cat']=='target'])})", 
            f"ğŸ”´ è¶…çº² ({len(df[df['final_cat']=='beyond'])})", 
            f"ğŸŸ¢ å·²æŒæ¡ ({len(df[df['final_cat']=='known'])})"
        ])
        
        def render_tab(tab_obj, data_df, label, expand_default=False, df_key=""):
            with tab_obj:
                if not data_df.empty:
                    display_lines = []
                    for _, row in data_df.iterrows():
                        if show_rank:
                            rank_str = str(int(row['rank'])) if row['rank'] != 99999 else "æœªæ”¶å½•"
                            # [ä¿®æ”¹] å±•ç¤ºå¢åŠ äº† Freq (è¯é¢‘)
                            freq_str = f" | Freq: {row['freq']}"
                            display_lines.append(f"{row['word']} [Rank: {rank_str}{freq_str}]")
                        else:
                            display_lines.append(row['word'])
                    
                    with st.expander("ğŸ‘ï¸ æŸ¥çœ‹å•è¯åˆ—è¡¨", expanded=expand_default):
                        st.markdown("<p class='copy-hint'>ğŸ‘† é¼ æ ‡æ‚¬åœåœ¨ä¸‹æ–¹æ¡†å†…ï¼Œç‚¹å‡»å³ä¸Šè§’ ğŸ“‹ å›¾æ ‡ä¸€é”®å¤åˆ¶å•è¯</p>", unsafe_allow_html=True)
                        st.code("\n".join(display_lines), language='text')
                    
                    st.divider()
                    
                    st.markdown("#### âš™ï¸ å®šåˆ¶å¡ç‰‡å†…å®¹")
                    ui_col1, ui_col2 = st.columns(2)
                    
                    with ui_col1:
                        st.markdown("**æ­£é¢é…ç½® (Front)**")
                        export_format = st.radio("è¾“å‡ºæ ¼å¼:", ["TXT", "CSV"], horizontal=True, key=f"fmt_{df_key}", index=0)
                        ui_front = st.radio("å‘ˆç°å½¢å¼:", ["çŸ­è¯­/æ­é… (Phrase)", "ä»…å•è¯ (Word Only)"], horizontal=True, key=f"front_{df_key}", index=0)
                        ui_pos = st.checkbox("é™„åŠ è¯æ€§æ ‡ç¤º (å¦‚ v, n)", value=True, key=f"pos_{df_key}")
                        ui_poly = st.radio("å¤šä¹‰è¯å¤„ç†:", ["æ‹†åˆ†ä¸ºå¤šå¼ å¡ç‰‡ (åŸç‰ˆé»˜è®¤)", "ä»…ç”Ÿæˆæ ¸å¿ƒé‡Šä¹‰ (1è¯1å¡)"], index=1, horizontal=True, key=f"poly_{df_key}")

                    with ui_col2:
                        st.markdown("**èƒŒé¢é…ç½® (Back)**")
                        ui_def = st.radio("é‡Šä¹‰è¯­è¨€:", ["çº¯è‹±æ–‡ (EN)", "çº¯ä¸­æ–‡ (ZH)", "ä¸­è‹±åŒè¯­ (EN+ZH)"], index=0, horizontal=True, key=f"def_{df_key}")
                        ui_ex = st.slider("ä¾‹å¥æ•°é‡:", 0, 5, 1, key=f"ex_{df_key}")
                        ui_ety = st.checkbox("åŒ…å«ã€è¯æ ¹è¯ç¼€/è¯æºã€‘", value=True, key=f"ety_{df_key}")

                    front_style_val = "phrase" if "çŸ­è¯­" in ui_front else "word"
                    def_lang_val = "en" if "çº¯è‹±æ–‡" in ui_def else "zh" if "çº¯ä¸­æ–‡" in ui_def else "en_zh"
                    split_poly_val = True if "æ‹†åˆ†" in ui_poly else False
                    
                    custom_prompt_text = get_dynamic_prompt_template(
                        export_format=export_format,
                        front_style=front_style_val,
                        add_pos=ui_pos,
                        def_lang=def_lang_val,
                        ex_count=ui_ex,
                        add_ety=ui_ety,
                        split_polysemy=split_poly_val
                    )
                    
                    words_to_process = data_df['raw'].tolist()

                    ai_tab1, ai_tab2 = st.tabs(["ğŸ¤– æ¨¡å¼ 1ï¼šå†…ç½® AI å¹¶å‘æé€Ÿç›´å‡º", "ğŸ“‹ æ¨¡å¼ 2ï¼šå¤åˆ¶ Prompt ç»™ç¬¬ä¸‰æ–¹ AI"])
                    
                    with ai_tab1:
                        st.info("ğŸ’¡ ç«™é•¿å·²ä¸ºæ‚¨å†…ç½®ä¸“å± AI ç®—åŠ›ã€‚é‡‡ç”¨ **å¤šæ ¸å¹¶å‘æŠ€æœ¯**ï¼Œæé€Ÿå“åº”ï¼Œå‘Šåˆ«å¡æ­»ï¼")
                        
                        custom_prompt = st.text_area(
                            "ğŸ“ æœ€ç»ˆ AI Prompt (ç³»ç»Ÿå·²æ ¹æ®æ‚¨çš„è®¾ç½®åŠ¨æ€ç”Ÿæˆï¼Œæ”¯æŒæ‰‹åŠ¨å¾®è°ƒ)", 
                            value=custom_prompt_text, 
                            height=380, 
                            key=f"prompt_{df_key}_{export_format}"
                        )
                        
                        st.caption("âš ï¸ **å…è´£å£°æ˜**ï¼šAI ç”Ÿæˆçš„å†…å®¹ï¼ˆé‡Šä¹‰ã€ä¾‹å¥ç­‰ï¼‰å¯èƒ½å­˜åœ¨å¶å‘çš„ä¸å‡†ç¡®æˆ–å¹»è§‰ï¼Œè¯·ç»“åˆå®é™…è¯­å¢ƒä½¿ç”¨ï¼Œå»ºè®®å¯¼å…¥å‰ç¨ä½œå¤æ ¸ã€‚")
                        
                        if st.button("âš¡ å¬å”¤ DeepSeek æé€Ÿç”Ÿæˆå¡ç‰‡", key=f"btn_{df_key}", type="primary"):
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            status_text.markdown("**âš¡ æ­£åœ¨è¿æ¥ DeepSeek äº‘ç«¯ç®—åŠ›é›†ç¾¤...**") 
                            
                            ai_start_time = time.time()
                            ai_result = call_deepseek_api_chunked(custom_prompt, words_to_process, progress_bar, status_text)
                            ai_duration = time.time() - ai_start_time
                            
                            if "âŒ" in ai_result and len(ai_result) < 100:
                                st.error(ai_result)
                            else:
                                status_text.markdown(f"### ğŸ‰ ç¼–çº‚å…¨éƒ¨å®Œæˆï¼(æ€»è€—æ—¶: **{ai_duration:.2f}** ç§’)")
                                
                                mime_type = "text/csv" if export_format == "CSV" else "text/plain"
                                st.download_button(
                                    label=f"ğŸ“¥ ä¸€é”®ä¸‹è½½æ ‡å‡† Anki å¯¼å…¥æ–‡ä»¶ (.{export_format.lower()})", 
                                    data=ai_result.encode('utf-8-sig'), 
                                    file_name=f"anki_cards_{label}.{export_format.lower()}", 
                                    mime=mime_type,
                                    type="primary",
                                    use_container_width=True
                                )
                                
                                st.markdown("##### ğŸ“ é¢„è§ˆæ¡†")
                                st.code(ai_result, language="text")
                    
                    with ai_tab2:
                        st.info("ğŸ’¡ å¦‚æœæ‚¨æƒ³ä½¿ç”¨ ChatGPT/Claude ç­‰è‡ªå·±çš„ AI å·¥å…·ï¼Œè¯·ç‚¹å‡»å³ä¸Šè§’ä¸€é”®å¤åˆ¶ä¸‹æ–¹å®Œæ•´æŒ‡ä»¤ï¼š")
                        full_prompt_to_copy = f"{custom_prompt_text}\n\nå¾…å¤„ç†å•è¯ï¼š\n{', '.join(words_to_process)}"
                        st.markdown("<p class='copy-hint'>ğŸ‘† é¼ æ ‡æ‚¬åœåœ¨ä¸‹æ–¹æ¡†å†…ï¼Œç‚¹å‡»å³ä¸Šè§’ ğŸ“‹ å›¾æ ‡ä¸€é”®å¤åˆ¶</p>", unsafe_allow_html=True)
                        st.code(full_prompt_to_copy, language='markdown')
                else: st.info("è¯¥åŒºé—´æš‚æ— å•è¯")

        render_tab(t_top, top_df, "Topç²¾é€‰", expand_default=True, df_key="top") 
        render_tab(t_target, df[df['final_cat']=='target'], "é‡ç‚¹", expand_default=False, df_key="target")
        render_tab(t_beyond, df[df['final_cat']=='beyond'], "è¶…çº²", expand_default=False, df_key="beyond")
        render_tab(t_known, df[df['final_cat']=='known'], "ç†Ÿè¯", expand_default=False, df_key="known")