import streamlit as st
import pandas as pd
import re
import os
import time
from datetime import datetime, timedelta, timezone
import lemminflect
import nltk
import genanki
import random
import tempfile
from bs4 import BeautifulSoup

# --- æ–‡ä»¶å¤„ç†åº“ ---
import pypdf
import docx
import ebooklib
from ebooklib import epub

# ==========================================
# 0. é¡µé¢åŸºç¡€é…ç½® & æ ·å¼
# ==========================================
st.set_page_config(
    page_title="Vocab Flow Ultra", 
    page_icon="âš¡ï¸", 
    layout="centered",
    initial_sidebar_state="collapsed" 
)

st.markdown("""
<style>
    .stTextArea textarea { font-family: 'Consolas', monospace; font-size: 14px; }
    .stButton>button { border-radius: 8px; font-weight: 600; width: 100%; margin-top: 5px; }
    .stat-box { padding: 15px; background-color: #f0fdf4; border: 1px solid #bbf7d0; border-radius: 8px; text-align: center; color: #166534; margin-bottom: 20px; }
    .or-divider { text-align: center; margin: 10px 0; color: #888; font-size: 0.9em; font-weight: bold; }
    [data-testid='stFileUploader'] { padding-top: 10px; }
    /* é’ˆå¯¹ Anki é¢„è§ˆçš„ç®€å•æ ·å¼ */
    .anki-preview { border: 1px dashed #ccc; padding: 10px; border-radius: 5px; background: #fafafa; margin-bottom: 5px; font-size: 0.9em; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºåŠ è½½ & å·¥å…·å‡½æ•°
# ==========================================
@st.cache_resource
def setup_nltk():
    try:
        root_dir = os.path.dirname(os.path.abspath(__file__))
        nltk_data_dir = os.path.join(root_dir, 'nltk_data')
        os.makedirs(nltk_data_dir, exist_ok=True)
        nltk.data.path.append(nltk_data_dir)
        for pkg in ['averaged_perceptron_tagger', 'punkt', 'punkt_tab']:
            try: nltk.data.find(f'tokenizers/{pkg}')
            except LookupError: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
    except: pass
setup_nltk()

@st.cache_data
def load_vocab_data():
    """åŠ è½½è¯é¢‘è¡¨"""
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            df.columns = [c.strip().lower() for c in df.columns]
            w_col = next((c for c in df.columns if 'word' in c), df.columns[0])
            r_col = next((c for c in df.columns if 'rank' in c), df.columns[1])
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            # æ’åºå¹¶å»é‡ï¼Œä¿ç•™æ’åé å‰çš„
            df = df.sort_values(r_col).drop_duplicates(subset=[w_col], keep='first')
            return pd.Series(df[r_col].values, index=df[w_col]).to_dict(), df
        except: return {}, None
    return {}, None

VOCAB_DICT, FULL_DF = load_vocab_data()

def get_lemma(word):
    try: return lemminflect.getLemma(word, upos='VERB')[0]
    except: return word

def clear_all_state():
    """ä¸€é”®æ¸…ç©ºçš„å›è°ƒå‡½æ•°"""
    st.session_state.clear()

def get_beijing_time_str():
    """è·å–åŒ—äº¬æ—¶é—´å­—ç¬¦ä¸² (UTC+8)"""
    utc_now = datetime.now(timezone.utc)
    beijing_now = utc_now + timedelta(hours=8)
    return beijing_now.strftime('%m%d_%H%M')

# ==========================================
# 2. æ–‡æœ¬æå–ä¸åˆ†æ
# ==========================================
def extract_text_from_file(uploaded_file):
    """å¤šæ ¼å¼æ–‡ä»¶è§£æ"""
    text = ""
    file_type = uploaded_file.name.split('.')[-1].lower()
    try:
        if file_type == 'txt':
            text = uploaded_file.getvalue().decode("utf-8", errors='ignore')
        elif file_type == 'pdf':
            reader = pypdf.PdfReader(uploaded_file)
            text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif file_type == 'docx':
            doc = docx.Document(uploaded_file)
            text = "\n".join([p.text for p in doc.paragraphs])
        elif file_type == 'epub':
            with tempfile.NamedTemporaryFile(delete=False, suffix='.epub') as tmp:
                tmp.write(uploaded_file.getvalue())
                tmp_path = tmp.name
            book = epub.read_epub(tmp_path)
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    text += soup.get_text() + " "
            os.remove(tmp_path)
    except Exception as e:
        return f"Error: {e}"
    return text

def analyze_logic(text, current_lvl, target_lvl):
    """æ ¸å¿ƒåˆ†æé€»è¾‘"""
    raw_tokens = re.findall(r"[a-z]+", text.lower())
    total_words = len(raw_tokens)
    unique_tokens = set(raw_tokens)
    
    target_words = []
    for w in unique_tokens:
        if len(w) < 3: continue 
        lemma = get_lemma(w)
        rank = VOCAB_DICT.get(lemma, 99999)
        
        # ç­›é€‰: Current < Rank <= Target
        if rank > current_lvl and rank <= target_lvl:
            target_words.append((lemma, rank))
            
    target_words.sort(key=lambda x: x[1])
    final_list = [x[0] for x in target_words]
    return final_list, total_words

# ==========================================
# 3. Anki ç”Ÿæˆé€»è¾‘ (ä¿®å¤è¯æºæ˜¾ç¤º)
# ==========================================
def generate_anki_package(cards_data, deck_name):
    # CSS æ ·å¼å¢å¼ºï¼šç¡®ä¿è¯æºé†’ç›®
    CSS = """
    .card { font-family: arial; font-size: 20px; text-align: center; color: #333; background-color: white; padding: 20px; }
    .nightMode .card { background-color: #2f2f31; color: #f5f5f5; }
    .word { font-size: 40px; font-weight: bold; color: #007AFF; margin-bottom: 10px; }
    .nightMode .word { color: #5FA9FF; }
    .phonetic { color: #888; font-size: 18px; font-family: sans-serif; margin-bottom: 15px; }
    
    .def-container { text-align: left; margin-top: 20px; border-top: 1px solid #ddd; padding-top: 15px; }
    
    .definition { font-weight: bold; color: #222; margin-bottom: 15px; font-size: 22px; }
    .nightMode .definition { color: #eee; }
    
    .examples { background: #f4f4f4; padding: 15px; border-radius: 8px; color: #444; font-style: italic; font-size: 20px; line-height: 1.4; margin-bottom: 15px; }
    .nightMode .examples { background: #383838; color: #ddd; }
    
    /* è¯æºæ ·å¼å¢å¼º */
    .etymology { 
        display: block; 
        font-size: 18px; 
        color: #555; 
        border: 1px dashed #bbb; 
        padding: 8px 12px; 
        border-radius: 6px;
        background-color: #fffaf0;
        margin-top: 10px;
    }
    .nightMode .etymology { 
        color: #aaa; 
        border-color: #555;
        background-color: #333;
    }
    """
    
    # éšæœºç”Ÿæˆ Model IDï¼Œé˜²æ­¢ä¸åŒç‰Œç»„å†²çª
    model_id = random.randrange(1 << 30, 1 << 31)
    
    model = genanki.Model(
        model_id, 
        f'VocabFlow Model {model_id}',
        fields=[
            {'name': 'Word'}, 
            {'name': 'IPA'}, 
            {'name': 'Meaning'}, 
            {'name': 'Examples'}, 
            {'name': 'Etymology'}
        ],
        templates=[{
            'name': 'Card 1',
            'qfmt': '<div class="word">{{Word}}</div><div class="phonetic">{{IPA}}</div>',
            'afmt': '''
            {{FrontSide}}
            <div class="def-container">
                <div class="definition">{{Meaning}}</div>
                <div class="examples">{{Examples}}</div>
                <div class="etymology">ğŸŒ± <b>Etymology:</b> {{Etymology}}</div>
            </div>
            ''',
        }], css=CSS
    )
    
    deck = genanki.Deck(random.randrange(1 << 30, 1 << 31), deck_name)
    
    for c in cards_data:
        deck.add_note(genanki.Note(
            model=model, 
            fields=[
                str(c.get('word','')), 
                str(c.get('ipa','')), 
                str(c.get('meaning','')), 
                str(c.get('examples','')).replace('\n','<br>'), 
                str(c.get('etymology','')) # ç¡®ä¿è¿™é‡Œå–åˆ°äº†å€¼
            ]
        ))
        
    with tempfile.NamedTemporaryFile(delete=False, suffix='.apkg') as tmp:
        genanki.Package(deck).write_to_file(tmp.name)
        return tmp.name

def get_ai_prompt(words):
    """Prompt ä¼˜åŒ–"""
    w_list = ", ".join(words)
    return f"""
Act as a Dictionary API. Convert the following words into strictly formatted data.

**Words:** {w_list}

**CRITICAL FORMATTING RULES (Must Follow):**
1. **Format:** `Word | IPA | Definition | Examples | Etymology`
2. **NO Markdown Tables:** Do NOT use tables. Do NOT use `|` at the start or end of lines.
3. **Separator:** Use `|` ONLY to separate fields.
4. **Content:**
   - Definition: Concise (<12 words).
   - Examples: 2 sentences separated by `<br>`.
   - **Etymology:** REQUIRED. Provide root/suffix analysis (e.g., "bene(good)+vol(wish)"). If unknown, state origin (e.g., "From Old French...").

**Example of CORRECT Output:**
benevolent | /bÉ™ËˆnevÉ™lÉ™nt/ | kind and helpful | He is **benevolent**.<br>A **benevolent** fund. | bene(good) + vol(wish)

**Begin Output:**
"""

# ==========================================
# 4. ä¸»ç¨‹åº
# ==========================================
st.title("âš¡ï¸ Vocab Flow Ultra")

if not VOCAB_DICT:
    st.error("âš ï¸ ç¼ºå¤± `coca_cleaned.csv`")

# Input Tabs
tab_extract, tab_anki = st.tabs(["1ï¸âƒ£ å†…å®¹æå– & ç”Ÿæˆ", "2ï¸âƒ£ æ‰“åŒ… Anki"])

# ------------------------------------------
# TAB 1: æå–é€»è¾‘
# ------------------------------------------
with tab_extract:
    mode_context, mode_rank = st.tabs(["ğŸ“„ è¯­å¢ƒåˆ†æ (æ–‡æœ¬/æ–‡ä»¶)", "ğŸ”¢ è¯é¢‘åˆ—è¡¨ (Rank & Random)"])
    
    # --- A. è¯­å¢ƒåˆ†æ ---
    with mode_context:
        st.markdown("#### 1. è®¾å®šè¯æ±‡åˆ†çº§")
        c1, c2 = st.columns(2)
        curr = c1.number_input("å¿½ç•¥å¤ªç®€å•çš„ (Current Level)", 1000, 20000, 4000, step=500)
        targ = c2.number_input("å¿½ç•¥å¤ªéš¾çš„ (Target Level)", 2000, 50000, 15000, step=500)
        
        st.markdown("#### 2. è¾“å…¥å†…å®¹")
        uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£ (PDF/TXT/DOCX/EPUB)", type=['txt','pdf','docx','epub'])
        st.markdown('<div class="or-divider">- OR -</div>', unsafe_allow_html=True)
        pasted_text = st.text_area("ğŸ“„ ...æˆ–åœ¨æ­¤ç›´æ¥ç²˜è´´æ–‡æœ¬", height=150)
        
        if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
            raw_text = ""
            if uploaded_file:
                with st.spinner(f"æ­£åœ¨è¯»å– {uploaded_file.name}..."):
                    raw_text = extract_text_from_file(uploaded_file)
            elif pasted_text.strip():
                raw_text = pasted_text
                
            if raw_text and len(raw_text) > 10:
                final_words, total = analyze_logic(raw_text, curr, targ)
                st.session_state['gen_words'] = final_words
                st.session_state['total_count'] = total
            else:
                st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆå†…å®¹")

        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ® (Reset)", type="secondary", on_click=clear_all_state):
            pass

    # --- B. çº¯è¯é¢‘åˆ—è¡¨ (æ–°å¢éšæœºåŠŸèƒ½) ---
    with mode_rank:
        st.info("ä» COCA è¯é¢‘è¡¨ä¸­ç”Ÿæˆå•è¯åˆ—è¡¨ã€‚")
        
        # ä¸¤ä¸ªæ¨¡å¼ï¼šé¡ºåº vs éšæœº
        gen_type = st.radio("ç”Ÿæˆæ¨¡å¼", ["ğŸ”¢ é¡ºåºæˆªå– (ä¾‹å¦‚: 8000ååçš„50ä¸ª)", "ğŸ”€ èŒƒå›´éšæœº (ä¾‹å¦‚: 6000-8000åä¸­éšæœºå–50ä¸ª)"])
        
        if "é¡ºåº" in gen_type:
            c_a, c_b = st.columns(2)
            s_rank = c_a.number_input("èµ·å§‹æ’å (Start Rank)", 1, 20000, 8000, step=100)
            count = c_b.number_input("æ•°é‡ (Count)", 10, 500, 50, step=10)
            
            if st.button("ğŸš€ ç”Ÿæˆé¡ºåºåˆ—è¡¨", type="primary"):
                if FULL_DF is not None:
                    r_col = next(c for c in FULL_DF.columns if 'rank' in c)
                    w_col = next(c for c in FULL_DF.columns if 'word' in c)
                    subset = FULL_DF[FULL_DF[r_col] >= s_rank].sort_values(r_col).head(count)
                    st.session_state['gen_words'] = subset[w_col].tolist()
                    st.session_state['total_count'] = count
        else:
            # éšæœºæ¨¡å¼é€»è¾‘
            c_min, c_max, c_cnt = st.columns([1,1,1])
            min_r = c_min.number_input("æœ€å°æ’å (Min)", 1, 20000, 6000, step=500)
            max_r = c_max.number_input("æœ€å¤§æ’å (Max)", 1, 25000, 8000, step=500)
            r_count = c_cnt.number_input("éšæœºæ•°é‡ (Qty)", 10, 200, 50, step=10)
            
            if st.button("ğŸ² éšæœºæŠ½å–", type="primary"):
                if FULL_DF is not None:
                    try:
                        r_col = next(c for c in FULL_DF.columns if 'rank' in c)
                        w_col = next(c for c in FULL_DF.columns if 'word' in c)
                        
                        # ç­›é€‰èŒƒå›´
                        mask = (FULL_DF[r_col] >= min_r) & (FULL_DF[r_col] <= max_r)
                        candidates = FULL_DF[mask]
                        
                        avail_count = len(candidates)
                        if avail_count == 0:
                            st.error(f"âš ï¸ è¯¥èŒƒå›´å†… (Rank {min_r}-{max_r}) æ²¡æœ‰æ‰¾åˆ°å•è¯ã€‚")
                        else:
                            # éšæœºæŠ½æ ·
                            real_count = min(r_count, avail_count)
                            subset = candidates.sample(n=real_count)
                            # æŒ‰Rankæ’åºä¸€ä¸‹ï¼Œæ–¹ä¾¿æŸ¥çœ‹
                            subset = subset.sort_values(r_col)
                            
                            st.session_state['gen_words'] = subset[w_col].tolist()
                            st.session_state['total_count'] = real_count
                            st.success(f"æˆåŠŸä» {avail_count} ä¸ªå€™é€‰è¯ä¸­éšæœºæŠ½å–äº† {real_count} ä¸ªï¼")
                    except Exception as e:
                        st.error(f"ç”Ÿæˆå‡ºé”™: {e}")

        if st.button("ğŸ—‘ï¸ æ¸…ç©º (Reset)", type="secondary", key="reset_rank", on_click=clear_all_state):
            pass

    # --- ç»“æœå±•ç¤º ---
    if 'gen_words' in st.session_state:
        words = st.session_state['gen_words']
        st.divider()
        st.markdown(f"""
        <div class="stat-box">
            ğŸ“Š æ¥æºæ€»è¯æ•°: <b>{st.session_state.get('total_count', 0)}</b> | 
            ğŸ¯ ç­›é€‰åç”Ÿè¯: <b>{len(words)}</b> ä¸ª
        </div>
        """, unsafe_allow_html=True)

        if len(words) > 0:
            with st.expander("ğŸ‘ï¸ é¢„è§ˆå•è¯åˆ—è¡¨", expanded=False):
                st.write(", ".join(words))

            st.markdown("### ğŸ¤– è·å– AI Prompt")
            c_batch, c_info = st.columns([1, 2])
            batch_size = c_batch.number_input("æ¯ç»„å•è¯æ•°", 10, 200, 50, step=10)
            c_info.caption(f"ğŸ’¡ åˆ†ç»„å»ºè®®ï¼šæ¯æ¬¡å¤åˆ¶ä¸€ç»„ç»™AIï¼Œé˜²æ­¢ç”Ÿæˆä¸­æ–­ã€‚")
            
            batches = [words[i:i + batch_size] for i in range(0, len(words), batch_size)]
            
            for idx, batch in enumerate(batches):
                with st.expander(f"ç¬¬ {idx+1} ç»„ (å•è¯ {idx*batch_size+1} - {idx*batch_size+len(batch)})", expanded=(idx==0)):
                    prompt = get_ai_prompt(batch)
                    st.code(prompt, language="markdown")
                    st.caption("ğŸ‘† ç‚¹å‡»å³ä¸Šè§’å¤åˆ¶ -> å‘ç»™ AI -> å¤åˆ¶å›å¤ -> ç²˜è´´åˆ° 'æ‰“åŒ… Anki' é¡µé¢")

# ------------------------------------------
# TAB 2: æ‰“åŒ… Anki
# ------------------------------------------
with tab_anki:
    st.markdown("### ğŸ“¦ åˆ¶ä½œ Anki ç‰Œç»„")
    
    bj_time_str = get_beijing_time_str()
    default_name = f"Vocab_{bj_time_str}"
    
    if 'anki_input_text' not in st.session_state:
        st.session_state['anki_input_text'] = ""

    ai_resp = st.text_area(
        "åœ¨æ­¤ç²˜è´´ AI çš„å›å¤å†…å®¹ (ä¸‹è½½åä¸ä¼šæ¶ˆå¤±ï¼Œå¯ç»§ç»­æ·»åŠ )", 
        height=300, 
        placeholder="word1 | /ipa/ | meaning... \nword2 | ...",
        key="anki_input_text"
    )
    
    deck_name = st.text_input("ç‰Œç»„åç§° (å·²è‡ªåŠ¨è®¾ä¸ºåŒ—äº¬æ—¶é—´)", default_name)
    
    # è§£æé€»è¾‘ (å¢å¼ºå®¹é”™ç‡ï¼Œç¡®ä¿è¯æºè¢«æ•è·)
    cards = []
    skipped = 0
    if ai_resp.strip():
        for line in ai_resp.strip().split('\n'):
            line = line.strip()
            if not line: continue
            
            # ä¸¥æ ¼è¿‡æ»¤æ— æ•ˆè¡Œ
            if line.startswith("|") or line.endswith("|") or "---" in line: continue
            if "Word" in line and "IPA" in line: continue
            
            if "|" not in line: 
                skipped += 1
                continue
            
            # åˆ†å‰²å¹¶è‡ªåŠ¨è¡¥å…¨ç¼ºå¤±çš„åˆ—
            parts = [p.strip() for p in line.split('|')]
            
            # è¡¥å…¨é€»è¾‘ï¼šå¦‚æœä¸å¤Ÿ5åˆ—ï¼Œè‡ªåŠ¨è¡¥ç©ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢æŠ¥é”™
            while len(parts) < 5:
                parts.append("")
                
            if len(parts) >= 3: # è‡³å°‘è¦æœ‰ å•è¯|éŸ³æ ‡|é‡Šä¹‰
                cards.append({
                    'word': parts[0],
                    'ipa': parts[1],
                    'meaning': parts[2],
                    'examples': parts[3],
                    'etymology': parts[4] # è¿™é‡Œç°åœ¨ä¸€å®šå®‰å…¨
                })
            else:
                skipped += 1

    # æ˜¾ç¤ºçŠ¶æ€ä¸ä¸‹è½½
    if cards:
        st.success(f"âœ… å·²è¯†åˆ« {len(cards)} å¼ å¡ç‰‡")
        
        # ç®€å•çš„é¢„è§ˆï¼Œè®©ç”¨æˆ·ç¡®è®¤è¯æºæ˜¯å¦æå–åˆ°äº†
        with st.expander("ğŸ” æ£€æŸ¥è§£æç»“æœ (å‰3æ¡)"):
            for c in cards[:3]:
                st.markdown(f"**{c['word']}**: {c['etymology'] if c['etymology'] else 'âŒ æœªæ£€æµ‹åˆ°è¯æº'}")
        
        if skipped > 0:
            st.caption(f"âš ï¸ è¿‡æ»¤äº† {skipped} è¡Œæ— æ•ˆæ•°æ®")
            
        final_filename = f"{deck_name}.apkg"
        f_path = generate_anki_package(cards, deck_name)
        
        with open(f_path, "rb") as f:
            st.download_button(
                label=f"ğŸ“¥ ä¸‹è½½ {final_filename}",
                data=f,
                file_name=final_filename,
                mime="application/octet-stream",
                type="primary"
            )
    elif ai_resp.strip():
        st.warning("âš ï¸ ç²˜è´´å†…å®¹ä¸­æœªè¯†åˆ«åˆ°æœ‰æ•ˆå¡ç‰‡ï¼Œè¯·æ£€æŸ¥æ˜¯å¦åŒ…å« '|' åˆ†éš”ç¬¦")