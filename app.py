import streamlit as st
import pandas as pd
import re
import os
import sys
import subprocess

st.set_page_config(page_title="Vibe Vocab Studio", page_icon="ğŸ§ ", layout="wide")

# ==========================================
# 0. æ ¸å¿ƒï¼šè‡ªåŠ¨ç¯å¢ƒä¿®å¤ (Self-Healing)
# ==========================================
@st.cache_resource
def load_spacy_model():
    """
    é¡½å¼ºçš„åŠ è½½å™¨ï¼š
    1. æ£€æŸ¥æœ‰æ²¡æœ‰ spacyï¼Œæ²¡æœ‰å°±æŠ¥é”™
    2. æ£€æŸ¥æœ‰æ²¡æœ‰æ¨¡å‹ en_core_web_smï¼Œæ²¡æœ‰å°±ç°åœºä¸‹è½½
    """
    try:
        import spacy
    except ImportError:
        st.error("âŒ ä¸¥é‡é”™è¯¯ï¼šrequirements.txt æœªç”Ÿæ•ˆï¼Œæ‰¾ä¸åˆ° spacy åº“ã€‚è¯·å°è¯• Reboot Appã€‚")
        st.stop()

    model_name = "en_core_web_sm"
    try:
        # å°è¯•ç›´æ¥åŠ è½½
        nlp = spacy.load(model_name)
    except OSError:
        # å¦‚æœæŠ¥é”™è¯´æ‰¾ä¸åˆ°æ¨¡å‹ï¼Œå°±è°ƒç”¨å‘½ä»¤è¡Œä¸‹è½½
        st.warning(f"æ­£åœ¨è‡ªåŠ¨ä¸‹è½½è¯­è¨€æ¨¡å‹ {model_name}... (åˆæ¬¡è¿è¡Œéœ€è¦ 1 åˆ†é’Ÿ)")
        try:
            # ä½¿ç”¨ subprocess è°ƒç”¨å®‰è£…å‘½ä»¤
            subprocess.check_call([sys.executable, "-m", "spacy", "download", model_name])
            nlp = spacy.load(model_name)
        except Exception as e:
            st.error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ—¥å¿—: {e}")
            st.stop()
            
    return nlp

# åŠ è½½ NLP å¼•æ“
nlp = load_spacy_model()
st.sidebar.success("âœ… spaCy å¼•æ“å·²å°±ç»ª")

# ==========================================
# 1. è¯åº“åŠ è½½ (é’ˆå¯¹ coca_cleaned.csv ä¼˜åŒ–)
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv", "COCA20000è¯Excelç‰ˆ.xlsx - Sheet1.csv"]

@st.cache_data
def load_vocab():
    file_path = None
    for f in POSSIBLE_FILES:
        if os.path.exists(f):
            file_path = f
            break
            
    if not file_path: return None, "æœªæ‰¾åˆ°æ–‡ä»¶"

    # ä¼˜å…ˆè¯» coca_cleaned (æ ‡å‡†æ ¼å¼)
    if 'cleaned' in file_path:
        try:
            # ä½ çš„ coca_cleaned.csv æ˜¯æ ‡å‡†çš„ word, rank æ ¼å¼
            df = pd.read_csv(file_path)
            # å¼ºåˆ¶å°å†™å»ç©ºæ ¼
            df['word'] = df['word'].astype(str).str.lower().str.strip()
            # å»ºç«‹ç´¢å¼•
            vocab = pd.Series(df['rank'].values, index=df['word']).to_dict()
            return vocab, "åŠ è½½æˆåŠŸ (Cleaned)"
        except: pass

    # å…œåº•è¯»åŸå§‹æ–‡ä»¶
    for enc in ['utf-8', 'utf-8-sig', 'gbk']:
        try:
            df = pd.read_csv(file_path, encoding=enc)
            cols = [str(c).lower() for c in df.columns]
            df.columns = cols
            
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c or 'è¯é¢‘' in c), cols[1] if len(cols)>1 else cols[0])
            
            df['w'] = df[w_col].astype(str).str.lower().str.strip()
            df['r'] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
            
            vocab = pd.Series(df['r'].values, index=df['w']).to_dict()
            return vocab, "åŠ è½½æˆåŠŸ (Raw)"
        except: continue
        
    return None, "åŠ è½½å¤±è´¥"

vocab_dict, msg = load_vocab()

# ==========================================
# 2. ç•Œé¢æ˜¾ç¤º
# ==========================================
st.title("ğŸ§  Vibe Vocab v11.0 (è‡ªåŠ¨ä¿®å¤ç‰ˆ)")
st.caption("spaCy é©±åŠ¨ Â· è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ Â· å½»åº•è§£å†³è¿˜åŸé—®é¢˜")

if not vocab_dict:
    st.error(msg)
    st.stop()

st.sidebar.info(f"ğŸ“š è¯åº“: {msg}")

# === éªŒè¯åŒº ===
st.sidebar.markdown("---")
with st.sidebar.expander("ğŸ” è¿˜åŸ & æ’åæµ‹è¯•", expanded=True):
    # æµ‹è¯•è¿˜åŸ
    doc = nlp("families are better")
    res = [token.lemma_ for token in doc]
    st.write(f"families are better -> {res}")
    
    # æµ‹è¯•æ’å
    check_be = vocab_dict.get('be', 'Not Found')
    st.write(f"'be' rank: {check_be}")
    
    if check_be == 'Not Found' or check_be > 100:
        st.error("âš ï¸ è¯åº“è¯»å–å¯èƒ½æœ‰è¯¯ï¼Œ'be' çš„æ’åä¸å¯¹ï¼")
    else:
        st.success("âœ… è¯åº“è¯»å–æ­£å¸¸")

vocab_range = st.sidebar.slider("å­¦ä¹ åŒºé—´", 1, 20000, (6000, 8000), 500)
r_start, r_end = vocab_range

# ==========================================
# 3. æ ¸å¿ƒå¤„ç†é€»è¾‘ (spaCy)
# ==========================================
def process_text(text):
    # ä½¿ç”¨ spaCy å¤„ç†æ•´ä¸ªæ–‡æœ¬
    doc = nlp(text.lower())
    
    seen_lemmas = set()
    unique_items = []
    
    for token in doc:
        # åªä¿ç•™å­—æ¯ï¼Œä¸”é•¿åº¦å¤§äº1
        if token.is_alpha and len(token.text) > 1:
            lemma = token.lemma_.lower() # è¿™é‡Œæ‹¿åˆ°çš„å°±æ˜¯ family, be, go
            original = token.text.lower()
            
            if lemma not in seen_lemmas:
                unique_items.append((original, lemma))
                seen_lemmas.add(lemma)
    
    # æŒ‰æ’åæ’åºé€»è¾‘
    # æˆ‘ä»¬å…ˆæŸ¥ rankï¼Œå†æ’åº
    processed_list = []
    
    for original, lemma in unique_items:
        rank = 99999
        match = lemma # é»˜è®¤ç”¨è¿˜åŸåçš„è¯(family)å»æŸ¥
        note = ""

        # 1. æŸ¥è¿˜åŸåçš„è¯ (family)
        if lemma in vocab_dict:
            rank = vocab_dict[lemma]
            if original != lemma:
                note = f"<{original}>" # å¤‡æ³¨ï¼šåŸè¯æ˜¯ families
        
        # 2. å¦‚æœè¿˜åŸåçš„è¯æ²¡æŸ¥åˆ°ï¼Œæˆ–è€…æ˜¯ç”Ÿè¯(rank>20000)ï¼Œå†è¯•è¯•åŸè¯(families)
        # (é˜²æ­¢ spaCy è¿˜åŸé”™è¯¯ï¼Œæˆ–è€…è¯åº“é‡Œåªæ”¶å½•äº†å˜å½¢ä½“)
        elif original in vocab_dict:
            r_orig = vocab_dict[original]
            if r_orig < rank:
                rank = r_orig
                match = original
                note = ""

        processed_list.append({'å•è¯': match, 'æ’å': int(rank), 'å¤‡æ³¨': note})

    # è½¬ DataFrame
    df_all = pd.DataFrame(processed_list)
    if df_all.empty:
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    # åˆ†ç±»
    known = df_all[df_all['æ’å'] <= r_start]
    target = df_all[(df_all['æ’å'] > r_start) & (df_all['æ’å'] <= r_end)]
    beyond = df_all[df_all['æ’å'] > r_end]
    
    return known, target, beyond

# ==========================================
# 4. ä¸»ç•Œé¢
# ==========================================
text_input = st.text_area("åœ¨æ­¤ç²˜è´´æ–‡æœ¬:", height=150)

if st.button("ğŸš€ å¼€å§‹åˆ†æ (spaCy Powered)", type="primary"):
    if not text_input.strip():
        st.warning("è¯·è¾“å…¥å†…å®¹")
    else:
        with st.spinner("spaCy æ­£åœ¨åŠ è½½æ¨¡å‹å¹¶åˆ†æ..."):
            df_k, df_t, df_b = process_text(text_input)
        
        st.success("åˆ†æå®Œæˆ")
        t1, t2, t3 = st.tabs([
            f"ğŸŸ¡ é‡ç‚¹ ({len(df_t)})", 
            f"ğŸ”´ è¶…çº² ({len(df_b)})", 
            f"ğŸŸ¢ ç†Ÿè¯ ({len(df_k)})"
        ])
        
        with t1: st.dataframe(df_t, use_container_width=True)
        with t2: st.dataframe(df_b, use_container_width=True)
        with t3: st.dataframe(df_k, use_container_width=True)