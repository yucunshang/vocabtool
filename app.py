import streamlit as st
import pandas as pd
import re
import os
import json
import time
import requests
import zipfile
import concurrent.futures

# æ–‡æœ¬å¤„ç†åº“
import lemminflect
import nltk

# ==========================================
# 0. å…¨å±€å¸¸é‡ä¸é…ç½®
# ==========================================
PAGE_CONFIG = {"layout": "wide", "page_title": "Vocab Master Pro", "page_icon": "ğŸš€"}
MAX_WORKERS = 5
CHUNK_SIZE = 30
MAX_WORDS_LIMIT = 300
DEEPSEEK_API_URL = "https://api.deepseek.com/chat/completions"

CUSTOM_CSS = """
<style>
    .stCode { font-family: 'Consolas', 'Courier New', monospace !important; font-size: 16px !important; }
    header {visibility: hidden;} footer {visibility: hidden;}
    .block-container { padding-top: 1rem; }
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    [data-testid="stMetricValue"] { font-size: 28px !important; color: var(--primary-color) !important; }
    .param-box { background-color: var(--secondary-background-color); padding: 15px 20px 5px 20px; border-radius: 10px; border: 1px solid var(--border-color-light); margin-bottom: 20px; }
    .copy-hint { color: #888; font-size: 14px; margin-bottom: 5px; margin-top: 10px; padding-left: 5px; }
</style>
"""

# ==========================================
# 1. åŸºç¡€åˆå§‹åŒ–
# ==========================================
st.set_page_config(**PAGE_CONFIG)
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)

try:
    import PyPDF2
    import docx
except ImportError:
    pass

# ==========================================
# 2. æ•°æ®ä¸ NLP åˆå§‹åŒ–
# ==========================================
@st.cache_data(show_spinner=False)
def load_knowledge_base():
    base_path = 'data'
    data = {'terms': {}, 'proper': {}, 'patch': {}, 'ambiguous': set()}
    
    files_map = {
        'terms': ('terms.json', lambda x: {k.lower(): v for k, v in x.items()}),
        'proper': ('proper.json', lambda x: {k.lower(): v for k, v in x.items()}),
        'patch': ('patch.json', lambda x: x),
        'ambiguous': ('ambiguous.json', lambda x: set(x))
    }

    if not os.path.exists(base_path):
        return data['terms'], data['proper'], data['patch'], data['ambiguous']

    for key, (filename, processor) in files_map.items():
        file_path = os.path.join(base_path, filename)
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data[key] = processor(json.load(f))
            except Exception:
                pass
                
    return data['terms'], data['proper'], data['patch'], data['ambiguous']

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

@st.cache_resource(show_spinner="æ­£åœ¨åˆå§‹åŒ– NLP å¼•æ“...")
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    os.makedirs(nltk_data_dir, exist_ok=True)
    nltk.data.path.insert(0, nltk_data_dir)
    
    required_packages = ['averaged_perceptron_tagger', 'punkt', 'punkt_tab']
    for pkg in required_packages:
        try:
            nltk.data.find(f'tokenizers/{pkg}')
        except LookupError:
            try:
                nltk.data.find(f'taggers/{pkg}')
            except LookupError:
                try:
                    nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
                except Exception:
                    pass
setup_nltk()

def get_lemma(w):
    try:
        lemmas_dict = lemminflect.getAllLemmas(w)
        if not lemmas_dict: return w.lower()
        for pos in ['ADJ', 'ADV', 'VERB', 'NOUN']:
            if pos in lemmas_dict: return lemmas_dict[pos][0]
        return list(lemmas_dict.values())[0][0]
    except Exception:
        return w.lower()

@st.cache_data(show_spinner=False)
def load_vocab():
    vocab = {}
    file_path = next((f for f in ["coca_cleaned.csv", "data.csv"] if os.path.exists(f)), None)
    
    if file_path:
        try:
            df = pd.read_csv(file_path)
            df.columns = [str(c).strip().lower() for c in df.columns]
            w_col = next((c for c in df.columns if 'word' in c or 'å•è¯' in c), None)
            r_col = next((c for c in df.columns if 'rank' in c or 'æ’åº' in c), None)
            
            if w_col and r_col:
                df[w_col] = df[w_col].astype(str).str.lower().str.strip()
                df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
                df = df.sort_values(r_col, ascending=True).drop_duplicates(subset=[w_col], keep='first')
                vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except Exception as e:
            st.warning(f"âš ï¸ è¯é¢‘è¡¨åŠ è½½å¤±è´¥: {e}")

    for word, rank in BUILTIN_PATCH_VOCAB.items(): 
        vocab[word] = rank
        
    URGENT_OVERRIDES = {
        "china": 400, "turkey": 1500, "march": 500, "may": 100, "august": 1500, "polish": 2500,
        "monday": 300, "tuesday": 300, "wednesday": 300, "thursday": 300, "friday": 300, "saturday": 300, "sunday": 300,
        "january": 400, "february": 400, "april": 400, "june": 400, "july": 400, "september": 400, "october": 400, "november": 400, "december": 400,
        "usa": 200, "uk": 200, "google": 1000, "apple": 1000, "microsoft": 1500
    }
    vocab.update(URGENT_OVERRIDES)
    return vocab

vocab_dict = load_vocab()

# ==========================================
# 3. æ–‡æ¡£è§£æå¼•æ“
# ==========================================
def extract_text_from_file(uploaded_file):
    ext = uploaded_file.name.split('.')[-1].lower()
    uploaded_file.seek(0)
    text_content = ""
    try:
        if ext == 'txt':
            text_content = uploaded_file.getvalue().decode("utf-8", errors="ignore")
        elif ext == 'pdf':
            if 'PyPDF2' not in globals(): return "âš ï¸ ç¼ºå°‘ PyPDF2 åº“"
            reader = PyPDF2.PdfReader(uploaded_file)
            text_content = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif ext == 'docx':
            if 'docx' not in globals(): return "âš ï¸ ç¼ºå°‘ python-docx åº“"
            doc = docx.Document(uploaded_file)
            text_content = " ".join([p.text for p in doc.paragraphs])
        elif ext == 'epub':
            with zipfile.ZipFile(uploaded_file) as z:
                text_blocks = []
                for filename in z.namelist():
                    if filename.endswith(('.html', '.xhtml', '.htm', '.xml')):
                        try:
                            content = z.read(filename).decode('utf-8', errors='ignore')
                            clean_text = re.sub(r'<[^>]+>', ' ', content)
                            text_blocks.append(clean_text)
                        except: pass
                text_content = " ".join(text_blocks)
    except Exception as e:
        return f"âš ï¸ æ–‡ä»¶è§£æå¤±è´¥: {str(e)}"
    return text_content

# ==========================================
# 4. Prompt æ¨¡æ¿å¼•æ“ (å·²æ›´æ–°ï¼šåŠ¨æ€å¯¹åº”æ ¼å¼)
# ==========================================
def get_base_prompt_template(export_type="CSV"):
    """
    æ ¹æ®é€‰æ‹©çš„ export_type (CSV/TXT) åŠ¨æ€ç”Ÿæˆå¯¹åº”çš„ Prompt
    """
    if export_type == "CSV":
        format_rule = """4. è¾“å‡ºæ ¼å¼æ ‡å‡† (CSV æ ¼å¼)
- åˆ†éš”ç¬¦ï¼šä¸¥æ ¼ä½¿ç”¨è‹±æ–‡é€—å· (,) åˆ†éš”ä¸¤åˆ—ã€‚
- å¼•ç”¨è§„åˆ™ï¼šç”±äºå†…å®¹åŒ…å«é€—å·æˆ–æ¢è¡Œï¼Œ**æ¯ä¸ªå­—æ®µå¿…é¡»ä¸¥æ ¼ä½¿ç”¨åŒå¼•å· ("...") åŒ…è£¹**ã€‚
- ç»“æ„ï¼š "Front_Content","Back_Content" """
        example = """"run a business","to manage a company<br><br><em>He quit to run a business.</em><br><br>ã€è¯æºã€‘æºè‡ª..." """
    else: # TXT (Tab åˆ†éš”)
        format_rule = """4. è¾“å‡ºæ ¼å¼æ ‡å‡† (TXT/Tab æ ¼å¼)
- åˆ†éš”ç¬¦ï¼šä¸¥æ ¼ä½¿ç”¨ **åˆ¶è¡¨ç¬¦ (Tab)** åˆ†éš”ä¸¤åˆ— (ä¸è¦ä½¿ç”¨é€—å·)ã€‚
- å¼•ç”¨è§„åˆ™ï¼šä¸è¦ä½¿ç”¨å¼•å·åŒ…è£¹å­—æ®µï¼Œé™¤éå†…å®¹ä¸­ç¡®å®åŒ…å« Tabã€‚
- ç»“æ„ï¼š Front_Content [TAB] Back_Content """
        # æ³¨æ„ï¼šè¿™é‡Œç”¨ [TAB] è¡¨ç¤ºåˆ¶è¡¨ç¬¦ï¼Œå®é™… Prompt ä¸­éœ€è¦æ˜ç¡®
        example = """run a business	to manage a company<br><br><em>He quit to run a business.</em><br><br>ã€è¯æºã€‘æºè‡ª..."""

    return f"""ã€è§’è‰²è®¾å®šã€‘ ä½ æ˜¯ä¸€ä½ç²¾é€šè¯æºå­¦ã€è®¤çŸ¥å¿ƒç†å­¦ä»¥åŠ Anki ç®—æ³•çš„â€œè‹±è¯­è¯æ±‡ä¸“å®¶â€ã€‚è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹æ ‡å‡†ï¼Œå¤„ç†æˆ‘æä¾›çš„å•è¯åˆ—è¡¨ï¼š

1. æ ¸å¿ƒåŸåˆ™
- å«ä¹‰æ‹†åˆ†ï¼šè‹¥å•è¯æœ‰å¤šä¸ªå¸¸ç”¨ä¹‰é¡¹ï¼Œæ‹†åˆ†ä¸ºå¤šæ¡æ•°æ®ã€‚
- ä¸¥ç¦å †ç Œï¼šæ¯å¼ å¡ç‰‡åªæ‰¿è½½ä¸€ä¸ªç‰¹å®šè¯­å¢ƒä¸‹çš„å«ä¹‰ã€‚

2. å¡ç‰‡æ­£é¢ (Column 1)
- å†…å®¹ï¼šæä¾›è‡ªç„¶çš„çŸ­è¯­æˆ–æ­é… (Phrase/Collocation)ï¼Œè€Œéå•ä¸ªå­¤ç«‹å•è¯ã€‚
- æ ·å¼ï¼šçº¯æ–‡æœ¬ã€‚

3. å¡ç‰‡èƒŒé¢ (Column 2 - æ•´åˆé¡µ)
- èƒŒé¢ä¿¡æ¯å¿…é¡»å…¨éƒ¨åˆå¹¶åœ¨ç¬¬äºŒåˆ—ï¼Œå¹¶ä½¿ç”¨ HTML æ ‡ç­¾æ’ç‰ˆã€‚
- ç»“æ„é¡ºåºï¼šè‹±æ–‡é‡Šä¹‰ <br><br> <em>ä¾‹å¥</em> <br><br> ã€è¯æº/è®°å¿†æ³•ã€‘ä¸­æ–‡è§£æ

{format_rule}

5. æ•°æ®æ¸…æ´—
- è‡ªåŠ¨ä¿®æ­£æ‹¼å†™é”™è¯¯ï¼›å¯¹ç¼©å†™æä¾›å…¨ç§°ã€‚

ğŸ’¡ æœ€ç»ˆè¾“å‡ºç¤ºä¾‹ (ä¸¥æ ¼æ¨¡ä»¿æ­¤æ ¼å¼)ï¼š
{example}

ã€ç³»ç»Ÿç»å¯¹å¼ºåˆ¶æŒ‡ä»¤ã€‘
ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„æ•°æ®ä»£ç ï¼Œä¸è¦åŒ…å« ```csv æˆ– markdown æ ‡è®°ï¼Œä¸è¦å›å¤ä»»ä½•å®¢å¥—è¯ã€‚"""

# ==========================================
# 5. å¤šæ ¸å¹¶å‘ API å¼•æ“
# ==========================================
def _fetch_deepseek_chunk(batch_words, prompt_template, api_key):
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    full_prompt = f"{prompt_template}\n\nå¾…å¤„ç†å•è¯åˆ—è¡¨ï¼š\n{', '.join(batch_words)}"
    
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": full_prompt}],
        "temperature": 0.3,
        "max_tokens": 4096,
        "stream": False
    }
    
    for attempt in range(3):
        try:
            resp = requests.post(DEEPSEEK_API_URL, json=payload, headers=headers, timeout=60)
            if resp.status_code == 429: 
                time.sleep(2 * (attempt + 1))
                continue
            if resp.status_code == 402: return "âŒ ERROR_402_NO_BALANCE"
            elif resp.status_code == 401: return "âŒ ERROR_401_INVALID_KEY"
            resp.raise_for_status()
            
            result = resp.json()['choices'][0]['message']['content'].strip()
            # æ¸…æ´— Markdown æ ‡è®°
            if result.startswith("```"):
                lines = result.split('\n')
                if len(lines) > 1:
                    result = '\n'.join(lines[1:-1]).strip()
            return result
        except requests.exceptions.RequestException:
            if attempt == 2: return f"\nğŸš¨ è¯·æ±‚å¤±è´¥"
            time.sleep(2)
    return f"\nğŸš¨ ç”Ÿæˆè¶…æ—¶"

def call_deepseek_api_chunked(prompt_template, words, progress_bar, status_text):
    api_key = st.secrets.get("DEEPSEEK_API_KEY")
    if not api_key: return "âš ï¸ é”™è¯¯ï¼šæœªé…ç½® DEEPSEEK_API_KEY"
    if not words: return "âš ï¸ é”™è¯¯ï¼šæ²¡æœ‰å•è¯"
    
    if len(words) > MAX_WORDS_LIMIT:
        st.warning(f"âš ï¸ æœ¬æ¬¡ä»…æˆªå–å‰ {MAX_WORDS_LIMIT} ä¸ªå•è¯ã€‚")
        words = words[:MAX_WORDS_LIMIT]

    chunks = [words[i:i + CHUNK_SIZE] for i in range(0, len(words), CHUNK_SIZE)]
    total_words = len(words)
    processed_count = 0
    results_ordered = [None] * len(chunks)
    
    status_text.markdown("ğŸš€ **æ­£åœ¨è¿æ¥ DeepSeek...**")
    
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_index = {
                executor.submit(_fetch_deepseek_chunk, chunk, prompt_template, api_key): i 
                for i, chunk in enumerate(chunks)
            }
            for future in concurrent.futures.as_completed(future_to_index):
                idx = future_to_index[future]
                try:
                    res = future.result()
                    if "ERROR_" in res: return res
                    results_ordered[idx] = res
                except: results_ordered[idx] = ""
                
                processed_count += len(chunks[idx])
                progress_bar.progress(min(processed_count / total_words, 1.0))
                status_text.markdown(f"**âš¡ å¤„ç†è¿›åº¦ï¼š** `{processed_count} / {total_words}`")
    except Exception as e:
        return f"âŒ å¼‚å¸¸: {str(e)}"

    return "\n".join([r for r in results_ordered if r])

# ==========================================
# 6. åˆ†æå¼•æ“
# ==========================================
def analyze_words(unique_word_list):
    unique_items = [] 
    JUNK = {'s', 't', 'd', 'm', 'll', 've', 're'}
    for item in unique_word_list:
        if len(item) < 2 and item not in ['a', 'i']: continue
        if item in JUNK: continue
        
        rank = vocab_dict.get(item, 99999)
        if item in BUILTIN_TECHNICAL_TERMS:
            unique_items.append({"word": f"{item} ({BUILTIN_TECHNICAL_TERMS[item]})", "rank": rank if rank!=99999 else 15000})
        elif item in PROPER_NOUNS_DB or item in AMBIGUOUS_WORDS:
            unique_items.append({"word": PROPER_NOUNS_DB.get(item, item.title()), "rank": rank})
        elif rank != 99999:
            unique_items.append({"word": item, "rank": rank})
            
    return pd.DataFrame(unique_items)

# ==========================================
# 7. UI ä¸ä¸»é€»è¾‘
# ==========================================
st.title("ğŸš€ Vocab Master Pro - Stable V5.1")
st.markdown("ğŸ’¡ æ”¯æŒç²˜è´´é•¿æ–‡æˆ–ä¸Šä¼ æ–‡ä»¶ï¼Œ**æ ¼å¼åŒ– Prompt è‡ªåŠ¨é€‚é…**ã€‚")

if "raw_input_text" not in st.session_state: st.session_state.raw_input_text = ""
if "uploader_key" not in st.session_state: st.session_state.uploader_key = 0 
if "is_processed" not in st.session_state: st.session_state.is_processed = False

def clear_all():
    st.session_state.raw_input_text = ""
    st.session_state.uploader_key += 1 
    st.session_state.is_processed = False

# --- å‚æ•°æ  ---
st.markdown("<div class='param-box'>", unsafe_allow_html=True)
c1, c2, c3, c4 = st.columns(4)
with c1: current_level = st.number_input("ğŸ¯ èµ·å§‹è¯æ±‡é‡", 0, 30000, 7500, 500)
with c2: target_level = st.number_input("ğŸ¯ ç›®æ ‡è¯æ±‡é‡", 0, 30000, 15000, 500)
with c3: top_n = st.number_input("ğŸ”¥ æå–æ•°é‡", 10, 500, 50, 10)
with c4: min_rank = st.number_input("ğŸ“‰ è¿‡æ»¤å‰Né«˜é¢‘è¯", 0, 20000, 3500, 500)
st.markdown("</div>", unsafe_allow_html=True)

# --- è¾“å…¥æ  ---
col_in1, col_in2 = st.columns([3, 2])
with col_in1: raw_text = st.text_area("ğŸ“¥ æ–‡æœ¬", height=150, key="raw_input_text")
with col_in2: uploaded_file = st.file_uploader("ğŸ“‚ æ–‡ä»¶", type=["txt", "pdf", "docx", "epub"], key=f"uploader_{st.session_state.uploader_key}")

c_btn1, c_btn2 = st.columns([5, 1])
if c_btn1.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", use_container_width=True):
    with st.spinner("Processing..."):
        txt = raw_text
        if uploaded_file: txt += "\n" + extract_text_from_file(uploaded_file)
        
        if not txt.strip():
            st.warning("æ— æœ‰æ•ˆæ–‡æœ¬")
        else:
            words = re.findall(r"[a-zA-Z']+", txt)
            lemmas = [get_lemma(w) for w in words]
            st.session_state.base_df = analyze_words(list(set([l.lower() for l in lemmas])))
            st.session_state.lemma_text = " ".join(lemmas)
            st.session_state.is_processed = True

if c_btn2.button("æ¸…ç©º", use_container_width=True): clear_all()

st.divider()

# ==========================================
# 8. ç»“æœå±•ç¤º
# ==========================================
if st.session_state.is_processed:
    df = st.session_state.base_df.copy()
    if not df.empty:
        df['cat'] = pd.cut(df['rank'], bins=[-1, current_level, target_level, 999999], labels=['known', 'target', 'beyond'])
        df = df.sort_values('rank')
        
        # æ•°æ®é›†å®šä¹‰
        datasets = {
            "ğŸ”¥ Topç²¾é€‰": df[df['rank'] >= min_rank].head(top_n),
            "ğŸŸ¡ é‡ç‚¹è¯": df[df['cat']=='target'],
            "ğŸ”´ è¶…çº²è¯": df[df['cat']=='beyond'],
            "ğŸŸ¢ å·²æŒæ¡": df[df['cat']=='known']
        }
        
        tabs = st.tabs(list(datasets.keys()) + ["åŸæ–‡"])
        
        for i, (label, sub_df) in enumerate(datasets.items()):
            with tabs[i]:
                if sub_df.empty:
                    st.info("æš‚æ— æ•°æ®")
                    continue
                
                # é¢„è§ˆ
                with st.expander(f"æŸ¥çœ‹åˆ—è¡¨ ({len(sub_df)}è¯)", expanded=(i==0)):
                    st.code("\n".join(sub_df['word'].tolist()), language='text')

                # ç”ŸæˆåŒº
                st.write("#### ğŸ¤– AI å¡ç‰‡ç”Ÿæˆ")
                col_fmt, col_act = st.columns([1, 4])
                with col_fmt:
                    # æ ¼å¼é€‰æ‹©å™¨
                    fmt_opt = st.radio("æ ¼å¼:", ["CSV", "TXT"], horizontal=True, key=f"fmt_{i}")
                    ext = "csv" if fmt_opt == "CSV" else "txt"
                    
                with col_act:
                    if st.button(f"âš¡ ç”Ÿæˆ {label} Ankiå¡ç‰‡", key=f"gen_{i}"):
                        pure_words = sub_df['word'].tolist()
                        # è·å–åŠ¨æ€ Prompt
                        prompt = get_base_prompt_template(fmt_opt)
                        
                        pb = st.progress(0)
                        st_status = st.empty()
                        
                        res = call_deepseek_api_chunked(prompt, pure_words, pb, st_status)
                        
                        if "âŒ" in res:
                            st.error(res)
                        else:
                            st_status.success("å®Œæˆï¼")
                            st.download_button(f"ğŸ“¥ ä¸‹è½½ .{ext}", res, f"anki_{label}.{ext}", "text/plain", type="primary")
                            st.code(res, language="text" if fmt_opt=="TXT" else "csv")
                            
        with tabs[-1]:
            st.download_button("ä¸‹è½½è¿˜åŸåå…¨æ–‡", st.session_state.lemma_text, "lemmatized.txt")