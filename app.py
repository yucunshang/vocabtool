import streamlit as st
import pandas as pd
import re
import os
import random
import json
import time
from datetime import datetime, timedelta, timezone

# ==========================================
# 0. é¡µé¢é…ç½®
# ==========================================
st.set_page_config(
    page_title="Vocab Flow Ultra", 
    page_icon="âš¡ï¸", 
    layout="centered", 
    initial_sidebar_state="collapsed"
)

st.markdown("""
<style>
    .stTextArea textarea { font-family: 'Consolas', monospace; font-size: 14px; }
    .stButton>button { border-radius: 8px; font-weight: 600; width: 100%; margin-top: 5px; }
    .stat-box { padding: 15px; background-color: #f0fdf4; border: 1px solid #bbf7d0; border-radius: 8px; text-align: center; color: #166534; margin-bottom: 20px; }
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stExpander { border: 1px solid #e0e0e0; border-radius: 8px; margin-bottom: 10px; }
    .preview-box { font-family: monospace; font-size: 12px; background: #f4f4f5; padding: 10px; border-radius: 5px; color: #666; max-height: 150px; overflow-y: auto; }
    
    /* æŒ‡å—æ ·å¼ */
    .guide-step { background-color: #f8f9fa; padding: 20px; border-radius: 10px; margin-bottom: 20px; border-left: 5px solid #0056b3; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
    .guide-title { font-size: 18px; font-weight: bold; color: #0f172a; margin-bottom: 10px; display: block; }
    .guide-tip { font-size: 14px; color: #64748b; background: #eef2ff; padding: 8px; border-radius: 4px; margin-top: 8px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. èµ„æºæ‡’åŠ è½½
# ==========================================
@st.cache_resource(show_spinner="æ­£åœ¨åŠ è½½ NLP å¼•æ“...")
def load_nlp_resources():
    import nltk
    import lemminflect
    try:
        root_dir = os.path.dirname(os.path.abspath(__file__))
        nltk_data_dir = os.path.join(root_dir, 'nltk_data')
        os.makedirs(nltk_data_dir, exist_ok=True)
        nltk.data.path.append(nltk_data_dir)
        for pkg in ['averaged_perceptron_tagger', 'punkt', 'punkt_tab']:
            try: nltk.data.find(f'tokenizers/{pkg}')
            except LookupError: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
    except: pass
    return nltk, lemminflect

def get_file_parsers():
    import pypdf
    import docx
    import ebooklib
    from ebooklib import epub
    from bs4 import BeautifulSoup
    return pypdf, docx, ebooklib, epub, BeautifulSoup

def get_genanki():
    import genanki
    import tempfile
    return genanki, tempfile

@st.cache_data
def load_vocab_data():
    possible_files = ["coca_cleaned.csv", "data.csv", "vocab.csv"]
    file_path = next((f for f in possible_files if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            df.columns = [c.strip().lower() for c in df.columns]
            w_col = next((c for c in df.columns if 'word' in c), df.columns[0])
            r_col = next((c for c in df.columns if 'rank' in c), df.columns[1])
            df = df.dropna(subset=[w_col])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce')
            df = df.sort_values(r_col).drop_duplicates(subset=[w_col], keep='first')
            return pd.Series(df[r_col].values, index=df[w_col]).to_dict(), df
        except: return {}, None
    return {}, None

VOCAB_DICT, FULL_DF = load_vocab_data()

def get_beijing_time_str():
    utc_now = datetime.now(timezone.utc)
    beijing_now = utc_now + timedelta(hours=8)
    return beijing_now.strftime('%m%d_%H%M')

def clear_all_state():
    st.session_state.clear()

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘
# ==========================================
def extract_text_from_file(uploaded_file):
    pypdf, docx, ebooklib, epub, BeautifulSoup = get_file_parsers()
    text = ""
    file_type = uploaded_file.name.split('.')[-1].lower()
    try:
        if file_type == 'txt':
            text = uploaded_file.getvalue().decode("utf-8", errors='ignore')
        elif file_type == 'pdf':
            reader = pypdf.PdfReader(uploaded_file)
            text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif file_type == 'docx':
            doc = docx.Document(uploaded_file)
            text = "\n".join([p.text for p in doc.paragraphs])
        elif file_type == 'epub':
            genanki, tempfile = get_genanki()
            with tempfile.NamedTemporaryFile(delete=False, suffix='.epub') as tmp:
                tmp.write(uploaded_file.getvalue())
                tmp_path = tmp.name
            book = epub.read_epub(tmp_path)
            for item in book.get_items():
                if item.get_type() == ebooklib.ITEM_DOCUMENT:
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    text += soup.get_text() + " "
            os.remove(tmp_path)
    except Exception as e:
        return f"Error: {e}"
    return text

def analyze_logic(text, current_lvl, target_lvl, include_unknown):
    nltk, lemminflect = load_nlp_resources()
    def get_lemma_local(word):
        try: return lemminflect.getLemma(word, upos='VERB')[0]
        except: return word

    # 1. æå–æ‰€æœ‰å•è¯
    raw_tokens = re.findall(r"[a-z]+", text.lower())
    total_words = len(raw_tokens)
    
    # 2. åŸå§‹å»é‡
    unique_tokens = set(raw_tokens)
    
    target_words = []
    seen_lemmas = set() # 3. è¯æ ¹å»é‡ (go/went é—®é¢˜)
    
    for w in unique_tokens:
        if len(w) < 2: continue 
        lemma = get_lemma_local(w)
        
        if lemma in seen_lemmas: continue
            
        rank = VOCAB_DICT.get(lemma, 99999)
        
        # 4. ç­›é€‰é€»è¾‘ï¼šåŒºé—´å†… OR (æ˜¯ç”Ÿè¯ ä¸” å…è®¸ç”Ÿè¯)
        is_in_range = (rank >= current_lvl and rank <= target_lvl)
        is_unknown_included = (rank == 99999 and include_unknown)
        
        if is_in_range or is_unknown_included:
            target_words.append((lemma, rank))
            seen_lemmas.add(lemma)
            
    # æœªçŸ¥è¯æ’åœ¨æœ€å
    target_words.sort(key=lambda x: x[1])
    return [x[0] for x in target_words], total_words

def parse_anki_data(raw_text):
    parsed_cards = []
    text = raw_text.replace("```json", "").replace("```", "").strip()
    # æ­£åˆ™æµå¼è§£æï¼Œä¸æƒ§æ¢è¡Œ
    matches = re.finditer(r'\{.*?\}', text, re.DOTALL)
    seen_phrases = set()

    for match in matches:
        json_str = match.group()
        try:
            data = json.loads(json_str, strict=False)
            front_text = data.get("w", "").strip()
            meaning = data.get("m", "").strip()
            examples = data.get("e", "").strip()
            etymology = data.get("r", "").strip()
            
            if not etymology or etymology.lower() == "none" or etymology == "":
                etymology = ""

            if not front_text or not meaning: continue
            front_text = front_text.replace('**', '')
            if front_text in seen_phrases: continue
            seen_phrases.add(front_text)

            parsed_cards.append({
                'front_phrase': front_text,
                'meaning': meaning,
                'examples': examples,
                'etymology': etymology
            })
        except: continue
    return parsed_cards

# ==========================================
# 3. Anki ç”Ÿæˆ
# ==========================================
def generate_anki_package(cards_data, deck_name):
    genanki, tempfile = get_genanki()
    
    CSS = """
    .card { font-family: 'Arial', sans-serif; font-size: 20px; text-align: center; color: #333; background-color: white; padding: 20px; }
    .nightMode .card { background-color: #2e2e2e; color: #f0f0f0; }
    .phrase { font-size: 28px; font-weight: 700; color: #0056b3; margin-bottom: 20px; line-height: 1.3; }
    .nightMode .phrase { color: #66b0ff; }
    hr { border: 0; height: 1px; background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.2), rgba(0, 0, 0, 0)); margin-bottom: 15px; }
    .definition { font-weight: bold; color: #222; margin-bottom: 15px; font-size: 20px; text-align: left; }
    .nightMode .definition { color: #e0e0e0; }
    .examples { background: #f7f9fa; padding: 12px; border-left: 4px solid #0056b3; border-radius: 4px; color: #444; font-style: italic; font-size: 18px; line-height: 1.5; margin-bottom: 15px; text-align: left; }
    .nightMode .examples { background: #383838; color: #ccc; border-left-color: #66b0ff; }
    .footer-info { margin-top: 20px; border-top: 1px dashed #ccc; padding-top: 10px; text-align: left; }
    .etymology { display: block; font-size: 16px; color: #555; background-color: #fffdf5; padding: 10px; border-radius: 6px; margin-bottom: 5px; line-height: 1.4; border: 1px solid #fef3c7; }
    .nightMode .etymology { background-color: #333; color: #aaa; border-color: #444; }
    """
    model_id = random.randrange(1 << 30, 1 << 31)
    model = genanki.Model(
        model_id, f'VocabFlow JSON Model {model_id}',
        fields=[{'name': 'FrontPhrase'}, {'name': 'Meaning'}, {'name': 'Examples'}, {'name': 'Etymology'}],
        templates=[{
            'name': 'Phrase Card',
            'qfmt': '<div class="phrase">{{FrontPhrase}}</div>', 
            'afmt': '''
            {{FrontSide}}<hr>
            <div class="definition">{{Meaning}}</div>
            <div class="examples">{{Examples}}</div>
            {{#Etymology}}
            <div class="footer-info"><div class="etymology">ğŸŒ± <b>è¯æº:</b> {{Etymology}}</div></div>
            {{/Etymology}}
            ''',
        }], css=CSS
    )
    deck = genanki.Deck(random.randrange(1 << 30, 1 << 31), deck_name)
    for c in cards_data:
        deck.add_note(genanki.Note(model=model, fields=[str(c['front_phrase']), str(c['meaning']), str(c['examples']).replace('\n','<br>'), str(c['etymology'])]))
    with tempfile.NamedTemporaryFile(delete=False, suffix='.apkg') as tmp:
        genanki.Package(deck).write_to_file(tmp.name)
        return tmp.name

# ==========================================
# 4. Prompt Logic
# ==========================================
def get_ai_prompt(words, front_mode, def_mode, ex_count, need_ety):
    w_list = ", ".join(words)
    
    if front_mode == "å•è¯ (Word)":
        w_instr = "Key `w`: The word itself (lowercase)."
    else:
        w_instr = "Key `w`: A short practical collocation/phrase (2-5 words)."

    if def_mode == "ä¸­æ–‡":
        m_instr = "Key `m`: Concise Chinese definition (max 10 chars)."
    elif def_mode == "ä¸­è‹±åŒè¯­":
        m_instr = "Key `m`: English Definition + Chinese Definition."
    else:
        m_instr = "Key `m`: English definition (concise)."

    e_instr = f"Key `e`: {ex_count} example sentence(s). Use `<br>` to separate if multiple."

    if need_ety:
        r_instr = "Key `r`: Simplified Chinese Etymology (Root/Prefix)."
    else:
        r_instr = "Key `r`: Leave this empty string \"\"."

    return f"""
Task: Create Anki cards.
Words: {w_list}

**OUTPUT: NDJSON (One line per object).**

**Requirements:**
1. {w_instr}
2. {m_instr}
3. {e_instr}
4. {r_instr}

**Keys:** `w` (Front), `m` (Meaning), `e` (Examples), `r` (Etymology)

**Example:**
{{"w": "...", "m": "...", "e": "...", "r": "..."}}

**Start:**
"""

# ==========================================
# 5. UI ä¸»ç¨‹åº
# ==========================================
st.title("âš¡ï¸ Vocab Flow Ultra (V25)")

if not VOCAB_DICT:
    st.error("âš ï¸ ç¼ºå¤± `coca_cleaned.csv`")

tab_guide, tab_extract, tab_anki = st.tabs(["ğŸ“– ä½¿ç”¨æŒ‡å— (å®Œæ•´ç‰ˆ)", "1ï¸âƒ£ å•è¯æå–", "2ï¸âƒ£ Anki åˆ¶ä½œ"])

with tab_guide:
    st.markdown("""
    ### ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ Vocab Flow Ultra
    è¿™æ˜¯ä¸€ä¸ª**ä»é˜…è¯»ææ–™ä¸­æå–ç”Ÿè¯**ï¼Œå¹¶åˆ©ç”¨ **AI** è‡ªåŠ¨ç”Ÿæˆ **Anki å¡ç‰‡**çš„æ•ˆç‡å·¥å…·ã€‚
    
    ---
    
    <div class="guide-step">
    <span class="guide-title">Step 1: æå–ç”Ÿè¯ (Extract)</span>
    åœ¨ <code>1ï¸âƒ£ å•è¯æå–</code> æ ‡ç­¾é¡µï¼š<br><br>
    <strong>1. ä¸Šä¼ æ–‡ä»¶</strong><br>
    æ”¯æŒ <code>.pdf</code>, <code>.txt</code>, <code>.epub</code>, <code>.docx</code>ï¼Œæˆ–è€…ç›´æ¥ç²˜è´´æ–‡æœ¬ã€‚<br>
    <div class="guide-tip">ğŸ’¡ ç³»ç»Ÿä¼šè‡ªåŠ¨è¿‡æ»¤æ‰æ–‡æ¡£ä¸­çš„éå•è¯å­—ç¬¦ï¼Œå¹¶å°† <code>went</code>, <code>goes</code> è¿˜åŸä¸º <code>go</code> è¿›è¡Œç»Ÿè®¡ã€‚</div>
    <br>
    <strong>2. è®¾ç½®è¿‡æ»¤èŒƒå›´ (Rank Filter)</strong><br>
    åˆ©ç”¨ COCA 20000 è¯é¢‘è¡¨è¿›è¡Œç§‘å­¦ç­›é€‰ï¼š
    <ul>
        <li><strong>å¿½ç•¥æ’åå‰ N</strong> (Min Rank)ï¼šä¾‹å¦‚è®¾ä¸º <code>2000</code>ï¼Œä¼šè¿‡æ»¤æ‰ `the, is, you` ç­‰æœ€åŸºç¡€çš„é«˜é¢‘è¯ã€‚å¦‚æœä½ åŸºç¡€å¾ˆå¥½ï¼Œå¯ä»¥è®¾ä¸º <code>5000</code>ã€‚</li>
        <li><strong>å¿½ç•¥æ’åå N</strong> (Max Rank)ï¼šä¾‹å¦‚è®¾ä¸º <code>15000</code>ï¼Œä¼šè¿‡æ»¤æ‰æå…¶ç”Ÿåƒ»çš„è¯ã€‚</li>
        <li><strong>ğŸ”“ åŒ…å«ç”Ÿåƒ»è¯</strong> (Unknown)ï¼šã€Šå†°ä¸ç«ä¹‹æ­Œã€‹ç­‰å°è¯´åŒ…å«å¤§é‡äººåæˆ–è‡ªé€ è¯ï¼Œå®ƒä»¬æ²¡æœ‰æ’åã€‚å‹¾é€‰æ­¤é¡¹å¯ä»¥å¼ºåˆ¶æå–å®ƒä»¬ã€‚</li>
    </ul>
    <br>
    <strong>3. ç‚¹å‡» ğŸš€ å¼€å§‹åˆ†æ</strong><br>
    ç³»ç»Ÿä¼šæ˜¾ç¤ºâ€œæ•°æ®çœ‹æ¿â€ï¼Œå‘Šè¯‰ä½ æ–‡æ¡£æ€»å­—æ•°ã€ç­›é€‰å‡ºäº†å¤šå°‘ç”Ÿè¯ï¼Œä»¥åŠè€—æ—¶ã€‚
    </div>

    <div class="guide-step">
    <span class="guide-title">Step 2: è·å– Prompt (AI Generation)</span>
    åˆ†æå®Œæˆåï¼Œä½ ä¼šçœ‹åˆ°ç”Ÿæˆçš„å•è¯åˆ—è¡¨ã€‚<br><br>
    <strong>1. è‡ªå®šä¹‰è®¾ç½® (Customize)</strong><br>
    ç‚¹å‡» <code>âš™ï¸ è‡ªå®šä¹‰ Prompt è®¾ç½®</code> å±•å¼€é¢æ¿ï¼š
    <ul>
        <li><strong>æ­£é¢å†…å®¹</strong>ï¼šé€‰æ‹©èƒŒå•è¯æœ¬èº«ï¼Œè¿˜æ˜¯èƒŒçŸ­è¯­æ­é…ã€‚</li>
        <li><strong>èƒŒé¢é‡Šä¹‰</strong>ï¼šè‹±æ–‡ã€ä¸­æ–‡æˆ–ä¸­è‹±åŒè¯­ã€‚</li>
        <li><strong>ä¾‹å¥/è¯æº</strong>ï¼šæŒ‰éœ€å¼€å¯ã€‚</li>
    </ul>
    <br>
    <strong>2. å¤åˆ¶ Prompt</strong><br>
    ç³»ç»Ÿä¼šè‡ªåŠ¨å°†å•è¯åˆ†ç»„ï¼ˆé˜²æ­¢ AI é•¿åº¦æº¢å‡ºï¼‰ã€‚
    <ul>
        <li>ğŸ“± <strong>æ‰‹æœºç«¯</strong>ï¼šä½¿ç”¨ä¸‹æ–¹çš„â€œçº¯æ–‡æœ¬æ¡†â€ï¼Œé•¿æŒ‰å…¨é€‰ -> å¤åˆ¶ã€‚</li>
        <li>ğŸ’» <strong>ç”µè„‘ç«¯</strong>ï¼šç‚¹å‡»ä»£ç å—å³ä¸Šè§’çš„ Copy ğŸ“„ å›¾æ ‡ã€‚</li>
    </ul>
    <br>
    <strong>3. å‘é€ç»™ AI</strong><br>
    å°†å¤åˆ¶çš„å†…å®¹å‘é€ç»™ ChatGPT / Claude / Gemini / DeepSeekã€‚AI ä¼šè¿”å›ä¸€ä¸² JSON æ•°æ®ã€‚
    </div>

    <div class="guide-step">
    <span class="guide-title">Step 3: åˆ¶ä½œ Anki ç‰Œç»„ (Create Deck)</span>
    åœ¨ <code>2ï¸âƒ£ Anki åˆ¶ä½œ</code> æ ‡ç­¾é¡µï¼š<br><br>
    <strong>1. ç²˜è´´ AI å›å¤</strong><br>
    å°† AI ç”Ÿæˆçš„ JSON å†…å®¹ç²˜è´´åˆ°è¾“å…¥æ¡†ä¸­ã€‚<br>
    <div class="guide-tip">ğŸ’¡ <strong>æ”¯æŒè¿½åŠ ç²˜è´´</strong>ï¼šå¦‚æœä½ æœ‰ 5 ç»„å•è¯ï¼Œå¯ä»¥æŠŠ AI çš„ 5 æ¬¡å›å¤ä¾æ¬¡ç²˜è´´åœ¨åŒä¸€ä¸ªæ¡†é‡Œï¼Œä¸éœ€è¦åˆ†æ‰¹ä¸‹è½½ã€‚</div>
    <br>
    <strong>2. ä¸‹è½½ä¸å¯¼å…¥</strong><br>
    ç‚¹å‡» <strong>ğŸ“¥ ä¸‹è½½ .apkg</strong>ï¼Œç„¶ååŒå‡»è¯¥æ–‡ä»¶ï¼Œå®ƒä¼šè‡ªåŠ¨å¯¼å…¥åˆ°ä½ çš„ Anki è½¯ä»¶ä¸­ã€‚
    </div>
    
    <div class="guide-step" style="border-left-color: #10b981; background-color: #ecfdf5;">
    <span class="guide-title">ğŸ’¡ è¿›é˜¶æŠ€å·§</span>
    <ul>
        <li><strong>ä¸€é”®å¤åˆ¶æ‰€æœ‰å•è¯</strong>ï¼šåœ¨â€œåˆ†ææŠ¥å‘Šâ€ä¸‹æ–¹ï¼Œæœ‰ä¸€ä¸ªâ€œå…¨éƒ¨ç”Ÿè¯â€çš„ä»£ç å—ï¼Œç‚¹å‡»å³ä¸Šè§’å›¾æ ‡å¯ä¸€æ¬¡æ€§å¯¼å‡ºåˆ° Excelã€‚</li>
        <li><strong>æ–‡ä»¶è¯»å–æ£€æŸ¥</strong>ï¼šå¦‚æœè§‰å¾—æå–çš„è¯å¤ªå°‘ï¼Œå¯ä»¥å±•å¼€â€œğŸ” æ–‡ä»¶è¯»å–éªŒå°¸â€æŸ¥çœ‹æ–‡æ¡£å¼€å¤´å’Œç»“å°¾ï¼Œç¡®è®¤ç¨‹åºæ˜¯å¦è¯»å®Œäº†æ•´æœ¬ä¹¦ã€‚</li>
        <li><strong>è¯æ ¹å»é‡</strong>ï¼šV23 ç‰ˆæœ¬å·²å‡çº§å»é‡ç®—æ³•ï¼Œä¸ä¼šå†åŒæ—¶å‡ºç° <code>go</code> å’Œ <code>went</code>ã€‚</li>
    </ul>
    </div>
    """, unsafe_allow_html=True)

with tab_extract:
    mode_context, mode_rank = st.tabs(["ğŸ“„ è¯­å¢ƒåˆ†æ", "ğŸ”¢ è¯é¢‘åˆ—è¡¨"])
    
    with mode_context:
        c1, c2 = st.columns(2)
        curr = c1.number_input("å¿½ç•¥æ’åå‰ N çš„è¯", 1, 20000, 100, step=100)
        targ = c2.number_input("å¿½ç•¥æ’åå N çš„è¯", 2000, 50000, 20000, step=500)
        
        include_unknown = st.checkbox("ğŸ”“ åŒ…å«è¯å…¸é‡Œæ²¡æœ‰çš„ç”Ÿåƒ»è¯/äººå (Rank > 20000)", value=False, help="ã€Šå†°ä¸ç«ä¹‹æ­Œã€‹ç­‰å¥‡å¹»å°è¯´æœ‰å¾ˆå¤šè‡ªé€ è¯ï¼Œå‹¾é€‰æ­¤é¡¹å¯ä»¥æå–å®ƒä»¬ã€‚")
        
        uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡æ¡£ (TXT/PDF/DOCX/EPUB)")
        pasted_text = st.text_area("ğŸ“„ ...æˆ–ç²˜è´´æ–‡æœ¬", height=100)
        
        if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
            with st.status("æ­£åœ¨å¤„ç†...", expanded=True) as status:
                start_time = time.time()
                status.write("ğŸ“‚ è¯»å–æ–‡ä»¶...")
                raw_text = extract_text_from_file(uploaded_file) if uploaded_file else pasted_text
                
                if len(raw_text) > 10:
                    status.write(f"ğŸ” æå– {len(raw_text)} å­—ç¬¦ï¼ŒåŠ è½½ NLP åº“...")
                    final_words, raw_count = analyze_logic(raw_text, curr, targ, include_unknown)
                    
                    st.session_state['gen_words'] = final_words
                    st.session_state['raw_count'] = raw_count
                    st.session_state['process_time'] = time.time() - start_time
                    st.session_state['raw_text_preview'] = raw_text 
                    
                    status.update(label="âœ… åˆ†æå®Œæˆ", state="complete", expanded=False)
                else:
                    status.update(label="âš ï¸ å†…å®¹å¤ªçŸ­", state="error")
        
        if st.button("ğŸ—‘ï¸ æ¸…ç©º", type="secondary", on_click=clear_all_state): pass

    with mode_rank:
        gen_type = st.radio("æ¨¡å¼", ["ğŸ”¢ é¡ºåº", "ğŸ”€ éšæœº"], horizontal=True)
        if "é¡ºåº" in gen_type:
             c_a, c_b = st.columns(2)
             s_rank = c_a.number_input("èµ·å§‹æ’å", 1, 20000, 1000, step=100)
             count = c_b.number_input("æ•°é‡", 10, 500, 50, step=10)
             if st.button("ğŸš€ ç”Ÿæˆ"):
                 start_time = time.time()
                 if FULL_DF is not None:
                     r_col = next(c for c in FULL_DF.columns if 'rank' in c)
                     w_col = next(c for c in FULL_DF.columns if 'word' in c)
                     subset = FULL_DF[FULL_DF[r_col] >= s_rank].sort_values(r_col).head(count)
                     st.session_state['gen_words'] = subset[w_col].tolist()
                     st.session_state['raw_count'] = 0
                     st.session_state['process_time'] = time.time() - start_time
        else:
             c_min, c_max, c_cnt = st.columns([1,1,1])
             min_r = c_min.number_input("Min Rank", 1, 20000, 1, step=100)
             max_r = c_max.number_input("Max Rank", 1, 25000, 5000, step=100)
             r_count = c_cnt.number_input("Count", 10, 200, 50, step=10)
             if st.button("ğŸ² æŠ½å–"):
                 start_time = time.time()
                 if FULL_DF is not None:
                     r_col = next(c for c in FULL_DF.columns if 'rank' in c)
                     w_col = next(c for c in FULL_DF.columns if 'word' in c)
                     mask = (FULL_DF[r_col] >= min_r) & (FULL_DF[r_col] <= max_r)
                     candidates = FULL_DF[mask]
                     if len(candidates) > 0:
                         subset = candidates.sample(n=min(r_count, len(candidates))).sort_values(r_col)
                         st.session_state['gen_words'] = subset[w_col].tolist()
                         st.session_state['raw_count'] = 0
                         st.session_state['process_time'] = time.time() - start_time

    if 'gen_words' in st.session_state and st.session_state['gen_words']:
        words = st.session_state['gen_words']
        
        st.divider()
        st.markdown("### ğŸ“Š åˆ†ææŠ¥å‘Š")
        
        with st.expander("ğŸ” **æ–‡ä»¶è¯»å–éªŒå°¸ (Check First/Last 500 chars)**"):
            raw_preview = st.session_state.get('raw_text_preview', "")
            if raw_preview:
                st.markdown("**Head (å¼€å¤´ 500 å­—ç¬¦):**")
                st.markdown(f"<div class='preview-box'>{raw_preview[:500]}...</div>", unsafe_allow_html=True)
                st.markdown("**Tail (ç»“å°¾ 500 å­—ç¬¦):**")
                st.markdown(f"<div class='preview-box'>...{raw_preview[-500:]}</div>", unsafe_allow_html=True)
            else:
                st.info("æ— åŸæ–‡æ¡£æ•°æ®ï¼ˆè¯é¢‘æ¨¡å¼æˆ–æœªä¸Šä¼ ï¼‰ã€‚")

        k1, k2, k3 = st.columns(3)
        raw_c = st.session_state.get('raw_count', 0)
        p_time = st.session_state.get('process_time', 0.1)
        k1.metric("ğŸ“„ æ–‡æ¡£æ€»å­—æ•°", f"{raw_c:,}")
        k2.metric("ğŸ¯ ç­›é€‰ç”Ÿè¯", f"{len(words)}")
        k3.metric("âš¡ è€—æ—¶", f"{p_time:.2f}s")
        
        st.markdown("### ğŸ“‹ å…¨éƒ¨ç”Ÿè¯ (ç‚¹å‡»å³ä¸Šè§’å¤åˆ¶)")
        all_words_str = ", ".join(words)
        st.code(all_words_str, language="text")

        with st.expander("âš™ï¸ **è‡ªå®šä¹‰ Prompt è®¾ç½® (ç‚¹å‡»å±•å¼€)**", expanded=True):
            col_s1, col_s2 = st.columns(2)
            front_mode = col_s1.selectbox("æ­£é¢å†…å®¹", ["çŸ­è¯­æ­é… (Phrase)", "å•è¯ (Word)"])
            def_mode = col_s2.selectbox("èƒŒé¢é‡Šä¹‰", ["è‹±æ–‡", "ä¸­æ–‡", "ä¸­è‹±åŒè¯­"])
            
            col_s3, col_s4 = st.columns(2)
            ex_count = col_s3.slider("ä¾‹å¥æ•°é‡", 1, 3, 1)
            need_ety = col_s4.checkbox("åŒ…å«è¯æº/è¯æ ¹", value=True)

        batch_size = st.number_input("AI åˆ†ç»„å¤§å°", 10, 200, 100, step=10)
        batches = [words[i:i + batch_size] for i in range(0, len(words), batch_size)]
        
        for idx, batch in enumerate(batches):
            with st.expander(f"ğŸ“Œ ç¬¬ {idx+1} ç»„ (å…± {len(batch)} è¯)", expanded=(idx==0)):
                prompt_text = get_ai_prompt(batch, front_mode, def_mode, ex_count, need_ety)
                st.caption("ğŸ“± æ‰‹æœºç«¯ä¸“ç”¨ï¼š")
                st.text_area(f"text_area_{idx}", value=prompt_text, height=100, label_visibility="collapsed")
                st.caption("ğŸ’» ç”µè„‘ç«¯ï¼š")
                st.code(prompt_text, language="text")

with tab_anki:
    st.markdown("### ğŸ“¦ åˆ¶ä½œ Anki")
    bj_time_str = get_beijing_time_str()
    if 'anki_input_text' not in st.session_state: st.session_state['anki_input_text'] = ""

    st.caption("ğŸ‘‡ ç²˜è´´ AI å›å¤ï¼š")
    ai_resp = st.text_area("JSON è¾“å…¥æ¡†", height=300, key="anki_input_text")
    deck_name = st.text_input("ç‰Œç»„å", f"Vocab_{bj_time_str}")
    
    if ai_resp.strip():
        parsed_data = parse_anki_data(ai_resp)
        if parsed_data:
            st.success(f"âœ… æˆåŠŸè§£æ {len(parsed_data)} æ¡æ•°æ®")
            df_view = pd.DataFrame(parsed_data)
            df_view.rename(columns={'front_phrase': 'æ­£é¢', 'meaning': 'èƒŒé¢', 'etymology': 'è¯æº'}, inplace=True)
            st.dataframe(df_view[['æ­£é¢', 'èƒŒé¢', 'è¯æº']], use_container_width=True, hide_index=True)
            
            f_path = generate_anki_package(parsed_data, deck_name)
            with open(f_path, "rb") as f:
                st.download_button(f"ğŸ“¥ ä¸‹è½½ {deck_name}.apkg", f, file_name=f"{deck_name}.apkg", mime="application/octet-stream", type="primary")
        else:
            st.warning("âš ï¸ ç­‰å¾…ç²˜è´´...")