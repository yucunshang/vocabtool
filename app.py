import streamlit as st
import pandas as pd
import os
import sys
import subprocess

# ==========================================
# 1. Google Translate é£æ ¼é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Analyzer", page_icon="ğŸ…°ï¸")

# è‡ªå®šä¹‰ CSSï¼šè®©ç•Œé¢æ›´åƒ Google Translate (å¤§æ–‡æœ¬æ¡†ã€æ¸…çˆ½å­—ä½“)
st.markdown("""
<style>
    /* è¾“å…¥æ¡†æ ·å¼ */
    .stTextArea textarea {
        font-size: 16px !important;
        font-family: 'Roboto', sans-serif;
        border-radius: 8px;
    }
    /* æ•°å­—è¾“å…¥æ¡†æ ·å¼ */
    .stNumberInput input {
        font-weight: bold;
        color: #1a73e8; /* Google Blue */
    }
    /* éšè—é¡¶éƒ¨å¤šä½™çš„å½©æ¡ */
    header {visibility: hidden;}
    /* è°ƒæ•´é¡¶éƒ¨é—´è· */
    .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
    /* ç»“æœåˆ—è¡¨æ ·å¼ */
    .vocab-list {
        font-family: monospace;
        font-size: 15px;
        line-height: 1.6;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ ¸å¿ƒå¼•æ“ (spaCy + è‡ªåŠ¨ä¿®å¤)
# ==========================================
@st.cache_resource
def load_nlp():
    """åŠ è½½æˆ–è‡ªåŠ¨ä¸‹è½½ spaCy æ¨¡å‹"""
    try:
        import spacy
    except ImportError:
        return None
    
    model_name = "en_core_web_sm"
    try:
        return spacy.load(model_name)
    except:
        # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰
        try:
            subprocess.check_call([sys.executable, "-m", "spacy", "download", model_name])
            return spacy.load(model_name)
        except:
            return None

# ==========================================
# 3. è¯åº“åŠ è½½ (é™é»˜åŠ è½½)
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv"]

@st.cache_data
def load_vocab():
    file_path = next((f for f in POSSIBLE_FILES if os.path.exists(f)), None)
    if not file_path: return None
    
    try:
        df = pd.read_csv(file_path)
        # æç®€æ¸…æ´—
        cols = [str(c).strip().lower() for c in df.columns]
        df.columns = cols
        
        # æ™ºèƒ½æ‰¾åˆ—
        w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
        r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
        
        df[w_col] = df[w_col].astype(str).str.lower().str.strip()
        df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
        
        return pd.Series(df[r_col].values, index=df[w_col]).to_dict()
    except:
        return None

# åˆå§‹åŒ–èµ„æº
nlp = load_nlp()
vocab = load_vocab()

# ==========================================
# 4. ç•Œé¢å¸ƒå±€ (Top Bar + Split View)
# ==========================================

# --- é¡¶éƒ¨ï¼šè®¾ç½®æ  ---
c1, c2, c3 = st.columns([1, 1, 3])
with c1:
    # æ­¥é•¿ 500ï¼Œé»˜è®¤ 6000
    current_level = st.number_input("å½“å‰è¯æ±‡é‡ (Current)", min_value=0, max_value=20000, value=6000, step=500)
with c2:
    # æ­¥é•¿ 500ï¼Œé»˜è®¤ 8000
    target_level = st.number_input("ç›®æ ‡è¯æ±‡é‡ (Target)", min_value=0, max_value=20000, value=8000, step=500)
with c3:
    st.write("") # å ä½

st.divider()

# --- ä¸»ä½“ï¼šå·¦å³åˆ†æ  ---
left_col, right_col = st.columns([1, 1])

# === å·¦ä¾§ï¼šè¾“å…¥åŒº ===
with left_col:
    text_input = st.text_area(
        label="è¾“å…¥æ–‡æœ¬",
        placeholder="åœ¨æ­¤ç²˜è´´è‹±è¯­æ–‡ç« ...",
        height=600,
        label_visibility="collapsed"
    )
    
    # æ”¾åœ¨å·¦ä¾§åº•éƒ¨çš„æŒ‰é’®
    analyze_btn = st.button("âš¡ å¼€å§‹åˆ†æ / Analyze", type="primary", use_container_width=True)

# === å³ä¾§ï¼šç»“æœåŒº ===
with right_col:
    if not nlp:
        st.error("æ­£åœ¨åˆå§‹åŒ– NLP å¼•æ“ï¼Œè¯·ç¨ç­‰æˆ–åˆ·æ–°...")
    elif not vocab:
        st.error("æœªæ‰¾åˆ°è¯åº“æ–‡ä»¶ (coca_cleaned.csv)ï¼Œè¯·å…ˆä¸Šä¼ ã€‚")
    elif analyze_btn and text_input:
        
        with st.spinner("Analyzing..."):
            # 1. spaCy å¤„ç† (å¢åŠ  max_length é˜²æ­¢å¤§æ–‡æœ¬æŠ¥é”™)
            nlp.max_length = 2000000 
            doc = nlp(text_input.lower())
            
            # 2. æå–ä¸è¿˜åŸ
            seen = set()
            data = []
            
            for token in doc:
                # è¿‡æ»¤éå­—æ¯ (å¤„ç†å¤§å°å†™ã€ç¬¦å·ã€éè‹±æ–‡)
                if token.is_alpha and len(token.text) > 1:
                    lemma = token.lemma_ # è¿˜åŸ: families -> family
                    
                    if lemma not in seen:
                        # æŸ¥æ’å
                        rank = vocab.get(lemma, 99999)
                        
                        # äºŒæ¬¡æŸ¥æ‰¾é€»è¾‘ (é˜²æ­¢ spaCy è¿˜åŸè¿‡åº¦ï¼Œæˆ–è€…è¯åº“é‡Œåªæœ‰åŸè¯)
                        if rank == 99999 and token.text in vocab:
                            rank = vocab[token.text]
                            lemma = token.text
                            
                        data.append({'word': lemma, 'rank': int(rank)})
                        seen.add(lemma)
            
            # 3. åˆ†ç»„
            df = pd.DataFrame(data)
            
            if not df.empty:
                df = df.sort_values('rank')
                
                # ä¸‰ä¸ªæ¡¶
                known = df[df['rank'] <= current_level]
                target = df[(df['rank'] > current_level) & (df['rank'] <= target_level)]
                beyond = df[df['rank'] > target_level]
                
                # 4. æ˜¾ç¤ºç»“æœ (Tabs)
                t1, t2, t3 = st.tabs([
                    f"ğŸŸ¡ é‡ç‚¹ ({len(target)})", 
                    f"ğŸ”´ è¶…çº² ({len(beyond)})", 
                    f"ğŸŸ¢ å·²æŒæ¡ ({len(known)})"
                ])
                
                # å®šä¹‰çº¯æ–‡æœ¬æ¸²æŸ“å‡½æ•°
                def render_text_list(dataframe):
                    if dataframe.empty:
                        st.caption("åˆ—è¡¨ä¸ºç©º")
                        return
                    
                    # ç”Ÿæˆæ–‡æœ¬åˆ—è¡¨: 1. word (1234)
                    lines = []
                    for i, row in dataframe.iterrows():
                        # æ ¼å¼ï¼šå•è¯ (æ’å)
                        lines.append(f"{row['word']} ({row['rank']})")
                    
                    # ä½¿ç”¨æ»šåŠ¨å®¹å™¨æ˜¾ç¤ºï¼Œé˜²æ­¢é¡µé¢è¿‡é•¿
                    with st.container(height=500):
                        # join æ¢è¡Œç¬¦ï¼Œç›´æ¥æ˜¾ç¤ºçº¯æ–‡æœ¬
                        st.text("\n".join(lines))

                with t1:
                    render_text_list(target)
                with t2:
                    render_text_list(beyond)
                with t3:
                    render_text_list(known)
            else:
                st.warning("æœªæ£€æµ‹åˆ°æœ‰æ•ˆè‹±æ–‡å•è¯ã€‚")
                
    elif analyze_btn and not text_input:
        st.warning("è¯·å…ˆåœ¨å·¦ä¾§ç²˜è´´æ–‡æœ¬ã€‚")
    else:
        # ç©ºé—²çŠ¶æ€æ˜¾ç¤ºå ä½ç¬¦
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥æ–‡æœ¬ï¼Œç„¶åç‚¹å‡»åˆ†æã€‚")
        st.caption("æ”¯æŒå¤§æ–‡æœ¬ç²˜è´´ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¿‡æ»¤ç¬¦å·å’Œéè‹±æ–‡å†…å®¹ã€‚")