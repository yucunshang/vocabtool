import streamlit as st
import pandas as pd
import re
import os
import lemminflect
import nltk
import json
import time  # æ–°å¢ï¼šç”¨äºç²¾ç¡®æµ‹é€Ÿ

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(layout="wide", page_title="Vocab Master Pro", page_icon="ğŸš€")

st.markdown("""
<style>
    .stCode {
        font-family: 'Consolas', 'Courier New', monospace !important;
        font-size: 16px !important;
    }
    header {visibility: hidden;}
    footer {visibility: hidden;}
    .block-container { padding-top: 1rem; }
    [data-testid="stSidebarCollapsedControl"] {display: none;}
    
    /* ä¼˜åŒ–æ•°æ®çœ‹æ¿å¤–è§‚ */
    [data-testid="stMetricValue"] {
        font-size: 28px !important;
        color: var(--primary-color) !important;
    }
    
    /* å‚æ•°é¢æ¿åº•è‰²æ¡† */
    .param-box {
        background-color: var(--secondary-background-color);
        padding: 15px 20px 5px 20px;
        border-radius: 10px;
        border: 1px solid var(--border-color-light);
        margin-bottom: 20px;
    }
    
    /* å¤åˆ¶æç¤ºæ–‡å­—é«˜äº® */
    .copy-hint {
        color: #888;
        font-size: 14px;
        margin-bottom: 5px; 
        margin-top: 10px;
        padding-left: 5px;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# 2. æ•°æ®åŠ è½½ (Data Loading)
# ==========================================
@st.cache_data
def load_knowledge_base():
    try:
        with open('data/terms.json', 'r', encoding='utf-8') as f:
            terms = json.load(f)
        with open('data/proper.json', 'r', encoding='utf-8') as f:
            proper = json.load(f)
        with open('data/patch.json', 'r', encoding='utf-8') as f:
            patch = json.load(f)
        with open('data/ambiguous.json', 'r', encoding='utf-8') as f:
            ambiguous = set(json.load(f))
            
        terms = {k.lower(): v for k, v in terms.items()}
        proper = {k.lower(): v for k, v in proper.items()}
        
        return terms, proper, patch, ambiguous
    except FileNotFoundError:
        st.error("âš ï¸ ç¼ºå°‘æ•°æ®æ–‡ä»¶ï¼è¯·ç¡®ä¿ `data/` æ–‡ä»¶å¤¹ä¸‹åŒ…å« terms.json, proper.json, patch.json, ambiguous.json")
        return {}, {}, {}, set()

BUILTIN_TECHNICAL_TERMS, PROPER_NOUNS_DB, BUILTIN_PATCH_VOCAB, AMBIGUOUS_WORDS = load_knowledge_base()

# ==========================================
# 3. åˆå§‹åŒ– NLP (è¯å½¢è¿˜åŸå¼•æ“)
# ==========================================
@st.cache_resource
def setup_nltk():
    root_dir = os.path.dirname(os.path.abspath(__file__))
    nltk_data_dir = os.path.join(root_dir, 'nltk_data')
    if not os.path.exists(nltk_data_dir):
        os.makedirs(nltk_data_dir)
    nltk.data.path.append(nltk_data_dir)
    for pkg in ['averaged_perceptron_tagger', 'punkt']:
        try: nltk.download(pkg, download_dir=nltk_data_dir, quiet=True)
        except: pass

setup_nltk()

def get_lemma(w):
    """æå–å•ä¸ªå•è¯çš„åŸå‹"""
    lemmas_dict = lemminflect.getAllLemmas(w)
    if not lemmas_dict:
        return w.lower()
    if 'ADJ' in lemmas_dict: return lemmas_dict['ADJ'][0]
    elif 'ADV' in lemmas_dict: return lemmas_dict['ADV'][0]
    elif 'VERB' in lemmas_dict: return lemmas_dict['VERB'][0]
    elif 'NOUN' in lemmas_dict: return lemmas_dict['NOUN'][0]
    else: return list(lemmas_dict.values())[0][0]

# ==========================================
# 4. è¯åº“åŠ è½½
# ==========================================
POSSIBLE_FILES = ["coca_cleaned.csv", "data.csv"]

@st.cache_data
def load_vocab():
    vocab = {}
    file_path = next((f for f in POSSIBLE_FILES if os.path.exists(f)), None)
    if file_path:
        try:
            df = pd.read_csv(file_path)
            cols = [str(c).strip().lower() for c in df.columns]
            df.columns = cols
            w_col = next((c for c in cols if 'word' in c or 'å•è¯' in c), cols[0])
            r_col = next((c for c in cols if 'rank' in c or 'æ’åº' in c), cols[1])
            df[w_col] = df[w_col].astype(str).str.lower().str.strip()
            df[r_col] = pd.to_numeric(df[r_col], errors='coerce').fillna(99999)
            df = df.sort_values(r_col, ascending=True)
            df = df.drop_duplicates(subset=[w_col], keep='first')
            vocab = pd.Series(df[r_col].values, index=df[w_col]).to_dict()
        except: pass
    
    for word, rank in BUILTIN_PATCH_VOCAB.items():
        vocab[word] = rank
        
    URGENT_OVERRIDES = {
        "china": 400, "turkey": 1500, "march": 500, "may": 100, "august": 1500, "polish": 2500,
        "monday": 300, "tuesday": 300, "wednesday": 300, "thursday": 300, "friday": 300, "saturday": 300, "sunday": 300,
        "january": 400, "february": 400, "april": 400, "june": 400, "july": 400, "september": 400, "october": 400, "november": 400, "december": 400,
        "usa": 200, "uk": 200, "google": 1000, "apple": 1000, "microsoft": 1500
    }
    for word, rank in URGENT_OVERRIDES.items():
        vocab[word] = rank
        
    return vocab

vocab_dict = load_vocab()

# ==========================================
# 5. AI æŒ‡ä»¤ç”Ÿæˆå™¨
# ==========================================
def generate_ai_prompt(word_list, output_format, def_mode="single", is_term_list=False):
    words_str = ", ".join(word_list)
    core_principle_text = ""
    
    if is_term_list or def_mode == "term":
        core_principle_text = """1. æ ¸å¿ƒåŸåˆ™ï¼šé¢†åŸŸé”å®š (Domain Locked)
- **é¢†åŸŸåŒ¹é…**ï¼šå¦‚æœå•è¯å¸¦æœ‰ (Domain) æ ‡ç­¾ï¼Œ**å¿…é¡»**ä»…æä¾›ç¬¦åˆè¯¥é¢†åŸŸèƒŒæ™¯çš„ä¸“ä¸šé‡Šä¹‰ã€‚
- **åŸå­æ€§**ï¼šä¸€å¼ å¡ç‰‡åªè§£é‡Šè¯¥é¢†åŸŸçš„ä¸€ä¸ªå«ä¹‰ã€‚"""
    elif def_mode == "split":
        core_principle_text = """1. æ ¸å¿ƒåŸåˆ™ï¼šåŸå­æ€§ (Atomicity)
- **å«ä¹‰æ‹†åˆ†**ï¼šè‹¥ä¸€ä¸ªå•è¯æœ‰å¤šä¸ªä¸åŒå¸¸ç”¨é‡Šä¹‰ï¼ˆåè¯ vs åŠ¨è¯ï¼Œå­—é¢ä¹‰ vs å¼•ç”³ä¹‰ï¼‰ï¼Œ**å¿…é¡»æ‹†åˆ†ä¸ºå¤šæ¡ï¼ˆ1-3ï¼‰ç‹¬ç«‹æ•°æ®**ï¼ˆå³ä¸ºåŒä¸€ä¸ªå•è¯ç”Ÿæˆå¤šè¡Œ/å¤šå¼ å¡ç‰‡ï¼‰ã€‚
- **ä¸¥ç¦å †ç Œ**ï¼šæ¯å¼ å¡ç‰‡åªæ‰¿è½½ä¸€ä¸ªç‰¹å®šè¯­å¢ƒä¸‹çš„å«ä¹‰ï¼Œä¸å‡†å°†å¤šä¸ªé‡Šä¹‰æŒ¤åœ¨ä¸€èµ·ã€‚"""
    else: 
        core_principle_text = """1. æ ¸å¿ƒåŸåˆ™ï¼šæç®€é€Ÿè®° (Minimalist)
- **å•ä¸€é‡Šä¹‰**ï¼šè¯·**ä»…æä¾› 1 ä¸ªæœ€æ ¸å¿ƒã€æœ€å¸¸ç”¨çš„é‡Šä¹‰**ã€‚
- **ä¸¥ç¦æ‹†åˆ†**ï¼šå¯¹äºè¿™äº›ç”Ÿè¯ï¼Œä¸è¦ç”Ÿæˆå¤šå¼ å¡ç‰‡ï¼Œä¸€å¼ å¡ç‰‡å³å¯ã€‚
- **å‡è½»è´Ÿæ‹…**ï¼šç›®çš„æ˜¯å¿«é€Ÿæ··ä¸ªè„¸ç†Ÿï¼Œä¸è¦é¢é¢ä¿±åˆ°ã€‚"""

    if output_format == 'csv':
        format_req = "CSV Code Block (åç¼€å .csv)"
        format_desc = "è¯·ç›´æ¥è¾“å‡ºæ ‡å‡† CSV ä»£ç å—ã€‚"
    else:
        format_req = "TXT Code Block (åç¼€å .txt)"
        format_desc = "è¯·è¾“å‡ºçº¯æ–‡æœ¬ TXT ä»£ç å—ã€‚"

    prompt = f"""
è¯·æ‰®æ¼”ä¸€ä½ä¸“ä¸šçš„ Anki åˆ¶å¡ä¸“å®¶ã€‚è¿™æ˜¯æˆ‘æ•´ç†çš„å•è¯åˆ—è¡¨ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ã€æ ¸å¿ƒåŸåˆ™ã€‘ä¸ºæˆ‘ç”Ÿæˆå¯¼å…¥æ–‡ä»¶ã€‚

{core_principle_text}

2. å¡ç‰‡æ­£é¢ (Column 1: Front)
- å†…å®¹ï¼šæä¾›è‡ªç„¶çš„çŸ­è¯­æˆ–æ­é… (Phrase/Collocation)ã€‚
- æ ·å¼ï¼šçº¯æ–‡æœ¬ã€‚
- æ³¨æ„ï¼šå¦‚æœæ˜¯â€œå«ä¹‰æ‹†åˆ†â€æ¨¡å¼ï¼Œæ­£é¢å¯ä»¥æ˜¯ä¸€æ ·çš„å•è¯/çŸ­è¯­ï¼Œä½†èƒŒé¢è§£é‡Šä¸åŒã€‚

3. å¡ç‰‡èƒŒé¢ (Column 2: Back)
- æ ¼å¼ï¼šHTML æ’ç‰ˆï¼ŒåŒ…å«ä¸‰éƒ¨åˆ†ï¼Œå¿…é¡»ä½¿ç”¨ <br><br> åˆ†éš”ã€‚
- ç»“æ„ï¼šè‹±æ–‡é‡Šä¹‰<br><br><em>æ–œä½“ä¾‹å¥</em><br><br>ã€è¯æº/è¯æ ¹è¯ç¼€ã€‘ä¸­æ–‡åŠ©è®° (è¯æºä¼˜å…ˆ)

4. è¾“å‡ºæ ¼å¼æ ‡å‡† ({format_req})
- {format_desc}
- å…³é”®æ ¼å¼ï¼šä½¿ç”¨è‹±æ–‡é€—å· (,) åˆ†éš”ï¼Œä¸”æ¯ä¸ªå­—æ®µå†…å®¹å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å· ("...") åŒ…è£¹ã€‚

å¾…å¤„ç†å•è¯ï¼š
{words_str}
"""
    return prompt

# ==========================================
# 6. æ ¸å¿ƒåˆ†æå¼•æ“
# ==========================================
def analyze_words(unique_word_list):
    """ç›´æ¥å¯¹å»é‡ä¸”è¿˜åŸåçš„å•è¯åˆ—è¡¨è¿›è¡Œè¯é¢‘å®šçº§"""
    unique_items = [] 
    JUNK_WORDS = {'s', 't', 'd', 'm', 'll', 've', 're'}
    
    for item_lower in unique_word_list:
        if len(item_lower) < 2 and item_lower not in ['a', 'i']: continue
        if item_lower in JUNK_WORDS: continue
        
        actual_rank = vocab_dict.get(item_lower, 99999)
        
        # 1. æœ¯è¯­èº«ä»½
        if item_lower in BUILTIN_TECHNICAL_TERMS:
            domain = BUILTIN_TECHNICAL_TERMS[item_lower]
            term_rank = actual_rank if actual_rank != 99999 else 15000
            unique_items.append({
                "word": f"{item_lower} ({domain})", 
                "rank": term_rank,
                "raw": item_lower
            })
            continue
        
        # 2. ä¸“åä¸æ­§ä¹‰è¯
        if item_lower in PROPER_NOUNS_DB or item_lower in AMBIGUOUS_WORDS:
            display = PROPER_NOUNS_DB.get(item_lower, item_lower.title())
            unique_items.append({
                "word": display,
                "rank": actual_rank, 
                "raw": item_lower
            })
            continue
            
        # 3. æ™®é€šè¯
        if actual_rank != 99999:
            unique_items.append({
                "word": item_lower,
                "rank": actual_rank,
                "raw": item_lower
            })
            
    return pd.DataFrame(unique_items)

# ==========================================
# 7. ç•Œé¢å¸ƒå±€ä¸ç»Ÿä¸€æµæ°´çº¿
# ==========================================
st.title("ğŸš€ Vocab Master Pro - å…¨èƒ½é•¿æ–‡è§£æå¼•æ“")
st.markdown("ğŸ’¡ **ä¸€ç«™å¼å·¥ä½œæµ**ï¼šæ”¯æŒç²˜è´´å‡ åä¸‡å­—çš„è¶…é•¿æ–‡æœ¬ï¼Œ**æ›´æ”¯æŒç›´æ¥ä¸Šä¼  TXT åŸè‘—æ–‡ä»¶**ï¼Œçªç ´æµè§ˆå™¨æ€§èƒ½æé™ï¼ç³»ç»Ÿå°†ä¸€é”®å®Œæˆã€è¯å½¢è¿˜åŸã€‘ã€ã€å…¨é‡åˆ†çº§ã€‘å¹¶æå–ã€Top N ç²¾é€‰ã€‘ã€‚")

if "raw_input_text" not in st.session_state:
    st.session_state.raw_input_text = ""
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0 

def clear_all_inputs():
    st.session_state.raw_input_text = ""
    st.session_state.uploader_key += 1 

# --- å‚æ•°é…ç½®åŒº ---
st.markdown("<div class='param-box'>", unsafe_allow_html=True)
c1, c2, c3, c4, c5 = st.columns(5)
with c1: current_level = st.number_input("ğŸ¯ å½“å‰æ°´å¹³ (èµ·)", 0, 30000, 9000, 500, help="ä½äºæ­¤è¯é¢‘çš„è§†ä¸ºå·²æŒæ¡")
with c2: target_level = st.number_input("ğŸ¯ ç›®æ ‡æ°´å¹³ (æ­¢)", 0, 30000, 15000, 500, help="é«˜äºæ­¤è¯é¢‘çš„è§†ä¸ºè¶…çº²")
with c3: top_n = st.number_input("ğŸ”¥ ç²¾é€‰ Top N", 10, 500, 50, 10, help="ä»å‰©ä½™ç”Ÿè¯ä¸­æŒ‘é€‰çš„æœ€æ ¸å¿ƒæ•°é‡")
with c4: min_rank_threshold = st.number_input("ğŸ“‰ å¿½ç•¥å‰ N è¯", 0, 20000, 3000, 500, help="ç²¾é€‰æ—¶å¿½ç•¥å¤ªç®€å•çš„åŸºç¡€è¯")
with c5: 
    st.write("") 
    st.write("") 
    show_rank = st.checkbox("ğŸ”¢ é™„åŠ æ˜¾ç¤º Rank", value=False)
st.markdown("</div>", unsafe_allow_html=True)

# --- åŒé€šé“è¾“å…¥åŒº ---
col_input1, col_input2 = st.columns([3, 2])
with col_input1:
    raw_text = st.text_area("ğŸ“¥ åœ¨æ­¤ç²˜è´´è‹±æ–‡åŸæ–‡ (æ”¯æŒ10ä¸‡å­—ä»¥å†…)...", height=200, key="raw_input_text")
with col_input2:
    st.info("ğŸ’¡ **çªç ´æé™**ï¼šè¶…10ä¸‡å­—çš„è‹±æ–‡åŸè‘—/è®ºæ–‡ï¼Œè¯·å‹¿ç²˜è´´ï¼Œç›´æ¥åœ¨æ­¤ä¸Šä¼  ğŸ‘‡")
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼  .txt çº¯æ–‡æœ¬æ–‡ä»¶", type=["txt"], key=f"uploader_{st.session_state.uploader_key}")

# --- æŒ‰é’®åŒº ---
col_btn1, col_btn2 = st.columns([5, 1])
with col_btn1:
    btn_process = st.button("ğŸš€ ä¸€é”®æ™ºèƒ½è§£æ (å¤„ç†é•¿æ–‡)", type="primary", use_container_width=True)
with col_btn2:
    st.button("ğŸ—‘ï¸ ä¸€é”®æ¸…ç©º", on_click=clear_all_inputs, use_container_width=True)

st.divider()

combined_text = raw_text
if uploaded_file is not None:
    file_content = uploaded_file.getvalue().decode("utf-8", errors="ignore")
    combined_text += "\n" + file_content

# --- ç»Ÿä¸€æµæ°´çº¿å¤„ç†é€»è¾‘ ---
if btn_process and combined_text.strip() and vocab_dict:
    # â±ï¸ è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    with st.spinner("ğŸ§  æ­£åœ¨è¿›è¡Œäº¿çº§è¯å½¢è¿˜åŸä¸å…¨é‡è¯é¢‘åŒ¹é…ï¼ˆæ–‡ä»¶è¶Šå¤§æ‰€éœ€æ—¶é—´è¶Šé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰..."):
        
        # 1. æå–æ€»è¯æ•°
        raw_words = re.findall(r"[a-zA-Z']+", combined_text)
        total_word_count = len(raw_words)
        
        # 2. æ™ºèƒ½è¿˜åŸ
        lemmatized_words = [get_lemma(w) for w in raw_words]
        full_lemmatized_text = " ".join(lemmatized_words)
        
        # 3. å»é‡
        unique_lemmas = list(set([w.lower() for w in lemmatized_words]))
        unique_word_count = len(unique_lemmas)
        
        # 4. åˆ†çº§
        df = analyze_words(unique_lemmas)
        valid_word_count = len(df)
        
        # â±ï¸ è®°å½•ç»“æŸæ—¶é—´å¹¶è®¡ç®—è€—æ—¶
        end_time = time.time()
        process_time = end_time - start_time
        
        # === æ ¸å¿ƒä¿®æ”¹ï¼šæ–°å¢ç¬¬å››åˆ—æ•°æ®çœ‹æ¿ï¼Œå±•ç¤ºé—ªç”µæé€Ÿè€—æ—¶ ===
        col_m1, col_m2, col_m3, col_m4 = st.columns(4)
        col_m1.metric(label="ğŸ“ è§£ææ€»å­—æ•°", value=f"{total_word_count:,}")
        col_m2.metric(label="âœ‚ï¸ å»é‡è¯æ ¹æ•°", value=f"{unique_word_count:,}")
        col_m3.metric(label="ğŸ¯ çº³å…¥åˆ†çº§è¯æ±‡", value=f"{valid_word_count:,}")
        col_m4.metric(label="âš¡ æé€Ÿè§£æè€—æ—¶", value=f"{process_time:.2f} ç§’")
        st.write("") # ç•™ç™½
        
        if not df.empty:
            def categorize(row):
                r = row['rank']
                if r <= current_level: return "known"
                elif r <= target_level: return "target"
                else: return "beyond"
            
            df['final_cat'] = df.apply(categorize, axis=1)
            df = df.sort_values(by='rank')
            
            valid_candidates = df[df['rank'] >= min_rank_threshold].copy()
            top_df = valid_candidates.sort_values(by='rank', ascending=True).head(top_n)
            
            t_top, t_target, t_beyond, t_known, t_raw = st.tabs([
                f"ğŸ”¥ Top {len(top_df)} æ ¸å¿ƒç²¾é€‰",
                f"ğŸŸ¡ é‡ç‚¹ ({len(df[df['final_cat']=='target'])})", 
                f"ğŸ”´ è¶…çº² ({len(df[df['final_cat']=='beyond'])})",
                f"ğŸŸ¢ å·²æŒæ¡ ({len(df[df['final_cat']=='known'])})",
                "ğŸ“ è¯å½¢è¿˜åŸå…¨æ–‡è¾“å‡º"
            ])
            
            def render_tab(tab_obj, data_df, label, def_mode, expand_default=False):
                with tab_obj:
                    if not data_df.empty:
                        pure_words = data_df['word'].tolist()
                        
                        display_lines = []
                        for _, row in data_df.iterrows():
                            if show_rank:
                                rank_str = str(int(row['rank'])) if row['rank'] != 99999 else "æœªæ”¶å½•"
                                display_lines.append(f"{row['word']} [Rank: {rank_str}]")
                            else:
                                display_lines.append(row['word'])
                        
                        with st.expander("ğŸ‘ï¸ æŸ¥çœ‹å®Œæ•´å•è¯åˆ—è¡¨", expanded=expand_default):
                            st.markdown("<p class='copy-hint'>ğŸ‘† é¼ æ ‡æ‚¬åœåœ¨ä¸‹æ–¹æ¡†å†…ï¼Œç‚¹å‡»å³ä¸Šè§’ ğŸ“‹ å›¾æ ‡ä¸€é”®å¤åˆ¶å•è¯</p>", unsafe_allow_html=True)
                            st.code("\n".join(display_lines), language='text')
                        
                        st.markdown(f"**ğŸ¤– AI æŒ‡ä»¤ ({label})**")
                        has_term = any('(' in w for w in pure_words)
                        
                        p_csv = generate_ai_prompt(pure_words, 'csv', def_mode, is_term_list=has_term)
                        p_txt = generate_ai_prompt(pure_words, 'txt', def_mode, is_term_list=has_term)
                        
                        t_csv, t_txt = st.tabs(["ğŸ“‹ CSV æŒ‡ä»¤", "ğŸ“ TXT æŒ‡ä»¤"])
                        with t_csv: 
                            st.markdown("<p class='copy-hint'>ğŸ‘† é¼ æ ‡æ‚¬åœåœ¨ä¸‹æ–¹æ¡†å†…ï¼Œç‚¹å‡»å³ä¸Šè§’ ğŸ“‹ å›¾æ ‡ä¸€é”®å¤åˆ¶æŒ‡ä»¤</p>", unsafe_allow_html=True)
                            st.code(p_csv, language='markdown')
                        with t_txt: 
                            st.markdown("<p class='copy-hint'>ğŸ‘† é¼ æ ‡æ‚¬åœåœ¨ä¸‹æ–¹æ¡†å†…ï¼Œç‚¹å‡»å³ä¸Šè§’ ğŸ“‹ å›¾æ ‡ä¸€é”®å¤åˆ¶æŒ‡ä»¤</p>", unsafe_allow_html=True)
                            st.code(p_txt, language='markdown')
                    else: st.info("è¯¥åŒºé—´æš‚æ— ç¬¦åˆæ¡ä»¶çš„å•è¯")

            render_tab(t_top, top_df, "æ ¸å¿ƒå•ä¹‰", def_mode="single", expand_default=True) 
            render_tab(t_target, df[df['final_cat']=='target'], "é‡ç‚¹", def_mode="single", expand_default=False)
            render_tab(t_beyond, df[df['final_cat']=='beyond'], "è¶…çº²", def_mode="single", expand_default=False)
            render_tab(t_known, df[df['final_cat']=='known'], "ç†Ÿè¯æ‹†åˆ†", def_mode="split", expand_default=False)
            
            # æ¸²æŸ“è¿˜åŸåŸæ–‡æ¿å— (é˜²å¡æ­» & ä¸‹è½½ä¼˜åŒ–ç‰ˆ)
            with t_raw:
                st.info("ğŸ’¡ è¿™æ˜¯è‡ªåŠ¨è¯å½¢è¿˜åŸï¼ˆLemmatizedï¼‰åçš„å…¨æ–‡ã€‚")
                
                # 1. ç›´æ¥æä¾›æœ¬åœ°æ–‡ä»¶ä¸‹è½½ï¼Œå®Œå…¨ç»•è¿‡æµè§ˆå™¨æ¸²æŸ“é™åˆ¶ï¼
                st.download_button(
                    label="ğŸ’¾ ä¸€é”®ä¸‹è½½å®Œæ•´è¯å½¢è¿˜åŸåŸæ–‡ (.txt)",
                    data=full_lemmatized_text,
                    file_name="lemmatized_full_text.txt",
                    mime="text/plain",
                    type="primary"
                )
                
                # 2. é™åˆ¶ç½‘é¡µç«¯çš„æ˜¾ç¤ºé•¿åº¦ï¼ˆè¶…è¿‡ 5ä¸‡å­—ç¬¦ å°±æˆªæ–­é˜²å¡ï¼‰
                display_limit = 50000
                if len(full_lemmatized_text) > display_limit:
                    st.warning("âš ï¸ ä¸ºé˜²æ­¢æµè§ˆå™¨å¡æ­»ï¼Œç½‘é¡µä»…å±•ç¤ºå‰ 50,000 ä¸ªå­—ç¬¦ã€‚è¯·ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®ä¸‹è½½å®Œæ•´ç‰ˆã€‚")
                    st.markdown("<p class='copy-hint'>ğŸ‘† é¼ æ ‡æ‚¬åœåœ¨ä¸‹æ–¹æ¡†å†…å¯å¤åˆ¶å±•ç¤ºéƒ¨åˆ†</p>", unsafe_allow_html=True)
                    st.code(full_lemmatized_text[:display_limit] + "\n\n... [æ–‡æœ¬è¶…é•¿ï¼Œå‰©ä½™å‡ åä¸‡å­—å·²æŠ˜å ï¼Œè¯·ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®ä¸‹è½½æŸ¥çœ‹] ...", language='text')
                else:
                    st.markdown("<p class='copy-hint'>ğŸ‘† é¼ æ ‡æ‚¬åœåœ¨ä¸‹æ–¹æ¡†å†…ï¼Œç‚¹å‡»å³ä¸Šè§’ ğŸ“‹ å›¾æ ‡ä¸€é”®å¤åˆ¶å…¨æ–‡</p>", unsafe_allow_html=True)
                    st.code(full_lemmatized_text, language='text')